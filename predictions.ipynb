{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import requests\n",
    "import heapq\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import floor\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch rows from the database. Run once and only when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8240287\n"
     ]
    }
   ],
   "source": [
    "df: pd.DataFrame = pd.read_pickle('bikeshare_data.pkl')\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fetch station_information, which will be used in later functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKE_BASE_ENDPOINT = \"https://tor.publicbikesystem.net/ube/gbfs/v1/en/\" \n",
    "ENDPOINTS = {\n",
    "        \"station_status\": BIKE_BASE_ENDPOINT + \"station_status\",\n",
    "        \"station_information\": BIKE_BASE_ENDPOINT + \"station_information\",\n",
    "}\n",
    "\n",
    "\n",
    "def _fetch_station_status(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station status for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_status\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "def _fetch_station_information(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station information for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_information\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "station_status = _fetch_station_status(ENDPOINTS)\n",
    "station_information = _fetch_station_information(ENDPOINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the working station IDS, discarding EOL stations and other define ID related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_IDs() -> list[str]:\n",
    "    \"\"\"Return a sorted list with all the stationIDs.\"\"\"\n",
    "    ids = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            ids.append(station_status[status_i][\"station_id\"])\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    return ids\n",
    "\n",
    "original_station_ids = get_all_IDs()\n",
    "NUM_STATIONS_ORIGINAL = len(original_station_ids)\n",
    "\n",
    "\n",
    "# to be used for testing\n",
    "def id_to_original_ID(id: int) -> int:\n",
    "    \"\"\"Convert the id (internal) to the original ID in the data.\"\"\"\n",
    "    return original_station_ids[id]\n",
    "\n",
    "@cache\n",
    "def station_encoding(station_id: int) -> list[int]:\n",
    "    \"\"\"Return a one hot encoding of the original station ID, station_id, based on its index in original_station_ids.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATIONS_ORIGINAL\n",
    "    index = original_station_ids.index(station_id)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all weather status, taking from the open weather map website\n",
    "WEATHER_STATUSES = [\n",
    "        200, 201, 202, 210, 211, 212, 221, 230, 231, 232,\n",
    "        300, 301, 302, 310, 311, 312, 313, 314, 321,\n",
    "        500, 501, 502, 503, 504, 511, 520, 521, 522, 531,\n",
    "        600, 601, 602, 611, 612, 613, 615, 616, 620, 621, 622,\n",
    "        701, 711, 721, 731, 741, 751, 761, 762, 771, 781,\n",
    "        800,\n",
    "        801, 802, 803, 804\n",
    "]\n",
    "NUM_STATUSES =  len(WEATHER_STATUSES)\n",
    "\n",
    "@cache\n",
    "def weather_encoding(weather_status: int) -> tuple[int]:\n",
    "    \"\"\"Return a one hot encoding of weather_status based on its index in WEATHER_STATUSES.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATUSES\n",
    "    index = WEATHER_STATUSES.index(weather_status)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain stations are turned of for a day or some period of time, we ignore those stations in our analysis\n",
    "\n",
    "group_sizes = df.groupby(\"StationID\").size()\n",
    "size_to_groups: dict = group_sizes.groupby(group_sizes).apply(lambda x: list(x.index)).to_dict()\n",
    "\n",
    "station_ids = size_to_groups[max(size_to_groups.keys())]\n",
    "NUM_STATIONS = len(station_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids_set = set(station_ids)\n",
    "\n",
    "def nearest_k_stations(K: int) -> dict[int, list[int]]:\n",
    "    \"\"\"Compute the K nearest stations to all stations, including itself.\n",
    "    \n",
    "    The returned dict is a mapping from (transformed) stationID to a list containing the \n",
    "    (transformed) station IDs of the nearest K stations.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    coordinates = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            if station_status[status_i][\"station_id\"] in station_ids_set:\n",
    "                # the tranformed stationID is the stationID used internally\n",
    "                # it is simply the index of the stationID in the original_station_ids list\n",
    "                transformed_station_id = len(ids)\n",
    "                ids.append(transformed_station_id)\n",
    "                coordinates.append((station_information[information_i][\"lat\"], station_information[information_i][\"lon\"]))\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    k_nearest = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        coordinate = coordinates[i]\n",
    "        distances = [(distance.euclidean(x, coordinate), j) for j, x in enumerate(coordinates)]\n",
    "        closest = heapq.nsmallest(K + 1, distances)\n",
    "        k_nearest[id] = [ids[j] for _, j in closest[1:]]\n",
    "    return k_nearest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data in the data frame. Further preprocessing will be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove station ids that are now removed from the endpoints, but were present when data collection started (1 station was problematic)\n",
    "df = df[df[\"StationID\"].isin(station_ids_set)]\n",
    "\n",
    "# df['StationID'] = df['StationID'].apply(station_encoding)\n",
    "# df['WeatherStatus'] = df['WeatherStatus'].apply(weather_encoding)\n",
    "\n",
    "# turn categorical variables into a one-hot encoding\n",
    "# df = pd.get_dummies(df, columns=[\"WeatherStatus\", \"StationID\"], drop_first=True)\n",
    "df = pd.get_dummies(df, columns=[\"WeatherStatus\",], drop_first=True)\n",
    "df = df.drop('StationID', axis=1)\n",
    "\n",
    "# Extract year, month, day of the year, hours, and minutes\n",
    "df['year'] = df['Time'].dt.year\n",
    "df['month'] = df['Time'].dt.month\n",
    "df['dayoftheyear'] = df['Time'].dt.dayofyear\n",
    "df['hours'] = df['Time'].dt.hour + (df['Time'].dt.minute / 60)\n",
    "df['weekday'] = df['Time'].dt.weekday \n",
    "df = df.drop('Time', axis=1)\n",
    "\n",
    "\n",
    "# TODO: consider normalizing other quantities later\n",
    "\n",
    "# CHECK: do the boolean columns cause issues in training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check why this is different\n",
    "# df.head(820)\n",
    "# df.iloc[820][\"StationID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8175265\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for splitting\n",
    "TRAIN_RATIO = 0.70\n",
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 1 - TRAIN_RATIO - VALIDATION_RATIO\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert TRAIN_RATIO + VALIDATION_RATIO + TEST_RATIO == 1.0\n",
    "\n",
    "# Calculate the indices for the splits\n",
    "n = len(df) // NUM_STATIONS\n",
    "train_end = floor(TRAIN_RATIO * n) * NUM_STATIONS  # First half for training\n",
    "val_end = floor((TRAIN_RATIO + VALIDATION_RATIO) * n) * NUM_STATIONS  # Next quarter for validation\n",
    "\n",
    "# Perform the split\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "assert df_train.shape[0] + df_val.shape[0] + df_test.shape[0] == df.shape[0]\n",
    "assert df_train.shape[0] % NUM_STATIONS == 0\n",
    "assert df_val.shape[0] % NUM_STATIONS == 0 \n",
    "assert df_test.shape[0] % NUM_STATIONS == 0 \n",
    "# df_train, df_val, and df_test are your resulting splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NumBikes', 'NumDocks', 'Latitude', 'Longitude', 'Temperature',\n",
      "       'WeatherStatus_701', 'WeatherStatus_741', 'WeatherStatus_800',\n",
      "       'WeatherStatus_801', 'WeatherStatus_802', 'WeatherStatus_803',\n",
      "       'WeatherStatus_804', 'year', 'month', 'dayoftheyear', 'hours',\n",
      "       'weekday'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[28, 12, 43.639832, ..., 207, 0.9, 3],\n",
       "       [1, 13, 43.66496415990742, ..., 207, 0.9, 3],\n",
       "       [0, 17, 43.667333, ..., 207, 0.9, 3],\n",
       "       ...,\n",
       "       [6, 30, 43.75404854399297, ..., 212, 13.8, 1],\n",
       "       [2, 13, 43.76507271818177, ..., 212, 13.8, 1],\n",
       "       [0, 19, 43.77121894873629, ..., 212, 13.8, 1]], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "df_test.to_numpy()\n",
    "df_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the torch Dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeShareDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, lookback: int, lookback_size: int, horizon: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame containing time series data.\n",
    "            lookback (int): Number of time steps to look back.\n",
    "            lookback_size (int): Length of time to jump over each lookback\n",
    "            horizon (int): Number of time steps to predict.\n",
    "            k (int):: Number of nearest stations information to consider in predictions.\n",
    "        \"\"\"\n",
    "        self.FLAT_NUM_FEATURES = data.shape[1] * (k + 1)\n",
    "        self.data = data.to_numpy()\n",
    "        self.lookback = lookback\n",
    "        self.lookback_size = lookback_size\n",
    "        self.horizon = horizon\n",
    "        self.k = k\n",
    "        self.k_nearest_dict = nearest_k_stations(k)\n",
    "        self.k_nearest_differences = self._compute_k_nearest_differences()\n",
    "\n",
    "    def _compute_k_nearest_differences(self) -> dict[int, list[int]]:\n",
    "        \"\"\"Return the differences between the indices of neighbours in the pandas dataframe.\"\"\"\n",
    "        k_nearest_differences = defaultdict(list)\n",
    "        for station in self.k_nearest_dict:\n",
    "            for near_station in self.k_nearest_dict[station]:\n",
    "                diff = near_station - station\n",
    "                k_nearest_differences[station].append(diff)\n",
    "            \n",
    "        return k_nearest_differences\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        padded_length = len(self.data) - (NUM_STATIONS * (self.horizon + FIRST_MINUTES_SKIPPED))\n",
    "        padded_length = max(padded_length, 0)\n",
    "        return padded_length\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        start_i = NUM_STATIONS * FIRST_MINUTES_SKIPPED\n",
    "        idx = idx + start_i     \n",
    "        current_station = idx % NUM_STATIONS\n",
    "        x = np.ndarray((self.lookback, self.k + 1, self.data.shape[1]))\n",
    "        for steps_behind in range(self.lookback):\n",
    "            for k in range(self.k + 1):\n",
    "                if k == 0:\n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS)]\n",
    "                else: \n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS) + self.k_nearest_differences[current_station][k - 1]]\n",
    "        x = x.reshape(self.lookback, self.FLAT_NUM_FEATURES)\n",
    "\n",
    "        y = np.ndarray((self.horizon))\n",
    "        for minutes_ahead in range(1, self.horizon + 1):\n",
    "            y[minutes_ahead - 1] = self.data[idx + (NUM_STATIONS * minutes_ahead)][0]  # index 0 is NumBikes\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 6\n",
    "lookback_size = 5\n",
    "horizon = 120\n",
    "k = 5\n",
    "train_data = BikeShareDataset(df_train, lookback, lookback_size, horizon, k)\n",
    "validation_data = BikeShareDataset(df_val, lookback, lookback_size, horizon, k)\n",
    "test_data = BikeShareDataset(df_test, lookback, lookback_size, horizon, k)\n",
    "\n",
    "batch_size = 124\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether the dataset's __get__item__ works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.asarray(\n",
    "#     [\n",
    "#         [[1, 2],\n",
    "#          [3, 4],\n",
    "#          [1, 2]],\n",
    "\n",
    "#         [[5, 6],\n",
    "#          [7, 8],\n",
    "#          [5, 6]],\n",
    "#     ]\n",
    "# )\n",
    "# N, M, K = x.shape\n",
    "# print(x.shape)\n",
    "# flattened_x = x.reshape(N, M * K)\n",
    "# print(flattened_x.shape)\n",
    "\n",
    "# print(flattened_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAT_NUM_FEATURES = df.shape[1] * (k + 1)\n",
    "\n",
    "class BikeShareLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=FLAT_NUM_FEATURES, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True\n",
    "        )\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear((hidden_size * self.lookback), horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, features = x.shape\n",
    "        x, _ = self.lstm(x)\n",
    "        # x is of shape (batch_size, lookbacks, 50)\n",
    "        x = x.reshape(batch_size, sequence_length * self.hidden_size)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class BikeShareNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(FLAT_NUM_FEATURES * lookback, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.horizon),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS = 5\n",
    "# Uncomment for LSTM\n",
    "#model = BikeShareLSTM(HIDDEN_SIZE, horizon, lookback, NUM_LAYERS).to(device)\n",
    "#print(model)\n",
    "\n",
    "model = BikeShareNN(HIDDEN_SIZE, horizon, lookback)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  6.,  6.,  ...,  9.,  9., 10.],\n",
      "        [ 4.,  4.,  4.,  ...,  4.,  4.,  4.],\n",
      "        [21., 21., 21.,  ...,  7.,  7.,  7.],\n",
      "        ...,\n",
      "        [14., 14., 14.,  ..., 11., 11., 11.],\n",
      "        [ 9.,  9.,  9.,  ...,  9.,  9.,  9.],\n",
      "        [ 4.,  4.,  4.,  ...,  7.,  7.,  8.]])\n"
     ]
    }
   ],
   "source": [
    "# loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "# batch = torch.tensor([[1, 2, 3], [1, 2, 3]], dtype=torch.float32)\n",
    "# expected = torch.tensor([[1, 2, 3], [1, 2, 2]], dtype=torch.float32)\n",
    "\n",
    "# sum(abs(batch - expected))\n",
    "\n",
    "X, y = next(iter(train_dataloader))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        assert pred.shape == y.shape\n",
    "        total_loss_by_time = torch.zeros((horizon), dtype=float)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss_by_time += torch.sum(torch.abs(pred - y), dim=0)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_loss_by_time = total_loss_by_time / (batch_size)\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print(f\"average loss by time {average_loss_by_time}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss =  0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 34.610062  [  124/5599865]\n",
      "average loss by time tensor([ 36.4311,  80.6226,  64.8122,  33.9491,  52.1231,   8.4828,   5.9761,\n",
      "         46.7278,  26.2456,   5.0962,  60.1098,  10.5333,  52.7787,   8.4992,\n",
      "         63.3831,   8.4662,   6.6622,  16.1559,  37.8567,  12.9910,  34.6022,\n",
      "         50.6888,  64.1698,  54.4755,  54.0087,  31.0969,   6.1398,   5.3943,\n",
      "         43.6340,  21.5384,  40.4547,  16.5137,  56.0799,  12.3257,  19.1296,\n",
      "         49.1351,  28.0119,  70.6817,   4.9868,  66.8319,  21.3097,  47.9546,\n",
      "         57.8292,  49.2479,  56.2210,   4.9165,  17.6049,  38.3029,  32.6056,\n",
      "         12.2303,   7.4100,  28.1315,  12.3335,   4.9472,  54.9073,  84.1805,\n",
      "         21.8329,  14.8523,   5.5130,   5.8737,  52.3988, 128.7854,  13.9914,\n",
      "         14.7053,  32.7729,  28.7151,  63.7839,  17.0054,  22.1223,  42.2768,\n",
      "         13.4712,  34.4911,   4.9969,  26.4793,  34.2034,  14.0521, 112.3674,\n",
      "         20.7027,  21.8439,  47.7120,  39.9807,  28.1340,   5.2384,  46.0014,\n",
      "         45.7482,  25.5699,  22.6557,  60.6907,  53.3411,  75.2597,  70.6892,\n",
      "         43.2706,  19.3450,  42.0016,  50.7427,  23.7907,   7.9524,   7.5397,\n",
      "         14.2719,  81.5612,  16.3882,   5.3770,  32.0093,  23.8716,  13.0330,\n",
      "         38.1156,   9.9998,  21.9680,  92.0146,  33.9503,  20.0944,  11.9954,\n",
      "         90.3243,  52.9308,  67.7632,  13.6247,  16.9510,  19.2949,  49.9634,\n",
      "         66.8693], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.908316  [12524/5599865]\n",
      "average loss by time tensor([6.7029, 6.7446, 6.6400, 6.8656, 6.5153, 6.7167, 6.6043, 6.7580, 6.7919,\n",
      "        6.8015, 6.9474, 6.9355, 6.8805, 6.8088, 6.6430, 6.8065, 6.6286, 6.8206,\n",
      "        6.6870, 6.5258, 6.7459, 6.7473, 6.5815, 6.7268, 6.7165, 6.6627, 6.7376,\n",
      "        6.7224, 6.6138, 6.7872, 6.6078, 6.9503, 6.7687, 6.7108, 6.8590, 7.0453,\n",
      "        6.9489, 6.8747, 6.8417, 6.8846, 6.8529, 6.9073, 6.8668, 6.9003, 6.8140,\n",
      "        6.9932, 6.9224, 6.8191, 7.0218, 6.9112, 6.8748, 6.8721, 6.8333, 7.0268,\n",
      "        6.9564, 6.8034, 6.9641, 6.8828, 7.0051, 7.0659, 6.8214, 6.9890, 6.9458,\n",
      "        7.1517, 7.0665, 6.9300, 6.9445, 6.8999, 7.1342, 6.9707, 6.9241, 7.0264,\n",
      "        6.9406, 6.7671, 7.0298, 6.9134, 6.7922, 6.7663, 6.8992, 6.9605, 6.8668,\n",
      "        6.8376, 6.9101, 6.9279, 6.9387, 6.9153, 6.8387, 7.0719, 7.0308, 6.9786,\n",
      "        7.0357, 6.9845, 7.0318, 6.9633, 6.9558, 7.0485, 6.9569, 6.9439, 7.2690,\n",
      "        7.0592, 7.1030, 6.9986, 7.0984, 7.0357, 6.9157, 6.9215, 6.9304, 7.1241,\n",
      "        6.9579, 7.0457, 7.1930, 7.1192, 7.0861, 7.1586, 7.3481, 7.1996, 7.2380,\n",
      "        7.2425, 7.0588, 7.1334], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.851437  [24924/5599865]\n",
      "average loss by time tensor([6.7980, 6.8268, 6.7807, 6.9834, 6.6255, 6.7584, 6.6383, 6.7582, 6.7862,\n",
      "        6.8015, 6.9741, 6.9577, 6.9504, 6.8411, 6.6937, 6.8081, 6.6941, 6.8655,\n",
      "        6.7857, 6.7356, 6.9354, 6.9828, 6.8034, 6.9712, 6.9489, 6.8493, 6.9192,\n",
      "        6.8846, 6.8061, 6.9348, 6.7523, 7.0202, 6.7991, 6.7397, 6.8229, 6.9561,\n",
      "        6.8987, 6.8478, 6.8057, 6.8313, 6.7571, 6.7733, 6.7513, 6.7656, 6.6662,\n",
      "        6.8553, 6.7212, 6.6469, 6.8249, 6.7389, 6.7087, 6.6915, 6.6742, 6.8668,\n",
      "        6.8056, 6.6637, 6.7963, 6.7401, 6.8109, 6.8892, 6.6687, 6.8343, 6.7771,\n",
      "        6.9804, 6.8824, 6.7588, 6.7856, 6.7987, 7.0325, 6.9132, 6.9048, 7.0558,\n",
      "        6.9189, 6.7160, 6.9969, 6.8757, 6.7356, 6.8048, 6.9505, 6.9640, 6.8601,\n",
      "        6.8853, 6.9380, 6.9261, 6.9605, 6.9557, 6.7940, 7.0075, 6.9614, 6.9020,\n",
      "        6.9105, 6.8530, 6.9558, 6.9204, 6.9087, 6.9900, 6.7727, 6.7562, 7.0969,\n",
      "        6.8920, 6.9109, 6.8626, 6.9739, 6.9259, 6.8176, 6.8688, 6.8554, 7.0172,\n",
      "        6.8059, 6.8398, 6.9831, 6.8694, 6.8461, 6.9248, 7.0197, 6.8796, 6.9072,\n",
      "        6.9297, 6.7760, 6.8369], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.910293  [37324/5599865]\n",
      "average loss by time tensor([6.8519, 6.8675, 6.8437, 7.0293, 6.7138, 6.8241, 6.6897, 6.8360, 6.8490,\n",
      "        6.8597, 6.9927, 6.9722, 6.9238, 6.8841, 6.7709, 6.9074, 6.7847, 6.9424,\n",
      "        6.8795, 6.7584, 6.9557, 7.0170, 6.8690, 7.0221, 6.9469, 6.8541, 6.9237,\n",
      "        6.8744, 6.8185, 6.9048, 6.7557, 7.0586, 6.8618, 6.8333, 6.9156, 7.0526,\n",
      "        6.9698, 6.8851, 6.8398, 6.8932, 6.8667, 6.9165, 6.9247, 6.9373, 6.8457,\n",
      "        7.0488, 6.9476, 6.8639, 6.9584, 6.9033, 6.8692, 6.8547, 6.8153, 6.9407,\n",
      "        6.8592, 6.7404, 6.8942, 6.8393, 6.9319, 7.0593, 6.8074, 6.9460, 6.8810,\n",
      "        7.0594, 6.9558, 6.8494, 6.8837, 6.8754, 7.0684, 6.9602, 6.9278, 7.0378,\n",
      "        6.9467, 6.8054, 7.0207, 6.8964, 6.8139, 6.8101, 6.8866, 6.8950, 6.8004,\n",
      "        6.7919, 6.8538, 6.8562, 6.9101, 6.8888, 6.7666, 6.9510, 6.9076, 6.8693,\n",
      "        6.9038, 6.8850, 6.9278, 6.9042, 6.8970, 6.9961, 6.8766, 6.8877, 7.1589,\n",
      "        6.9773, 6.9682, 6.9035, 6.9781, 6.9398, 6.8838, 6.8820, 6.9027, 7.0234,\n",
      "        6.8995, 6.9636, 7.1114, 7.0107, 6.9812, 7.0377, 7.1113, 6.9536, 7.0141,\n",
      "        7.0307, 6.9082, 6.9447], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.121488  [49724/5599865]\n",
      "average loss by time tensor([6.6227, 6.6827, 6.6584, 6.8654, 6.5310, 6.6604, 6.5492, 6.6836, 6.6671,\n",
      "        6.6951, 6.8824, 6.9226, 6.8646, 6.8109, 6.6016, 6.7428, 6.7038, 6.8737,\n",
      "        6.7958, 6.6618, 6.8870, 6.9142, 6.7504, 6.9036, 6.8799, 6.8398, 6.9446,\n",
      "        6.9704, 6.8996, 7.0201, 6.8516, 7.1456, 6.9456, 6.8900, 7.0034, 7.1331,\n",
      "        7.0646, 6.9972, 6.9228, 6.9747, 6.9288, 6.9708, 7.0140, 7.0497, 6.9215,\n",
      "        7.1216, 7.0335, 6.9209, 7.0998, 7.0065, 6.9971, 7.0363, 6.9672, 7.1755,\n",
      "        7.1002, 6.9940, 7.1372, 7.0539, 7.1460, 7.2776, 7.0612, 7.2109, 7.1250,\n",
      "        7.3079, 7.2223, 7.1222, 7.1166, 7.1009, 7.3222, 7.2370, 7.1840, 7.3507,\n",
      "        7.2278, 7.1086, 7.4075, 7.3153, 7.1675, 7.2005, 7.2876, 7.3100, 7.2335,\n",
      "        7.2083, 7.2828, 7.2760, 7.3274, 7.3217, 7.1759, 7.4344, 7.3812, 7.3439,\n",
      "        7.3550, 7.3135, 7.3349, 7.2126, 7.2371, 7.3486, 7.2122, 7.2263, 7.5434,\n",
      "        7.4227, 7.4732, 7.4461, 7.5788, 7.5341, 7.4798, 7.5028, 7.4972, 7.6508,\n",
      "        7.4691, 7.4806, 7.6022, 7.4966, 7.4348, 7.4810, 7.5259, 7.3980, 7.4186,\n",
      "        7.4343, 7.3124, 7.4189], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.186275  [62124/5599865]\n",
      "average loss by time tensor([7.0265, 7.0796, 7.0876, 7.2905, 6.9992, 7.1595, 7.0101, 7.1322, 7.1429,\n",
      "        7.1515, 7.2887, 7.2687, 7.2570, 7.2146, 7.0543, 7.2086, 7.1283, 7.2962,\n",
      "        7.2473, 7.1323, 7.3588, 7.3603, 7.1453, 7.2604, 7.2318, 7.1417, 7.1842,\n",
      "        7.1792, 7.1261, 7.2074, 7.0119, 7.2973, 7.1371, 7.1138, 7.2212, 7.3672,\n",
      "        7.3209, 7.2810, 7.2169, 7.2413, 7.2446, 7.2832, 7.2489, 7.2354, 7.1696,\n",
      "        7.3320, 7.2391, 7.1652, 7.3374, 7.2363, 7.1715, 7.2147, 7.1769, 7.3452,\n",
      "        7.2839, 7.1121, 7.2875, 7.1882, 7.2651, 7.3992, 7.1580, 7.3308, 7.2702,\n",
      "        7.4518, 7.3187, 7.1988, 7.2146, 7.1990, 7.4309, 7.3044, 7.2694, 7.4057,\n",
      "        7.3008, 7.1241, 7.3993, 7.2693, 7.1096, 7.1381, 7.2219, 7.2495, 7.1428,\n",
      "        7.1222, 7.1970, 7.2369, 7.2391, 7.1902, 7.0791, 7.3131, 7.2587, 7.2159,\n",
      "        7.2562, 7.1504, 7.2254, 7.1533, 7.1616, 7.2251, 7.0252, 7.0062, 7.2829,\n",
      "        7.0999, 7.1335, 7.0577, 7.1546, 7.1012, 6.9651, 6.9284, 6.9637, 7.1251,\n",
      "        6.9543, 7.0292, 7.1164, 7.0549, 7.0306, 7.0938, 7.1826, 7.0277, 7.0200,\n",
      "        7.0532, 6.9192, 6.9796], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.369645  [74524/5599865]\n",
      "average loss by time tensor([7.3099, 7.3843, 7.3423, 7.5297, 7.3064, 7.4735, 7.3545, 7.4997, 7.4959,\n",
      "        7.5011, 7.6547, 7.6634, 7.6091, 7.5563, 7.3797, 7.4922, 7.4090, 7.5650,\n",
      "        7.5162, 7.4374, 7.6128, 7.6693, 7.5026, 7.6733, 7.6312, 7.5580, 7.6459,\n",
      "        7.6148, 7.5721, 7.6559, 7.4884, 7.7799, 7.6003, 7.5886, 7.7056, 7.8432,\n",
      "        7.7385, 7.6615, 7.5852, 7.6695, 7.6092, 7.6688, 7.6625, 7.6729, 7.5918,\n",
      "        7.7120, 7.6126, 7.5277, 7.6796, 7.5982, 7.5535, 7.5187, 7.4567, 7.6276,\n",
      "        7.4878, 7.3432, 7.5029, 7.4087, 7.4844, 7.6254, 7.3448, 7.5075, 7.4139,\n",
      "        7.5716, 7.4953, 7.3976, 7.3705, 7.2428, 7.4428, 7.3722, 7.3549, 7.4766,\n",
      "        7.4078, 7.2087, 7.4963, 7.3792, 7.2533, 7.2896, 7.3933, 7.4281, 7.3651,\n",
      "        7.3585, 7.4598, 7.4346, 7.4763, 7.4662, 7.2792, 7.4415, 7.3392, 7.3273,\n",
      "        7.2661, 7.1682, 7.2020, 7.0933, 7.0175, 7.0852, 6.9177, 6.9073, 7.1838,\n",
      "        6.9787, 7.0085, 6.9634, 7.0098, 6.9652, 6.8614, 6.8441, 6.8563, 7.0023,\n",
      "        6.8084, 6.8206, 6.9525, 6.9154, 6.9111, 6.9562, 7.0250, 6.8793, 6.8671,\n",
      "        6.8547, 6.7707, 6.8497], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.963849  [86924/5599865]\n",
      "average loss by time tensor([6.9763, 7.0686, 7.0115, 7.1962, 6.9002, 7.0488, 6.9049, 7.1243, 7.1337,\n",
      "        7.1113, 7.2870, 7.2359, 7.2112, 7.1663, 6.9799, 7.1215, 6.9594, 7.1474,\n",
      "        7.1090, 6.9873, 7.2298, 7.2446, 7.1023, 7.2309, 7.2077, 7.1394, 7.2282,\n",
      "        7.2099, 7.1288, 7.2273, 7.0618, 7.3511, 7.1455, 7.0931, 7.1639, 7.2872,\n",
      "        7.2205, 7.1063, 7.0035, 7.0421, 7.0407, 7.0869, 7.0599, 7.0139, 6.9389,\n",
      "        7.1326, 6.9936, 6.8819, 7.0226, 6.9601, 6.9066, 6.8759, 6.8024, 6.9898,\n",
      "        6.9369, 6.8364, 7.0238, 6.9011, 7.0062, 7.1175, 6.9034, 7.0678, 6.9594,\n",
      "        7.0945, 7.0021, 6.9054, 6.8992, 6.9061, 7.0835, 7.0112, 6.9391, 7.0314,\n",
      "        6.9874, 6.8879, 7.1578, 7.0739, 6.9187, 6.9614, 7.0186, 7.0817, 6.9967,\n",
      "        6.9860, 7.0746, 7.0804, 7.0839, 7.0924, 6.9478, 7.1020, 7.1020, 7.0071,\n",
      "        6.9743, 6.9110, 6.9276, 6.8514, 6.8099, 6.8722, 6.7148, 6.6868, 6.9314,\n",
      "        6.7163, 6.7561, 6.6747, 6.7996, 6.7472, 6.6251, 6.5967, 6.6027, 6.7181,\n",
      "        6.5397, 6.5971, 6.6911, 6.5845, 6.5351, 6.6166, 6.6899, 6.5691, 6.5028,\n",
      "        6.4999, 6.4336, 6.4907], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.235054  [99324/5599865]\n",
      "average loss by time tensor([6.7072, 6.7791, 6.7584, 6.9603, 6.6907, 6.8486, 6.7017, 6.8309, 6.8581,\n",
      "        6.8874, 7.0477, 7.0023, 6.9552, 6.9303, 6.7683, 6.8982, 6.8118, 6.9599,\n",
      "        6.9030, 6.7704, 7.0000, 7.0299, 6.8773, 6.9823, 6.9965, 6.9250, 7.0452,\n",
      "        7.0488, 6.9996, 7.1126, 6.9491, 7.2212, 7.0385, 6.9721, 7.0668, 7.2715,\n",
      "        7.2191, 7.1383, 7.1195, 7.2291, 7.2304, 7.3440, 7.3071, 7.3353, 7.2248,\n",
      "        7.4413, 7.3583, 7.2755, 7.4103, 7.3578, 7.3015, 7.3191, 7.2477, 7.4151,\n",
      "        7.3675, 7.1989, 7.3747, 7.2678, 7.3852, 7.4718, 7.1944, 7.3629, 7.3045,\n",
      "        7.4812, 7.4142, 7.3218, 7.3653, 7.3155, 7.5309, 7.4060, 7.3996, 7.5068,\n",
      "        7.3959, 7.2021, 7.5054, 7.3909, 7.2763, 7.3345, 7.3871, 7.4122, 7.3153,\n",
      "        7.3239, 7.4165, 7.3968, 7.4264, 7.3803, 7.2632, 7.4629, 7.4132, 7.3716,\n",
      "        7.3917, 7.3203, 7.4352, 7.3529, 7.3779, 7.4853, 7.3283, 7.3000, 7.6233,\n",
      "        7.4639, 7.4587, 7.4343, 7.4605, 7.4279, 7.3441, 7.3762, 7.3896, 7.5232,\n",
      "        7.3965, 7.4810, 7.5569, 7.4816, 7.4257, 7.4287, 7.4526, 7.2966, 7.2900,\n",
      "        7.2893, 7.1579, 7.2326], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.087023  [111724/5599865]\n",
      "average loss by time tensor([6.8348, 6.9324, 6.9152, 7.0391, 6.7856, 6.9496, 6.8115, 6.9526, 6.9838,\n",
      "        7.0083, 7.1710, 7.1316, 7.0795, 7.0332, 6.8759, 6.9806, 6.8952, 7.0726,\n",
      "        6.9977, 6.9289, 7.1652, 7.1780, 6.9974, 7.1126, 7.0710, 6.9605, 7.0350,\n",
      "        6.9849, 6.9214, 7.0033, 6.8344, 7.0995, 6.9073, 6.8523, 6.9659, 7.1266,\n",
      "        7.0399, 6.9971, 6.9679, 6.9625, 6.9114, 7.0148, 6.9608, 6.9589, 6.8877,\n",
      "        7.1536, 7.0598, 7.0172, 7.1628, 7.1179, 7.0422, 7.0833, 7.0466, 7.1946,\n",
      "        7.1362, 6.9918, 7.1705, 7.0690, 7.1802, 7.2773, 7.0543, 7.2475, 7.1868,\n",
      "        7.3358, 7.2145, 7.0838, 7.1264, 7.0725, 7.2361, 7.1507, 7.1313, 7.2567,\n",
      "        7.1311, 6.9333, 7.2807, 7.1703, 6.9552, 7.0115, 7.0907, 7.1397, 7.0595,\n",
      "        7.0713, 7.1477, 7.1511, 7.1893, 7.1310, 6.9847, 7.1862, 7.1615, 7.1748,\n",
      "        7.1588, 7.1408, 7.2417, 7.1462, 7.1943, 7.2673, 7.0988, 7.0983, 7.4119,\n",
      "        7.2006, 7.2177, 7.1435, 7.2413, 7.2466, 7.1349, 7.1308, 7.1523, 7.2569,\n",
      "        7.1002, 7.1752, 7.2443, 7.2108, 7.1684, 7.2289, 7.2717, 7.1445, 7.1350,\n",
      "        7.1348, 7.0145, 7.0674], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.483246  [124124/5599865]\n",
      "average loss by time tensor([6.1027, 6.1797, 6.1472, 6.3031, 5.9899, 6.1601, 6.0044, 6.1216, 6.1494,\n",
      "        6.1364, 6.3503, 6.3170, 6.2819, 6.2385, 6.0742, 6.2137, 6.1326, 6.3022,\n",
      "        6.2555, 6.1311, 6.3541, 6.3984, 6.1741, 6.3209, 6.3283, 6.2225, 6.3489,\n",
      "        6.3583, 6.2798, 6.3915, 6.2105, 6.5257, 6.3447, 6.2921, 6.3965, 6.5698,\n",
      "        6.5051, 6.4514, 6.3476, 6.4527, 6.4188, 6.4894, 6.4621, 6.4693, 6.3467,\n",
      "        6.5430, 6.4254, 6.3867, 6.4986, 6.4008, 6.3993, 6.4496, 6.3452, 6.5472,\n",
      "        6.4929, 6.3426, 6.5582, 6.4497, 6.5059, 6.6632, 6.4636, 6.6628, 6.6353,\n",
      "        6.8514, 6.7468, 6.5939, 6.5931, 6.5487, 6.7791, 6.6701, 6.6218, 6.7081,\n",
      "        6.6175, 6.4148, 6.7087, 6.5940, 6.4571, 6.5183, 6.5729, 6.6154, 6.5514,\n",
      "        6.5342, 6.6260, 6.6051, 6.6285, 6.6127, 6.5045, 6.7085, 6.6119, 6.6474,\n",
      "        6.6599, 6.6020, 6.6869, 6.5927, 6.6541, 6.7284, 6.5744, 6.5660, 6.8371,\n",
      "        6.6685, 6.6391, 6.5781, 6.6659, 6.6583, 6.5105, 6.5100, 6.5265, 6.6878,\n",
      "        6.5268, 6.6040, 6.7267, 6.7268, 6.6858, 6.7708, 6.8160, 6.6983, 6.6814,\n",
      "        6.7238, 6.5944, 6.6257], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.194316  [136524/5599865]\n",
      "average loss by time tensor([7.0934, 7.1657, 7.1610, 7.3087, 7.0420, 7.2257, 7.0657, 7.2112, 7.2335,\n",
      "        7.1914, 7.3762, 7.3248, 7.2938, 7.2696, 7.0987, 7.2110, 7.0990, 7.2295,\n",
      "        7.1649, 7.0360, 7.3089, 7.3364, 7.2247, 7.3375, 7.3711, 7.2517, 7.2571,\n",
      "        7.2340, 7.1683, 7.2386, 7.0830, 7.3547, 7.1302, 7.0709, 7.2023, 7.3514,\n",
      "        7.2198, 7.1605, 7.0710, 7.1589, 7.1377, 7.2075, 7.1741, 7.1360, 7.0246,\n",
      "        7.2308, 7.1042, 7.0629, 7.1952, 7.1003, 7.0394, 7.0680, 6.9728, 7.1550,\n",
      "        7.1167, 7.0247, 7.2195, 7.1289, 7.1926, 7.3150, 7.0722, 7.2513, 7.1762,\n",
      "        7.3266, 7.2583, 7.1621, 7.1708, 7.1426, 7.2732, 7.1937, 7.1966, 7.2722,\n",
      "        7.1633, 6.9962, 7.3300, 7.2296, 7.1169, 7.1501, 7.2131, 7.2622, 7.1722,\n",
      "        7.1664, 7.2449, 7.2225, 7.2300, 7.1218, 6.9926, 7.2111, 7.1399, 7.1656,\n",
      "        7.1743, 7.0998, 7.2252, 7.1602, 7.2126, 7.2768, 7.1386, 7.1098, 7.4394,\n",
      "        7.2714, 7.2944, 7.2676, 7.3197, 7.2950, 7.2231, 7.2028, 7.2228, 7.3130,\n",
      "        7.1606, 7.2617, 7.2994, 7.2635, 7.2070, 7.2820, 7.3113, 7.1914, 7.1713,\n",
      "        7.1772, 7.0541, 7.1295], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.700549  [148924/5599865]\n",
      "average loss by time tensor([6.5575, 6.6127, 6.5898, 6.7174, 6.4413, 6.5981, 6.4055, 6.5067, 6.5785,\n",
      "        6.5637, 6.7240, 6.6548, 6.6762, 6.6465, 6.4674, 6.6058, 6.4931, 6.6089,\n",
      "        6.5269, 6.4043, 6.5779, 6.6207, 6.4668, 6.5621, 6.5507, 6.5168, 6.6500,\n",
      "        6.6052, 6.5681, 6.6455, 6.4861, 6.7883, 6.5773, 6.5097, 6.5964, 6.7212,\n",
      "        6.6248, 6.5847, 6.5164, 6.5925, 6.5345, 6.6438, 6.5815, 6.6241, 6.4858,\n",
      "        6.7247, 6.5585, 6.5388, 6.6480, 6.5668, 6.5003, 6.4980, 6.4209, 6.6085,\n",
      "        6.5440, 6.4121, 6.6341, 6.5529, 6.6351, 6.7245, 6.4658, 6.6755, 6.5762,\n",
      "        6.7687, 6.6621, 6.5766, 6.5468, 6.5020, 6.6700, 6.6363, 6.6425, 6.8360,\n",
      "        6.7470, 6.7079, 6.9829, 6.9019, 6.7510, 6.8003, 6.8481, 6.8837, 6.8039,\n",
      "        6.8257, 6.9273, 6.9208, 6.9203, 6.8795, 6.7425, 6.9314, 6.8547, 6.8544,\n",
      "        6.8672, 6.7080, 6.8216, 6.6821, 6.7848, 6.8560, 6.6734, 6.6758, 6.9443,\n",
      "        6.8283, 6.8741, 6.8249, 6.9110, 6.9073, 6.7994, 6.8342, 6.8738, 7.0338,\n",
      "        6.8157, 6.9180, 6.9675, 6.9665, 6.9483, 7.0653, 7.1047, 7.0262, 6.9754,\n",
      "        7.0267, 6.9289, 7.0035], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.021142  [161324/5599865]\n",
      "average loss by time tensor([5.9333, 5.9789, 5.9516, 6.0534, 5.7466, 5.8664, 5.6921, 5.8228, 5.8616,\n",
      "        5.8209, 5.9906, 5.9362, 5.9063, 5.8722, 5.7023, 5.8399, 5.8515, 5.9811,\n",
      "        5.9659, 5.8584, 6.0968, 6.0658, 5.8907, 5.9847, 5.9817, 5.8689, 5.9530,\n",
      "        5.9628, 5.8952, 5.9490, 5.7852, 6.0768, 5.9353, 5.9393, 6.0341, 6.2277,\n",
      "        6.1516, 6.1295, 6.0728, 6.1598, 6.0922, 6.1929, 6.1163, 6.1042, 5.9953,\n",
      "        6.2022, 6.0370, 6.0155, 6.0987, 6.0005, 5.9617, 6.0163, 5.9002, 6.0865,\n",
      "        5.9928, 5.8860, 6.0746, 5.9592, 6.0258, 6.1417, 5.8500, 6.0691, 5.9512,\n",
      "        6.1213, 6.0267, 5.9772, 5.9713, 5.9254, 6.0733, 6.0120, 6.0074, 6.0850,\n",
      "        5.9978, 5.8181, 6.1029, 6.0350, 5.8890, 5.9795, 6.0367, 6.0588, 5.9581,\n",
      "        5.9600, 6.0496, 6.0586, 6.0518, 5.9843, 5.8551, 6.0790, 6.0127, 6.0270,\n",
      "        6.0378, 5.9649, 6.0318, 5.9899, 6.0761, 6.1642, 6.0481, 6.0426, 6.2954,\n",
      "        6.1696, 6.2126, 6.1095, 6.1716, 6.1574, 6.0570, 6.0431, 6.1558, 6.2958,\n",
      "        6.1279, 6.2278, 6.2495, 6.2044, 6.1293, 6.2301, 6.2446, 6.1507, 6.0985,\n",
      "        6.0923, 5.9854, 6.0576], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.994913  [173724/5599865]\n",
      "average loss by time tensor([5.8648, 5.9044, 5.9195, 6.0419, 5.6951, 5.9131, 5.6697, 5.8022, 5.8496,\n",
      "        5.7716, 5.9586, 5.9320, 5.8717, 5.8229, 5.6377, 5.8022, 5.7625, 5.8878,\n",
      "        5.8334, 5.7428, 6.0426, 6.0750, 5.8579, 5.9712, 5.9767, 5.8187, 5.9546,\n",
      "        5.9099, 5.8699, 5.9255, 5.7516, 6.0744, 5.8871, 5.8743, 5.9818, 6.1370,\n",
      "        6.0627, 5.9942, 5.8830, 5.9841, 5.9313, 6.0464, 5.9583, 5.9398, 5.8116,\n",
      "        6.0665, 5.9331, 5.8992, 6.0745, 5.9988, 5.9380, 5.9848, 5.8946, 6.1324,\n",
      "        6.0184, 5.9239, 6.1213, 5.9663, 6.0850, 6.1871, 5.8839, 6.1595, 6.0331,\n",
      "        6.2395, 6.1458, 6.0562, 6.0719, 5.9749, 6.1528, 6.0379, 6.0378, 6.1239,\n",
      "        5.9827, 5.7826, 6.1015, 6.0600, 5.9037, 6.0154, 6.0847, 6.1376, 5.9913,\n",
      "        6.0110, 6.0863, 6.0781, 6.0913, 6.0581, 5.9340, 6.1902, 6.0788, 6.1196,\n",
      "        6.0550, 5.9367, 6.0549, 5.9268, 6.0024, 6.0894, 5.9465, 5.9135, 6.2024,\n",
      "        6.1557, 6.2132, 6.1526, 6.2407, 6.1935, 6.0693, 6.0190, 6.0970, 6.2558,\n",
      "        6.0646, 6.1617, 6.1295, 6.0922, 6.0472, 6.1237, 6.1474, 6.0475, 5.9638,\n",
      "        5.9815, 5.8975, 5.9576], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.658049  [186124/5599865]\n",
      "average loss by time tensor([5.4178, 5.5114, 5.5217, 5.6602, 5.3687, 5.6029, 5.4195, 5.5451, 5.5837,\n",
      "        5.5703, 5.7163, 5.6952, 5.6347, 5.6275, 5.4710, 5.5985, 5.5390, 5.6901,\n",
      "        5.6690, 5.5512, 5.8186, 5.8414, 5.6633, 5.7675, 5.7196, 5.6216, 5.6986,\n",
      "        5.6819, 5.6091, 5.6707, 5.5091, 5.7405, 5.5720, 5.5337, 5.6163, 5.7149,\n",
      "        5.6255, 5.6100, 5.5391, 5.6403, 5.5671, 5.6457, 5.5732, 5.5653, 5.4911,\n",
      "        5.6803, 5.5534, 5.5445, 5.6347, 5.6126, 5.5912, 5.6218, 5.5044, 5.7107,\n",
      "        5.6668, 5.6132, 5.8169, 5.6915, 5.7959, 5.8700, 5.6068, 5.8391, 5.7144,\n",
      "        5.8731, 5.8056, 5.7231, 5.7311, 5.6818, 5.8404, 5.7702, 5.7667, 5.8280,\n",
      "        5.7037, 5.5321, 5.8529, 5.8170, 5.6716, 5.7187, 5.7038, 5.7111, 5.6202,\n",
      "        5.6135, 5.7126, 5.7374, 5.7968, 5.7639, 5.6420, 5.8591, 5.7767, 5.8043,\n",
      "        5.8015, 5.7393, 5.8258, 5.7223, 5.7702, 5.8334, 5.6657, 5.6347, 5.8667,\n",
      "        5.6671, 5.7136, 5.6284, 5.6629, 5.6546, 5.5597, 5.4932, 5.5430, 5.6745,\n",
      "        5.4770, 5.5791, 5.6328, 5.6265, 5.5537, 5.6685, 5.6973, 5.5799, 5.5912,\n",
      "        5.5734, 5.4541, 5.5199], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.830089  [198524/5599865]\n",
      "average loss by time tensor([6.7215, 6.8153, 6.8721, 7.0009, 6.6969, 6.9346, 6.6865, 6.7643, 6.8487,\n",
      "        6.8358, 7.0139, 6.9968, 6.9612, 6.9279, 6.7266, 6.8563, 6.7831, 6.9457,\n",
      "        6.8969, 6.7483, 7.0292, 7.0507, 6.8506, 6.9857, 6.9844, 6.8206, 7.0129,\n",
      "        6.9320, 6.8855, 6.9458, 6.7788, 7.1074, 6.8764, 6.8228, 6.9505, 7.1145,\n",
      "        7.0420, 6.9607, 6.9041, 7.0177, 6.9547, 7.0633, 6.9637, 6.9687, 6.8236,\n",
      "        7.0310, 6.8479, 6.8494, 6.9040, 6.8576, 6.7653, 6.8472, 6.7614, 7.0144,\n",
      "        6.8954, 6.7921, 7.0332, 6.8439, 6.9473, 7.0097, 6.7290, 7.0162, 6.8731,\n",
      "        7.0786, 6.9911, 6.9163, 6.9250, 6.7953, 7.0101, 6.9623, 6.9584, 7.0323,\n",
      "        6.8681, 6.5985, 6.9607, 6.8436, 6.6418, 6.7677, 6.7839, 6.8092, 6.7008,\n",
      "        6.7614, 6.8217, 6.8926, 6.9583, 6.8580, 6.6954, 6.9424, 6.7579, 6.8116,\n",
      "        6.7913, 6.7063, 6.8127, 6.7287, 6.8230, 6.8839, 6.6558, 6.6450, 6.8861,\n",
      "        6.6540, 6.7209, 6.6408, 6.7139, 6.7440, 6.5756, 6.5134, 6.5708, 6.7459,\n",
      "        6.5273, 6.6475, 6.6810, 6.6061, 6.5184, 6.6200, 6.6410, 6.5256, 6.4967,\n",
      "        6.5463, 6.4428, 6.5049], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.679615  [210924/5599865]\n",
      "average loss by time tensor([5.6643, 5.7318, 5.7679, 5.8512, 5.5536, 5.7721, 5.4958, 5.5936, 5.6833,\n",
      "        5.6330, 5.7945, 5.7526, 5.6948, 5.6588, 5.4755, 5.5684, 5.5185, 5.6464,\n",
      "        5.5761, 5.4601, 5.7632, 5.7832, 5.5791, 5.6598, 5.6451, 5.5293, 5.6420,\n",
      "        5.6569, 5.5984, 5.6429, 5.4792, 5.7717, 5.5887, 5.5638, 5.6436, 5.7712,\n",
      "        5.6987, 5.6514, 5.5622, 5.6875, 5.6071, 5.7255, 5.5809, 5.6308, 5.4943,\n",
      "        5.7307, 5.5159, 5.4509, 5.5548, 5.4881, 5.4731, 5.5539, 5.4150, 5.6710,\n",
      "        5.5622, 5.4572, 5.6610, 5.5272, 5.6300, 5.7154, 5.5150, 5.7054, 5.5946,\n",
      "        5.7310, 5.6474, 5.5951, 5.6160, 5.5303, 5.6978, 5.6211, 5.6653, 5.7380,\n",
      "        5.5907, 5.3978, 5.7710, 5.7385, 5.5433, 5.6575, 5.6994, 5.7514, 5.6538,\n",
      "        5.7006, 5.8586, 5.8658, 5.9044, 5.7718, 5.6356, 5.8759, 5.7563, 5.7879,\n",
      "        5.7790, 5.7153, 5.8316, 5.7340, 5.8173, 5.8431, 5.6652, 5.6959, 5.9282,\n",
      "        5.7621, 5.8040, 5.7445, 5.8198, 5.8685, 5.7514, 5.6869, 5.7565, 5.9215,\n",
      "        5.7467, 5.8998, 5.8876, 5.8653, 5.8003, 5.9102, 5.8699, 5.7631, 5.7216,\n",
      "        5.7633, 5.6322, 5.6969], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.766836  [223324/5599865]\n",
      "average loss by time tensor([5.2796, 5.3531, 5.4069, 5.5839, 5.2224, 5.4704, 5.1993, 5.3573, 5.4769,\n",
      "        5.4180, 5.6383, 5.5679, 5.4897, 5.4553, 5.2108, 5.3136, 5.2600, 5.4569,\n",
      "        5.4608, 5.3038, 5.6197, 5.6433, 5.3763, 5.5310, 5.5101, 5.3837, 5.5805,\n",
      "        5.5296, 5.4749, 5.5474, 5.3417, 5.6894, 5.5252, 5.4944, 5.5744, 5.7328,\n",
      "        5.6714, 5.5557, 5.4657, 5.5894, 5.5337, 5.6841, 5.5677, 5.6319, 5.4891,\n",
      "        5.7841, 5.5866, 5.5963, 5.6721, 5.5910, 5.5261, 5.5993, 5.4668, 5.7915,\n",
      "        5.6673, 5.5880, 5.8655, 5.6662, 5.7779, 5.9494, 5.6703, 5.9780, 5.8547,\n",
      "        6.0543, 5.9215, 5.8622, 5.8444, 5.7454, 5.9502, 5.8927, 5.9206, 6.0544,\n",
      "        5.9357, 5.7370, 6.1127, 5.9902, 5.8299, 5.9496, 5.9703, 6.0264, 5.8624,\n",
      "        5.9075, 5.9919, 5.9518, 6.1063, 6.0085, 5.8934, 6.1548, 6.0349, 6.1240,\n",
      "        6.0827, 5.9607, 6.1303, 6.0115, 6.1343, 6.1610, 5.9592, 5.9215, 6.2259,\n",
      "        6.0523, 6.1675, 6.0940, 6.0877, 6.1144, 5.9717, 5.9538, 6.0642, 6.2104,\n",
      "        5.9591, 6.0935, 6.1297, 6.1072, 5.9732, 6.1489, 6.1847, 6.0819, 6.0044,\n",
      "        6.0132, 5.8884, 6.0010], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.078817  [235724/5599865]\n",
      "average loss by time tensor([6.7884, 6.9105, 6.9440, 7.0944, 6.7898, 7.0334, 6.7758, 6.9104, 6.9979,\n",
      "        6.9275, 7.1279, 7.0893, 7.0284, 6.9743, 6.7766, 6.8731, 6.8134, 6.9700,\n",
      "        6.9359, 6.7951, 7.0921, 7.1229, 6.8861, 7.0001, 6.9833, 6.8317, 7.0106,\n",
      "        6.9866, 6.9239, 6.9618, 6.7784, 7.1016, 6.9011, 6.8653, 6.9872, 7.1691,\n",
      "        7.1253, 7.0663, 6.9989, 7.1940, 7.1148, 7.3039, 7.1483, 7.1754, 7.0080,\n",
      "        7.2410, 7.0296, 7.0791, 7.1870, 7.1486, 7.0790, 7.1578, 7.0268, 7.2797,\n",
      "        7.1341, 7.0022, 7.2302, 7.0230, 7.1314, 7.2251, 6.9060, 7.1767, 7.0365,\n",
      "        7.2411, 7.2068, 7.1494, 7.1456, 7.0222, 7.2066, 7.1081, 7.1480, 7.2746,\n",
      "        7.1135, 6.8980, 7.2924, 7.1984, 6.9884, 7.1318, 7.1495, 7.2296, 7.1121,\n",
      "        7.1606, 7.2443, 7.2353, 7.2890, 7.1794, 6.9858, 7.2583, 7.1060, 7.1737,\n",
      "        7.1331, 7.0057, 7.1613, 7.0732, 7.1799, 7.1947, 7.0019, 6.9639, 7.3320,\n",
      "        7.1155, 7.2005, 7.1288, 7.1477, 7.1852, 7.0688, 6.9778, 7.0983, 7.2591,\n",
      "        7.0308, 7.2233, 7.2291, 7.2134, 7.0650, 7.2051, 7.2567, 7.1298, 7.0468,\n",
      "        7.0822, 7.0029, 7.1098], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.547287  [248124/5599865]\n",
      "average loss by time tensor([5.6533, 5.7144, 5.7483, 5.8798, 5.5765, 5.8147, 5.5448, 5.6795, 5.7501,\n",
      "        5.6542, 5.8893, 5.8447, 5.7841, 5.7498, 5.5877, 5.6861, 5.6461, 5.7356,\n",
      "        5.6715, 5.4733, 5.7985, 5.8104, 5.6233, 5.7694, 5.7852, 5.6484, 5.8298,\n",
      "        5.8117, 5.7359, 5.7610, 5.5963, 5.9102, 5.7223, 5.7003, 5.8000, 5.9604,\n",
      "        5.8843, 5.7834, 5.6684, 5.8155, 5.7209, 5.8519, 5.7000, 5.7272, 5.5753,\n",
      "        5.8348, 5.5968, 5.6182, 5.7100, 5.6466, 5.6294, 5.7076, 5.5199, 5.7541,\n",
      "        5.6165, 5.5197, 5.7467, 5.5662, 5.7042, 5.8000, 5.5047, 5.5951, 5.4582,\n",
      "        5.6228, 5.5274, 5.4808, 5.4622, 5.2956, 5.4826, 5.4238, 5.4432, 5.5292,\n",
      "        5.3588, 5.1617, 5.4979, 5.4057, 5.2230, 5.3406, 5.3613, 5.4138, 5.3548,\n",
      "        5.3578, 5.4426, 5.4309, 5.4775, 5.3847, 5.2124, 5.4931, 5.3454, 5.4485,\n",
      "        5.3729, 5.2405, 5.3798, 5.2547, 5.3825, 5.4116, 5.2240, 5.2167, 5.4695,\n",
      "        5.2495, 5.3601, 5.2805, 5.3343, 5.3852, 5.2861, 5.2517, 5.3730, 5.5238,\n",
      "        5.2929, 5.4326, 5.4465, 5.4444, 5.3338, 5.4497, 5.4402, 5.3361, 5.2731,\n",
      "        5.2834, 5.1503, 5.2147], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.646068  [260524/5599865]\n",
      "average loss by time tensor([6.3361, 6.4163, 6.4704, 6.5466, 6.2298, 6.4823, 6.2274, 6.4013, 6.5420,\n",
      "        6.4690, 6.6740, 6.6292, 6.5635, 6.4807, 6.2433, 6.3667, 6.3861, 6.4828,\n",
      "        6.4618, 6.3060, 6.6250, 6.6435, 6.4105, 6.5020, 6.5336, 6.3806, 6.5769,\n",
      "        6.5113, 6.4719, 6.4795, 6.3544, 6.6755, 6.4871, 6.4379, 6.5116, 6.6787,\n",
      "        6.6156, 6.4841, 6.3788, 6.5371, 6.4537, 6.6806, 6.5319, 6.6353, 6.5309,\n",
      "        6.8383, 6.5918, 6.6427, 6.7717, 6.7339, 6.6776, 6.7640, 6.6096, 6.8231,\n",
      "        6.7185, 6.5941, 6.8592, 6.6705, 6.8327, 6.9337, 6.5823, 6.9071, 6.7390,\n",
      "        6.9267, 6.7990, 6.7424, 6.7784, 6.7020, 6.9241, 6.8690, 6.8914, 6.9801,\n",
      "        6.7725, 6.5501, 7.0197, 6.8986, 6.6520, 6.7530, 6.7877, 6.8534, 6.7130,\n",
      "        6.7976, 6.9367, 6.9318, 6.9398, 6.7781, 6.6071, 6.8832, 6.7032, 6.8045,\n",
      "        6.7485, 6.5880, 6.7665, 6.6032, 6.7581, 6.7618, 6.5632, 6.5277, 6.8739,\n",
      "        6.6035, 6.7414, 6.6406, 6.6923, 6.7631, 6.6157, 6.5318, 6.6615, 6.8741,\n",
      "        6.6388, 6.8207, 6.7989, 6.7935, 6.6474, 6.8305, 6.7773, 6.6289, 6.5572,\n",
      "        6.5772, 6.4441, 6.5706], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.117261  [272924/5599865]\n",
      "average loss by time tensor([5.9998, 6.1383, 6.1803, 6.3388, 5.9023, 6.2652, 5.8685, 6.0751, 6.2077,\n",
      "        6.0799, 6.3728, 6.3620, 6.3059, 6.2623, 5.9530, 6.0908, 6.0505, 6.2431,\n",
      "        6.1825, 5.9607, 6.3806, 6.3907, 6.0712, 6.1912, 6.2270, 6.0139, 6.2476,\n",
      "        6.2164, 6.2078, 6.1678, 5.9583, 6.3530, 6.0938, 6.0186, 6.0777, 6.2697,\n",
      "        6.2582, 6.1413, 5.9765, 6.1739, 6.0568, 6.3280, 6.0869, 6.1413, 5.9399,\n",
      "        6.2648, 5.9477, 6.0210, 6.1433, 6.0345, 5.9671, 6.0771, 5.8920, 6.2107,\n",
      "        5.9987, 5.8944, 6.2184, 5.9454, 6.1306, 6.3121, 5.9082, 6.3105, 6.1263,\n",
      "        6.3862, 6.2037, 6.1468, 6.1579, 5.9921, 6.2470, 6.1252, 6.1887, 6.3036,\n",
      "        6.0632, 5.8221, 6.2897, 6.1604, 5.9554, 6.1158, 6.0938, 6.1521, 6.0415,\n",
      "        6.0952, 6.1736, 6.1434, 6.1865, 6.0271, 5.8609, 6.1641, 5.9576, 6.1384,\n",
      "        6.0333, 5.8738, 6.0773, 5.9616, 6.1718, 6.2347, 6.0573, 6.0343, 6.3587,\n",
      "        6.0674, 6.2212, 6.0946, 6.1369, 6.2198, 6.0389, 5.9133, 6.0705, 6.2511,\n",
      "        5.9940, 6.2156, 6.1865, 6.1623, 6.0209, 6.2081, 6.2560, 6.0750, 5.9363,\n",
      "        5.9863, 5.8489, 5.9426], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.592539  [285324/5599865]\n",
      "average loss by time tensor([5.5016, 5.5394, 5.6116, 5.8036, 5.4672, 5.7361, 5.4371, 5.5703, 5.6240,\n",
      "        5.5467, 5.7737, 5.7286, 5.6178, 5.5713, 5.3366, 5.4184, 5.3910, 5.4921,\n",
      "        5.4765, 5.3071, 5.6683, 5.6439, 5.3746, 5.5084, 5.5156, 5.3232, 5.5729,\n",
      "        5.5660, 5.5474, 5.5412, 5.3859, 5.7340, 5.5040, 5.4665, 5.5212, 5.6979,\n",
      "        5.6355, 5.5168, 5.3889, 5.5696, 5.4618, 5.7064, 5.5026, 5.5852, 5.4095,\n",
      "        5.7708, 5.4859, 5.5678, 5.6639, 5.5814, 5.5415, 5.6857, 5.4785, 5.7110,\n",
      "        5.5389, 5.4210, 5.6819, 5.4579, 5.6084, 5.7670, 5.4763, 5.8072, 5.6584,\n",
      "        5.7633, 5.6594, 5.6382, 5.6563, 5.5247, 5.7067, 5.5914, 5.6506, 5.7530,\n",
      "        5.5396, 5.4134, 5.7989, 5.7399, 5.5590, 5.6987, 5.6723, 5.7160, 5.6389,\n",
      "        5.6920, 5.7679, 5.7448, 5.8021, 5.6818, 5.5360, 5.7416, 5.5519, 5.6596,\n",
      "        5.5847, 5.4436, 5.5878, 5.4709, 5.5940, 5.6111, 5.4891, 5.4756, 5.7339,\n",
      "        5.5491, 5.6249, 5.5341, 5.5818, 5.6573, 5.5873, 5.4953, 5.6150, 5.7427,\n",
      "        5.5594, 5.7189, 5.6794, 5.6813, 5.5938, 5.7248, 5.7662, 5.6450, 5.5731,\n",
      "        5.5895, 5.4898, 5.6056], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.380466  [297724/5599865]\n",
      "average loss by time tensor([5.1671, 5.2308, 5.3225, 5.4950, 5.0270, 5.3915, 4.9850, 5.1880, 5.3452,\n",
      "        5.1804, 5.5118, 5.4916, 5.4096, 5.4053, 5.0903, 5.2094, 5.1526, 5.3102,\n",
      "        5.2798, 5.1001, 5.5724, 5.5462, 5.2005, 5.3276, 5.3441, 5.1315, 5.4427,\n",
      "        5.3825, 5.3288, 5.3303, 5.0993, 5.5518, 5.2895, 5.2346, 5.3558, 5.5837,\n",
      "        5.5114, 5.3750, 5.2147, 5.4315, 5.2595, 5.5122, 5.2614, 5.3658, 5.1968,\n",
      "        5.5741, 5.1908, 5.2503, 5.3474, 5.2500, 5.1750, 5.2949, 5.0684, 5.3978,\n",
      "        5.2030, 5.0493, 5.4016, 5.1577, 5.3326, 5.4758, 5.0585, 5.4833, 5.2594,\n",
      "        5.4847, 5.3653, 5.3177, 5.3563, 5.1987, 5.4512, 5.3354, 5.4057, 5.5352,\n",
      "        5.2866, 5.0359, 5.5053, 5.3798, 5.1229, 5.2982, 5.3158, 5.3880, 5.2429,\n",
      "        5.3486, 5.4436, 5.4159, 5.4903, 5.3191, 5.1679, 5.4948, 5.2684, 5.3922,\n",
      "        5.3036, 5.1384, 5.3636, 5.2399, 5.4468, 5.4474, 5.3643, 5.4156, 5.7053,\n",
      "        5.4371, 5.5871, 5.4873, 5.5348, 5.6384, 5.5020, 5.4161, 5.6253, 5.8029,\n",
      "        5.5721, 5.7971, 5.7705, 5.7733, 5.6344, 5.8538, 5.8765, 5.7675, 5.6650,\n",
      "        5.7272, 5.5824, 5.7016], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.765114  [310124/5599865]\n",
      "average loss by time tensor([5.8111, 5.8841, 5.9683, 6.1926, 5.7803, 6.1099, 5.7001, 5.8758, 6.0570,\n",
      "        5.8880, 6.2151, 6.1883, 6.0966, 6.0624, 5.7799, 5.8608, 5.8451, 5.9805,\n",
      "        5.9555, 5.7356, 6.1862, 6.2136, 5.8610, 5.9645, 5.9819, 5.7770, 6.0840,\n",
      "        6.0313, 6.0210, 5.9799, 5.7695, 6.2546, 5.9690, 5.8724, 5.9766, 6.2308,\n",
      "        6.1387, 5.9548, 5.8036, 6.0344, 5.9388, 6.2009, 5.9296, 6.0022, 5.7679,\n",
      "        6.1448, 5.7742, 5.8808, 5.9454, 5.8565, 5.7761, 5.8997, 5.6522, 5.9266,\n",
      "        5.7326, 5.6196, 5.9667, 5.7008, 5.9227, 6.0711, 5.5899, 5.9855, 5.6976,\n",
      "        5.9381, 5.7847, 5.7466, 5.7577, 5.5358, 5.7825, 5.6608, 5.7259, 5.8398,\n",
      "        5.5822, 5.3233, 5.8748, 5.7384, 5.4133, 5.5843, 5.5434, 5.6093, 5.4351,\n",
      "        5.5384, 5.6991, 5.6646, 5.7224, 5.5875, 5.3642, 5.7425, 5.5130, 5.7102,\n",
      "        5.5802, 5.3929, 5.6512, 5.5051, 5.7028, 5.6903, 5.4857, 5.4310, 5.7628,\n",
      "        5.4463, 5.5997, 5.4507, 5.4828, 5.5848, 5.4131, 5.3160, 5.4855, 5.6438,\n",
      "        5.3551, 5.6242, 5.5293, 5.5316, 5.3726, 5.6385, 5.6701, 5.5186, 5.3709,\n",
      "        5.4262, 5.2568, 5.4033], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.683649  [322524/5599865]\n",
      "average loss by time tensor([6.3177, 6.4236, 6.5206, 6.6444, 6.2145, 6.6179, 6.1530, 6.3712, 6.5704,\n",
      "        6.4041, 6.7624, 6.7441, 6.6456, 6.6126, 6.2837, 6.3724, 6.3886, 6.5658,\n",
      "        6.5777, 6.3801, 6.8590, 6.8856, 6.5433, 6.6894, 6.6852, 6.4641, 6.7801,\n",
      "        6.7161, 6.6828, 6.6368, 6.4478, 6.9753, 6.7097, 6.6490, 6.7407, 6.9996,\n",
      "        6.9091, 6.7506, 6.5670, 6.8635, 6.7143, 7.0363, 6.6880, 6.7804, 6.5417,\n",
      "        6.9862, 6.5838, 6.7342, 6.7915, 6.7192, 6.6570, 6.8177, 6.5779, 6.9284,\n",
      "        6.6897, 6.5980, 6.9935, 6.7028, 6.9055, 7.0564, 6.6384, 7.0965, 6.7952,\n",
      "        7.0385, 6.8656, 6.8396, 6.8761, 6.6807, 6.9933, 6.8381, 6.9172, 7.0531,\n",
      "        6.7310, 6.4638, 6.9695, 6.8435, 6.5627, 6.6962, 6.6302, 6.7046, 6.5418,\n",
      "        6.6239, 6.7283, 6.7130, 6.7870, 6.5726, 6.3786, 6.7979, 6.5322, 6.7686,\n",
      "        6.6266, 6.4631, 6.7129, 6.5694, 6.7942, 6.7832, 6.5801, 6.5000, 6.8905,\n",
      "        6.5239, 6.7552, 6.6106, 6.6473, 6.7926, 6.6266, 6.4911, 6.7025, 6.9190,\n",
      "        6.6015, 6.8548, 6.7333, 6.7360, 6.5326, 6.7807, 6.8119, 6.6321, 6.4949,\n",
      "        6.5633, 6.4410, 6.5618], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.779340  [334924/5599865]\n",
      "average loss by time tensor([6.5780, 6.6646, 6.7596, 6.8903, 6.4233, 6.8151, 6.4102, 6.6437, 6.8064,\n",
      "        6.6109, 6.9606, 6.9842, 6.8845, 6.8381, 6.5311, 6.6024, 6.6112, 6.7212,\n",
      "        6.7058, 6.4916, 6.9684, 6.9771, 6.6026, 6.7410, 6.7954, 6.5786, 6.8704,\n",
      "        6.8140, 6.7429, 6.6645, 6.4742, 6.9270, 6.6882, 6.6323, 6.7056, 6.9341,\n",
      "        6.8568, 6.7146, 6.5641, 6.8386, 6.6723, 6.9481, 6.6354, 6.7245, 6.4639,\n",
      "        6.9733, 6.4999, 6.6900, 6.7494, 6.6669, 6.5860, 6.7424, 6.4855, 6.8668,\n",
      "        6.6255, 6.5010, 6.9060, 6.5948, 6.8188, 6.9855, 6.4698, 6.9627, 6.6919,\n",
      "        6.9709, 6.7843, 6.8062, 6.8309, 6.5901, 6.8796, 6.7198, 6.8328, 6.9776,\n",
      "        6.6427, 6.3605, 7.0131, 6.9095, 6.5992, 6.8056, 6.7739, 6.8870, 6.7458,\n",
      "        6.8622, 6.9690, 6.9712, 7.0393, 6.8615, 6.6242, 7.0620, 6.7450, 6.9667,\n",
      "        6.8076, 6.5681, 6.8597, 6.6806, 6.9219, 6.9264, 6.7513, 6.7057, 7.1146,\n",
      "        6.7287, 6.9804, 6.8601, 6.8308, 6.9408, 6.8319, 6.6834, 6.9164, 7.1462,\n",
      "        6.8460, 7.1514, 6.9849, 6.9847, 6.7838, 7.0704, 7.0632, 6.8948, 6.7116,\n",
      "        6.8022, 6.6697, 6.7893], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.848294  [347324/5599865]\n",
      "average loss by time tensor([6.6386, 6.7300, 6.8535, 7.0198, 6.4838, 6.9289, 6.4213, 6.6709, 6.9223,\n",
      "        6.6831, 7.1711, 7.1592, 7.0441, 7.0252, 6.6669, 6.7781, 6.7886, 6.9185,\n",
      "        6.9333, 6.7018, 7.2986, 7.3278, 6.8744, 6.9793, 6.9916, 6.6599, 7.0270,\n",
      "        6.9575, 6.9151, 6.8069, 6.5671, 7.1911, 6.8085, 6.7726, 6.8543, 7.2003,\n",
      "        7.0872, 6.9142, 6.7360, 7.0469, 6.8522, 7.2511, 6.8734, 6.9920, 6.7191,\n",
      "        7.1957, 6.7253, 6.8985, 6.9447, 6.8359, 6.7802, 6.9291, 6.6122, 6.8981,\n",
      "        6.6427, 6.5608, 6.9617, 6.6203, 6.8480, 7.0276, 6.5370, 7.0526, 6.7585,\n",
      "        7.0566, 6.8563, 6.8945, 6.9786, 6.7232, 7.0093, 6.8319, 6.9609, 7.0975,\n",
      "        6.7737, 6.5131, 7.1541, 7.0223, 6.6843, 6.8853, 6.8072, 6.9055, 6.7334,\n",
      "        6.8525, 6.9937, 6.9771, 7.0551, 6.8485, 6.5589, 6.9120, 6.6061, 6.8560,\n",
      "        6.7400, 6.5006, 6.7976, 6.6338, 6.9024, 6.9173, 6.7199, 6.6763, 7.0489,\n",
      "        6.6654, 6.8884, 6.6795, 6.7415, 6.8810, 6.7227, 6.5119, 6.7565, 6.9224,\n",
      "        6.6016, 6.9223, 6.7982, 6.8549, 6.6429, 6.9661, 7.0021, 6.8493, 6.6833,\n",
      "        6.7529, 6.6312, 6.7649], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.232868  [359724/5599865]\n",
      "average loss by time tensor([5.9917, 6.0788, 6.2086, 6.3468, 5.8798, 6.3153, 5.8468, 6.0345, 6.2818,\n",
      "        6.0410, 6.4864, 6.4826, 6.3400, 6.3099, 5.9069, 5.9986, 6.0081, 6.1359,\n",
      "        6.1576, 5.9577, 6.4496, 6.4542, 6.0819, 6.1859, 6.1804, 5.9366, 6.2735,\n",
      "        6.2250, 6.1685, 6.0783, 5.8789, 6.3856, 6.0858, 6.0157, 6.0956, 6.3991,\n",
      "        6.3189, 6.1523, 6.0006, 6.2809, 6.1483, 6.5211, 6.1632, 6.2305, 5.9288,\n",
      "        6.3790, 5.8990, 6.1143, 6.1770, 6.1044, 6.0130, 6.1943, 5.9066, 6.2614,\n",
      "        5.9917, 5.8957, 6.2884, 5.9265, 6.1843, 6.3674, 5.8666, 6.3751, 6.0932,\n",
      "        6.3596, 6.1580, 6.2192, 6.3093, 6.0856, 6.3765, 6.2599, 6.3748, 6.4899,\n",
      "        6.1956, 6.0092, 6.5015, 6.3790, 6.0785, 6.2929, 6.1934, 6.2973, 6.1567,\n",
      "        6.2596, 6.3741, 6.3602, 6.4901, 6.3101, 6.1206, 6.5846, 6.2874, 6.5157,\n",
      "        6.3582, 6.1256, 6.3601, 6.1973, 6.4606, 6.4129, 6.2058, 6.1297, 6.4866,\n",
      "        6.0996, 6.3623, 6.2069, 6.2309, 6.3618, 6.2494, 6.0698, 6.3258, 6.5240,\n",
      "        6.2550, 6.5848, 6.4518, 6.4957, 6.3120, 6.5801, 6.6213, 6.4538, 6.3146,\n",
      "        6.3835, 6.2899, 6.5091], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.175596  [372124/5599865]\n",
      "average loss by time tensor([6.0568, 6.0946, 6.3096, 6.4686, 5.8817, 6.3729, 5.8438, 6.0022, 6.2688,\n",
      "        5.9832, 6.4887, 6.4750, 6.2916, 6.2298, 5.8096, 5.8965, 5.9428, 6.0623,\n",
      "        6.0489, 5.8122, 6.4470, 6.4437, 5.9759, 6.1751, 6.2115, 5.9290, 6.3444,\n",
      "        6.2480, 6.2031, 6.0930, 5.8587, 6.4182, 6.0480, 5.9529, 6.0424, 6.4120,\n",
      "        6.3397, 6.1509, 5.9370, 6.3242, 6.1339, 6.5125, 6.0778, 6.1629, 5.8631,\n",
      "        6.4693, 5.9447, 6.1482, 6.1742, 6.0765, 6.0015, 6.2171, 5.8965, 6.3204,\n",
      "        5.9826, 5.9403, 6.4301, 5.9945, 6.2798, 6.4765, 5.9066, 6.6194, 6.2450,\n",
      "        6.6215, 6.3717, 6.3799, 6.4254, 6.0742, 6.4124, 6.2424, 6.3763, 6.5115,\n",
      "        6.0715, 5.8487, 6.4912, 6.3140, 5.9277, 6.1190, 6.0378, 6.1530, 6.0077,\n",
      "        6.1300, 6.2971, 6.2606, 6.3540, 6.1294, 5.9406, 6.4000, 6.0395, 6.3602,\n",
      "        6.1710, 5.9488, 6.2187, 5.9763, 6.2782, 6.2577, 6.0010, 5.9777, 6.4578,\n",
      "        6.0072, 6.2963, 6.1326, 6.1325, 6.3441, 6.1554, 5.9353, 6.2442, 6.4299,\n",
      "        6.0647, 6.4355, 6.2709, 6.3179, 6.0868, 6.3825, 6.3844, 6.1778, 5.9603,\n",
      "        6.0126, 5.9097, 6.0389], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.769410  [384524/5599865]\n",
      "average loss by time tensor([5.6386, 5.7439, 5.9431, 6.1353, 5.5557, 6.0978, 5.5263, 5.7145, 5.9948,\n",
      "        5.6274, 6.1789, 6.1427, 5.9864, 5.9461, 5.5133, 5.5329, 5.5501, 5.7061,\n",
      "        5.7448, 5.5581, 6.2961, 6.2794, 5.6655, 5.8308, 5.8466, 5.5100, 5.9951,\n",
      "        5.8840, 5.8630, 5.6945, 5.4571, 6.1161, 5.6704, 5.5721, 5.6666, 6.0417,\n",
      "        5.9123, 5.6554, 5.4096, 5.8630, 5.6340, 6.1584, 5.6786, 5.8513, 5.5559,\n",
      "        6.1583, 5.5350, 5.8073, 5.8455, 5.7560, 5.6897, 5.9372, 5.6182, 6.0292,\n",
      "        5.6850, 5.6234, 6.1277, 5.6872, 5.9588, 6.1570, 5.5202, 6.1974, 5.7625,\n",
      "        6.1099, 5.8347, 5.8368, 5.8380, 5.4940, 5.8444, 5.6883, 5.8314, 5.9830,\n",
      "        5.5252, 5.3113, 5.9815, 5.8017, 5.4074, 5.6736, 5.5647, 5.7020, 5.5031,\n",
      "        5.6820, 5.8093, 5.7621, 5.8724, 5.5981, 5.4028, 5.8820, 5.4701, 5.7801,\n",
      "        5.5493, 5.3525, 5.6807, 5.4942, 5.8444, 5.7813, 5.5571, 5.5144, 6.0120,\n",
      "        5.5046, 5.7950, 5.5834, 5.6073, 5.8780, 5.7203, 5.5602, 5.8668, 6.0876,\n",
      "        5.6700, 6.1215, 5.8837, 5.9741, 5.6411, 6.0110, 6.0615, 5.8230, 5.6015,\n",
      "        5.6816, 5.5349, 5.7063], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.890276  [396924/5599865]\n",
      "average loss by time tensor([5.7236, 5.7782, 5.9266, 6.1045, 5.6198, 6.0391, 5.5860, 5.7172, 5.9561,\n",
      "        5.7101, 6.2125, 6.1713, 5.9774, 5.9516, 5.6272, 5.6581, 5.7312, 5.7946,\n",
      "        5.8253, 5.6480, 6.2581, 6.2538, 5.7712, 5.8901, 5.9197, 5.6798, 6.0803,\n",
      "        6.0069, 5.9836, 5.8594, 5.7069, 6.1792, 5.8095, 5.7913, 5.8686, 6.2366,\n",
      "        6.1048, 5.9117, 5.7324, 6.1277, 5.9014, 6.3443, 5.8606, 5.9688, 5.6637,\n",
      "        6.1791, 5.5758, 5.7819, 5.8112, 5.7212, 5.6651, 5.9007, 5.6172, 5.9762,\n",
      "        5.6718, 5.6616, 6.1288, 5.7128, 5.9637, 6.1607, 5.6486, 6.2096, 5.8322,\n",
      "        6.1936, 5.9272, 5.9265, 5.9754, 5.6942, 6.0253, 5.8404, 6.0410, 6.1830,\n",
      "        5.7164, 5.5707, 6.2011, 6.0440, 5.6358, 5.8454, 5.7102, 5.8240, 5.6399,\n",
      "        5.8041, 5.9786, 5.9322, 6.0437, 5.8193, 5.6196, 6.0427, 5.6799, 5.9657,\n",
      "        5.7461, 5.5884, 5.8404, 5.6982, 6.0424, 5.9747, 5.7396, 5.7012, 6.1482,\n",
      "        5.7248, 6.0328, 5.8329, 5.8325, 6.0356, 5.8406, 5.7074, 5.9711, 6.2478,\n",
      "        5.8480, 6.2390, 5.9718, 6.0417, 5.8137, 6.1318, 6.1714, 5.9782, 5.7839,\n",
      "        5.8110, 5.7330, 5.8376], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.719966  [409324/5599865]\n",
      "average loss by time tensor([6.6788, 6.7176, 6.9296, 7.1052, 6.5545, 7.0614, 6.4931, 6.6874, 6.9588,\n",
      "        6.6601, 7.2361, 7.2148, 7.0110, 6.9427, 6.5610, 6.5827, 6.6195, 6.6774,\n",
      "        6.6872, 6.4749, 7.1388, 7.1019, 6.5399, 6.6415, 6.7123, 6.4602, 6.8855,\n",
      "        6.8271, 6.8192, 6.6519, 6.4784, 7.1060, 6.6433, 6.6009, 6.6543, 7.0826,\n",
      "        6.9595, 6.6776, 6.4986, 6.9095, 6.6482, 7.1732, 6.6526, 6.8381, 6.5354,\n",
      "        7.1875, 6.5332, 6.7089, 6.7348, 6.6469, 6.6106, 6.8329, 6.5153, 6.8964,\n",
      "        6.5437, 6.5135, 6.9813, 6.5563, 6.7972, 6.9978, 6.4336, 7.0370, 6.6007,\n",
      "        6.9755, 6.6520, 6.7259, 6.7307, 6.4194, 6.7563, 6.5566, 6.7675, 6.9694,\n",
      "        6.4539, 6.2368, 7.0146, 6.7929, 6.3784, 6.6222, 6.4597, 6.6131, 6.4065,\n",
      "        6.5713, 6.7501, 6.7264, 6.8438, 6.5500, 6.4042, 6.9174, 6.4963, 6.8188,\n",
      "        6.6311, 6.3897, 6.6684, 6.4886, 6.8456, 6.7667, 6.5770, 6.5516, 7.0700,\n",
      "        6.5564, 6.8831, 6.6276, 6.5862, 6.8711, 6.6035, 6.4774, 6.7221, 7.0005,\n",
      "        6.5649, 6.9902, 6.7128, 6.8403, 6.5822, 6.9485, 6.9793, 6.7124, 6.4740,\n",
      "        6.5675, 6.3886, 6.4907], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.899810  [421724/5599865]\n",
      "average loss by time tensor([6.7988, 6.8612, 7.0522, 7.2121, 6.7263, 7.1910, 6.6537, 6.8406, 7.0682,\n",
      "        6.8206, 7.3195, 7.3003, 7.1226, 7.0786, 6.7200, 6.7355, 6.7305, 6.8144,\n",
      "        6.8355, 6.6612, 7.3193, 7.2511, 6.7026, 6.7725, 6.7983, 6.6182, 7.0181,\n",
      "        6.9181, 6.8526, 6.7489, 6.6010, 7.2321, 6.8440, 6.7934, 6.8526, 7.2774,\n",
      "        7.1597, 6.9040, 6.7599, 7.0878, 6.8527, 7.3466, 6.8185, 6.9949, 6.6988,\n",
      "        7.2889, 6.6973, 6.9077, 6.9659, 6.8857, 6.8634, 7.0269, 6.7538, 7.0835,\n",
      "        6.7913, 6.7032, 7.1256, 6.7324, 6.9807, 7.2372, 6.6604, 7.2770, 6.7948,\n",
      "        7.1978, 6.8817, 6.9514, 6.9859, 6.7148, 7.0313, 6.8722, 7.0973, 7.3006,\n",
      "        6.8053, 6.5961, 7.3040, 7.1148, 6.7565, 6.9682, 6.8816, 6.9507, 6.8179,\n",
      "        6.9340, 7.1463, 7.1182, 7.2119, 6.9251, 6.7511, 7.1740, 6.8059, 7.0857,\n",
      "        6.8514, 6.6202, 6.8497, 6.6933, 7.0206, 6.9759, 6.8002, 6.7541, 7.1914,\n",
      "        6.7122, 6.9723, 6.8064, 6.7791, 6.9680, 6.7673, 6.5549, 6.7115, 6.9463,\n",
      "        6.6035, 6.9493, 6.6981, 6.7853, 6.5682, 6.8794, 6.8988, 6.6974, 6.5375,\n",
      "        6.5786, 6.3976, 6.4870], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.270752  [434124/5599865]\n",
      "average loss by time tensor([5.9518, 6.0083, 6.0967, 6.3394, 5.8236, 6.2759, 5.7832, 5.9945, 6.2377,\n",
      "        5.9887, 6.5401, 6.5772, 6.3686, 6.2947, 5.9809, 6.0213, 6.0500, 6.1226,\n",
      "        6.1139, 5.8830, 6.4783, 6.4535, 5.9465, 6.0161, 6.0734, 5.8335, 6.2147,\n",
      "        6.1259, 6.0798, 5.9905, 5.8510, 6.3625, 5.9751, 5.9182, 5.9867, 6.3734,\n",
      "        6.2287, 6.0293, 5.9279, 6.2396, 6.0303, 6.5732, 6.0586, 6.1898, 5.9831,\n",
      "        6.5752, 5.9986, 6.1281, 6.1256, 6.0465, 5.9882, 6.1989, 5.9856, 6.3441,\n",
      "        6.0927, 6.0168, 6.4949, 6.1279, 6.3751, 6.6404, 6.1351, 6.7601, 6.3522,\n",
      "        6.7099, 6.4224, 6.4588, 6.5270, 6.2361, 6.5237, 6.3746, 6.5723, 6.8128,\n",
      "        6.3579, 6.1248, 6.8011, 6.6028, 6.2443, 6.4792, 6.3977, 6.5133, 6.3740,\n",
      "        6.4809, 6.6214, 6.5542, 6.7003, 6.4419, 6.2297, 6.6275, 6.2732, 6.5649,\n",
      "        6.4073, 6.1762, 6.4604, 6.2896, 6.6373, 6.5098, 6.3846, 6.3054, 6.7015,\n",
      "        6.2362, 6.4797, 6.3303, 6.2975, 6.4892, 6.2832, 6.0948, 6.3394, 6.5737,\n",
      "        6.2041, 6.5575, 6.2712, 6.3346, 6.0847, 6.4433, 6.4295, 6.1854, 5.9953,\n",
      "        6.0776, 5.9797, 6.1269], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.870179  [446524/5599865]\n",
      "average loss by time tensor([5.8126, 5.8383, 5.9215, 6.0854, 5.6886, 6.0940, 5.7022, 5.8657, 6.0284,\n",
      "        5.8167, 6.2582, 6.2571, 6.0502, 6.0382, 5.7876, 5.7930, 5.8091, 5.8509,\n",
      "        5.8865, 5.7216, 6.2592, 6.1938, 5.7531, 5.8248, 5.8408, 5.6885, 5.9657,\n",
      "        5.8717, 5.8743, 5.7707, 5.6181, 6.1119, 5.7999, 5.7585, 5.7910, 6.1308,\n",
      "        5.9965, 5.8235, 5.7137, 6.0013, 5.8143, 6.3125, 5.8142, 5.8889, 5.7325,\n",
      "        6.2497, 5.6823, 5.8586, 5.8526, 5.7853, 5.8045, 5.9663, 5.7184, 5.9677,\n",
      "        5.7604, 5.7268, 6.1240, 5.7975, 5.9699, 6.1449, 5.7102, 6.1866, 5.8109,\n",
      "        6.0529, 5.8219, 5.8501, 5.9090, 5.6943, 5.8604, 5.7659, 5.9028, 6.1322,\n",
      "        5.7312, 5.5459, 6.1213, 5.8933, 5.6760, 5.8231, 5.7426, 5.8048, 5.6994,\n",
      "        5.8203, 5.9054, 5.8742, 5.9847, 5.7998, 5.6794, 5.9861, 5.7246, 5.9359,\n",
      "        5.8067, 5.6295, 5.8319, 5.6963, 5.8862, 5.8251, 5.7155, 5.7057, 6.0376,\n",
      "        5.7061, 5.8654, 5.7888, 5.7696, 5.8898, 5.7854, 5.6541, 5.8094, 5.9934,\n",
      "        5.7448, 6.0340, 5.8277, 5.9098, 5.7787, 6.0021, 6.0411, 5.8967, 5.7679,\n",
      "        5.8572, 5.7864, 5.8686], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.441333  [458924/5599865]\n",
      "average loss by time tensor([5.5138, 5.5912, 5.7218, 5.8654, 5.3984, 5.8213, 5.3527, 5.5194, 5.7227,\n",
      "        5.5022, 6.0035, 6.0081, 5.7423, 5.7234, 5.3946, 5.3995, 5.4551, 5.5187,\n",
      "        5.5274, 5.3542, 5.9082, 5.8997, 5.3997, 5.4779, 5.5017, 5.2572, 5.5580,\n",
      "        5.4812, 5.4867, 5.3511, 5.1871, 5.6086, 5.3591, 5.3232, 5.3921, 5.6996,\n",
      "        5.5489, 5.3909, 5.2387, 5.5090, 5.3457, 5.7578, 5.3275, 5.4369, 5.2452,\n",
      "        5.7847, 5.2888, 5.4822, 5.4849, 5.4502, 5.4173, 5.5348, 5.2617, 5.5411,\n",
      "        5.2831, 5.2453, 5.6144, 5.2820, 5.4987, 5.5905, 5.0970, 5.6094, 5.2711,\n",
      "        5.5172, 5.3015, 5.3271, 5.3552, 5.1482, 5.3837, 5.2625, 5.4057, 5.6297,\n",
      "        5.2200, 5.0007, 5.6273, 5.4410, 5.1906, 5.3782, 5.2899, 5.3927, 5.2125,\n",
      "        5.3475, 5.4322, 5.4418, 5.5304, 5.3319, 5.1709, 5.5432, 5.1980, 5.4654,\n",
      "        5.3617, 5.1205, 5.4147, 5.2506, 5.5164, 5.4679, 5.2908, 5.2461, 5.5876,\n",
      "        5.1989, 5.4326, 5.3131, 5.3464, 5.5263, 5.3593, 5.1641, 5.4458, 5.6213,\n",
      "        5.3421, 5.6857, 5.4736, 5.5333, 5.3676, 5.5617, 5.5417, 5.4301, 5.2824,\n",
      "        5.3668, 5.2417, 5.3595], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.954161  [471324/5599865]\n",
      "average loss by time tensor([5.8500, 5.9488, 6.0912, 6.2395, 5.7649, 6.1962, 5.7110, 5.8844, 6.0731,\n",
      "        5.8561, 6.3293, 6.3153, 6.1410, 6.0925, 5.7671, 5.7880, 5.8602, 5.9090,\n",
      "        5.9692, 5.7816, 6.4257, 6.4060, 5.9451, 6.0383, 6.0694, 5.8395, 6.1563,\n",
      "        6.1305, 6.1425, 6.0576, 5.8599, 6.3732, 6.0394, 5.9899, 6.0140, 6.3154,\n",
      "        6.1809, 6.0111, 5.8177, 6.1567, 5.9590, 6.4829, 5.9590, 6.1036, 5.8730,\n",
      "        6.4299, 5.8291, 6.0371, 5.9915, 5.9008, 5.8605, 6.0317, 5.7014, 6.0391,\n",
      "        5.7930, 5.7393, 6.1253, 5.7620, 6.0414, 6.1935, 5.7356, 6.2543, 5.9082,\n",
      "        6.1840, 5.9520, 5.9945, 6.0039, 5.7184, 5.9793, 5.8672, 6.0117, 6.1490,\n",
      "        5.7643, 5.5586, 6.1836, 6.0257, 5.6809, 5.9232, 5.7824, 5.8650, 5.7114,\n",
      "        5.8864, 6.0195, 5.9772, 6.0179, 5.8042, 5.6037, 5.9894, 5.6603, 5.9776,\n",
      "        5.7991, 5.5645, 5.8672, 5.7344, 5.9909, 5.8969, 5.7994, 5.7799, 6.0604,\n",
      "        5.7338, 5.9473, 5.8319, 5.8230, 5.9983, 5.8935, 5.7055, 5.9228, 6.0204,\n",
      "        5.7937, 6.0546, 5.8826, 5.9160, 5.7573, 5.9958, 6.0456, 5.9389, 5.7441,\n",
      "        5.8281, 5.7491, 5.8444], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 7.012363  [483724/5599865]\n",
      "average loss by time tensor([6.6473, 6.7185, 6.8864, 6.9775, 6.4735, 6.9375, 6.3762, 6.5732, 6.8192,\n",
      "        6.5556, 7.0487, 7.0791, 6.9162, 6.8771, 6.5032, 6.5059, 6.5968, 6.6539,\n",
      "        6.7162, 6.5048, 7.1360, 7.1198, 6.6881, 6.8116, 6.8727, 6.6069, 6.9827,\n",
      "        6.9158, 6.9288, 6.7591, 6.5790, 7.0185, 6.7592, 6.6985, 6.7696, 7.0927,\n",
      "        7.0328, 6.8642, 6.7254, 7.0830, 6.8975, 7.4082, 6.9467, 7.1543, 6.8979,\n",
      "        7.4587, 6.8371, 7.1267, 7.0471, 6.9828, 6.9369, 7.1291, 6.8099, 7.2226,\n",
      "        6.8804, 6.7811, 7.2626, 6.8443, 7.0885, 7.2513, 6.7572, 7.3447, 6.9909,\n",
      "        7.3072, 7.1078, 7.1652, 7.2072, 6.8707, 7.1548, 7.0570, 7.2044, 7.3590,\n",
      "        6.9772, 6.7657, 7.3861, 7.2932, 6.9266, 7.1767, 7.0501, 7.1440, 6.9763,\n",
      "        7.2058, 7.3223, 7.2417, 7.3383, 7.0894, 6.8846, 7.3173, 7.0114, 7.3232,\n",
      "        7.1188, 6.8621, 7.1203, 6.9357, 7.3093, 7.2698, 7.1147, 7.0882, 7.4183,\n",
      "        7.0194, 7.3043, 7.1447, 7.1233, 7.3459, 7.1898, 6.9560, 7.2146, 7.3675,\n",
      "        7.0793, 7.4306, 7.1794, 7.2491, 7.0212, 7.3250, 7.3739, 7.2028, 6.9832,\n",
      "        7.0515, 6.9182, 7.0355], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.768944  [496124/5599865]\n",
      "average loss by time tensor([5.8064, 5.8851, 6.0270, 6.1159, 5.6696, 6.1292, 5.6457, 5.8153, 6.0504,\n",
      "        5.7814, 6.2048, 6.1910, 6.0594, 6.0311, 5.6922, 5.7065, 5.7745, 5.8604,\n",
      "        5.9223, 5.6933, 6.2751, 6.2290, 5.7874, 5.9044, 5.9313, 5.6665, 6.0822,\n",
      "        6.0209, 5.9936, 5.8077, 5.6744, 6.1637, 5.7958, 5.7321, 5.7663, 6.1086,\n",
      "        6.0135, 5.7868, 5.5570, 5.9684, 5.7976, 6.2737, 5.7618, 5.8818, 5.5598,\n",
      "        6.1091, 5.5159, 5.8099, 5.7552, 5.6678, 5.6630, 5.9179, 5.6052, 5.9758,\n",
      "        5.6232, 5.5711, 5.9862, 5.5568, 5.8298, 5.9424, 5.4553, 5.9867, 5.6186,\n",
      "        5.8546, 5.6010, 5.6349, 5.6961, 5.4131, 5.7582, 5.6137, 5.7597, 5.9045,\n",
      "        5.5028, 5.3604, 5.9291, 5.8151, 5.4771, 5.7393, 5.6279, 5.7563, 5.5983,\n",
      "        5.7657, 5.8981, 5.8627, 5.9558, 5.7002, 5.4724, 5.8956, 5.5253, 5.8607,\n",
      "        5.6313, 5.3536, 5.6500, 5.4746, 5.8387, 5.7779, 5.5895, 5.5260, 5.8812,\n",
      "        5.4253, 5.7382, 5.5581, 5.5712, 5.7671, 5.6271, 5.4487, 5.7574, 5.9178,\n",
      "        5.6100, 5.9574, 5.6956, 5.7880, 5.5072, 5.8375, 5.8306, 5.6294, 5.3780,\n",
      "        5.4483, 5.3707, 5.5173], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.875121  [508524/5599865]\n",
      "average loss by time tensor([4.6512, 4.6855, 4.8285, 4.9367, 4.6682, 4.9548, 4.6276, 4.6823, 4.8624,\n",
      "        4.6804, 5.0112, 4.9695, 4.8666, 4.8374, 4.6175, 4.6634, 4.6979, 4.7150,\n",
      "        4.7696, 4.6604, 5.0085, 5.0547, 4.7731, 4.8791, 4.9057, 4.7311, 4.9216,\n",
      "        4.8730, 4.8759, 4.7965, 4.7085, 4.9864, 4.7821, 4.7536, 4.7747, 5.0105,\n",
      "        4.9902, 4.8963, 4.8392, 5.0032, 4.8809, 5.1284, 4.9168, 5.0065, 4.8957,\n",
      "        5.1124, 4.8712, 4.9866, 4.9801, 4.9387, 4.8829, 4.9455, 4.8236, 4.9204,\n",
      "        4.7659, 4.7526, 4.9572, 4.7270, 4.8115, 4.9151, 4.6642, 4.9771, 4.7924,\n",
      "        4.9925, 4.8622, 4.8827, 4.9078, 4.7177, 4.8513, 4.7619, 4.9039, 4.9874,\n",
      "        4.7171, 4.6878, 5.0400, 4.9464, 4.7781, 4.9289, 4.8816, 4.9801, 4.8861,\n",
      "        4.9929, 5.0558, 5.0369, 5.1061, 4.9363, 4.8146, 5.0747, 4.8236, 5.0145,\n",
      "        4.8763, 4.8092, 4.8484, 4.7259, 4.9107, 4.8694, 4.7797, 4.7639, 5.0033,\n",
      "        4.7865, 4.9616, 4.8610, 4.8640, 4.9947, 4.9157, 4.8347, 4.9631, 5.0556,\n",
      "        4.8630, 5.0999, 4.9666, 5.0295, 4.8828, 5.0755, 5.0673, 4.9311, 4.7831,\n",
      "        4.7965, 4.7425, 4.7811], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.675095  [520924/5599865]\n",
      "average loss by time tensor([5.4809, 5.5171, 5.6987, 5.8553, 5.3571, 5.8140, 5.3050, 5.4174, 5.6766,\n",
      "        5.4083, 5.8856, 5.8710, 5.6913, 5.6262, 5.3317, 5.3646, 5.4589, 5.4934,\n",
      "        5.5866, 5.4332, 6.0524, 6.0365, 5.5280, 5.6410, 5.6806, 5.4754, 5.8706,\n",
      "        5.8014, 5.7936, 5.6370, 5.5558, 6.0032, 5.6864, 5.5935, 5.6222, 5.9441,\n",
      "        5.8699, 5.6994, 5.5597, 5.8852, 5.6555, 6.0654, 5.6378, 5.7693, 5.5738,\n",
      "        5.9994, 5.4921, 5.6992, 5.6723, 5.6157, 5.5953, 5.7628, 5.5640, 5.7973,\n",
      "        5.5810, 5.5524, 5.8515, 5.5501, 5.7365, 5.8731, 5.5062, 5.9523, 5.6463,\n",
      "        5.9052, 5.7138, 5.8076, 5.8598, 5.5948, 5.8214, 5.6918, 5.8701, 5.9903,\n",
      "        5.5835, 5.4266, 5.9676, 5.8578, 5.5306, 5.7486, 5.6074, 5.7231, 5.6357,\n",
      "        5.7487, 5.8724, 5.8664, 5.9494, 5.7192, 5.6135, 5.9372, 5.6220, 5.8561,\n",
      "        5.5746, 5.3912, 5.6221, 5.4866, 5.7492, 5.6592, 5.5168, 5.4553, 5.7462,\n",
      "        5.4232, 5.6546, 5.4961, 5.4681, 5.6955, 5.5696, 5.4711, 5.6690, 5.8008,\n",
      "        5.5597, 5.8367, 5.6265, 5.7003, 5.4985, 5.7680, 5.7834, 5.6400, 5.5346,\n",
      "        5.5930, 5.5276, 5.6148], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.203508  [533324/5599865]\n",
      "average loss by time tensor([5.1033, 5.1223, 5.2371, 5.3386, 5.0496, 5.3876, 5.0465, 5.1545, 5.3086,\n",
      "        5.0984, 5.4221, 5.4109, 5.2766, 5.2355, 5.0532, 5.0366, 5.1013, 5.1271,\n",
      "        5.1382, 5.0314, 5.4361, 5.4029, 5.0531, 5.1205, 5.1413, 5.0272, 5.2920,\n",
      "        5.2464, 5.2696, 5.1465, 5.0679, 5.3939, 5.1233, 5.0885, 5.1233, 5.4056,\n",
      "        5.2921, 5.1277, 5.0514, 5.3005, 5.1397, 5.5194, 5.1354, 5.2303, 5.0926,\n",
      "        5.4567, 5.0558, 5.2058, 5.1935, 5.1299, 5.1113, 5.2340, 5.0687, 5.2783,\n",
      "        5.0877, 5.0809, 5.3498, 5.1131, 5.2484, 5.3516, 5.0176, 5.3744, 5.0984,\n",
      "        5.3476, 5.1633, 5.2517, 5.2850, 5.0784, 5.2120, 5.1182, 5.2863, 5.3890,\n",
      "        5.1013, 5.0090, 5.4317, 5.3117, 5.0975, 5.1843, 5.1298, 5.1737, 5.0852,\n",
      "        5.1705, 5.2499, 5.2019, 5.2905, 5.1061, 5.0148, 5.2348, 5.0176, 5.2015,\n",
      "        5.0670, 4.9501, 5.1053, 5.0390, 5.2773, 5.2001, 5.1530, 5.1449, 5.4023,\n",
      "        5.1339, 5.3162, 5.2098, 5.2077, 5.3625, 5.2689, 5.1676, 5.2871, 5.4033,\n",
      "        5.2006, 5.4636, 5.2601, 5.3731, 5.2432, 5.4179, 5.4082, 5.2755, 5.1443,\n",
      "        5.1766, 5.1054, 5.1511], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.955869  [545724/5599865]\n",
      "average loss by time tensor([5.8276, 5.8250, 5.9433, 6.0890, 5.8116, 6.1322, 5.8459, 5.9137, 6.0278,\n",
      "        5.9004, 6.2091, 6.2057, 6.0578, 6.0544, 5.8287, 5.8190, 5.9034, 5.8632,\n",
      "        5.8699, 5.7684, 6.2013, 6.1390, 5.8003, 5.8408, 5.8280, 5.6737, 5.9127,\n",
      "        5.8244, 5.8272, 5.7745, 5.7117, 5.9839, 5.7697, 5.7106, 5.7372, 6.0210,\n",
      "        5.9898, 5.8651, 5.8083, 6.0556, 5.9472, 6.2801, 5.9422, 6.0927, 5.9453,\n",
      "        6.3292, 5.9768, 6.0665, 6.0345, 5.9686, 5.9537, 6.0558, 5.8943, 6.1095,\n",
      "        5.9570, 5.9803, 6.2663, 6.0186, 6.1256, 6.3177, 5.9635, 6.3280, 6.0863,\n",
      "        6.3148, 6.0727, 6.1313, 6.1780, 5.9349, 6.0201, 5.9425, 5.9975, 6.1049,\n",
      "        5.7718, 5.6289, 6.0082, 5.8956, 5.6512, 5.7923, 5.7192, 5.7954, 5.7299,\n",
      "        5.7951, 5.9335, 5.9034, 5.9986, 5.9181, 5.8276, 6.0674, 5.8660, 6.0840,\n",
      "        5.9444, 5.7865, 5.9593, 5.8886, 6.0770, 5.9647, 5.9301, 5.8991, 6.1174,\n",
      "        5.8522, 6.0165, 5.9101, 5.9010, 6.0363, 5.9322, 5.7728, 5.9404, 6.0973,\n",
      "        5.8556, 6.1744, 5.9363, 5.9873, 5.8412, 6.0574, 6.0944, 5.9965, 5.8473,\n",
      "        5.9356, 5.8848, 5.9740], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.859534  [558124/5599865]\n",
      "average loss by time tensor([5.6788, 5.7645, 5.8944, 6.0379, 5.6713, 6.0745, 5.6216, 5.7802, 5.9929,\n",
      "        5.7899, 6.2180, 6.2363, 6.0499, 5.9942, 5.7353, 5.7191, 5.8012, 5.8347,\n",
      "        5.8946, 5.7165, 6.2721, 6.2611, 5.8119, 5.8887, 5.9136, 5.6610, 6.0142,\n",
      "        5.8976, 5.8875, 5.7868, 5.6265, 6.1151, 5.8037, 5.7827, 5.8134, 6.1270,\n",
      "        5.9910, 5.8126, 5.6990, 5.9800, 5.7923, 6.2734, 5.7728, 5.8367, 5.6943,\n",
      "        6.1676, 5.6514, 5.8283, 5.7874, 5.7676, 5.7209, 5.8664, 5.7027, 5.9794,\n",
      "        5.7476, 5.7033, 6.0589, 5.7811, 5.9024, 6.0649, 5.6773, 6.1376, 5.8487,\n",
      "        6.0552, 5.8691, 5.9077, 5.9341, 5.7140, 5.8468, 5.8092, 5.9012, 6.0616,\n",
      "        5.6689, 5.5128, 6.0435, 5.7863, 5.5890, 5.7327, 5.6407, 5.6935, 5.5984,\n",
      "        5.7399, 5.8357, 5.8218, 5.9425, 5.8479, 5.7493, 6.0218, 5.7848, 5.9789,\n",
      "        5.8575, 5.6540, 5.8723, 5.7474, 6.0287, 5.9375, 5.8717, 5.8431, 6.0804,\n",
      "        5.7724, 5.9661, 5.8898, 5.8690, 6.0115, 5.8261, 5.6843, 5.8574, 6.0351,\n",
      "        5.7737, 6.0559, 5.8005, 5.8572, 5.7004, 5.9512, 5.9628, 5.8468, 5.6607,\n",
      "        5.7336, 5.6875, 5.8079], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.460505  [570524/5599865]\n",
      "average loss by time tensor([5.3497, 5.4202, 5.5682, 5.6473, 5.3103, 5.7021, 5.3194, 5.4036, 5.6016,\n",
      "        5.4674, 5.9050, 5.8867, 5.6948, 5.6862, 5.4207, 5.4269, 5.5150, 5.5351,\n",
      "        5.5513, 5.3665, 5.9231, 5.9122, 5.5262, 5.5865, 5.6274, 5.4071, 5.7532,\n",
      "        5.7060, 5.6878, 5.6000, 5.4890, 5.8528, 5.6115, 5.5502, 5.5533, 5.8034,\n",
      "        5.6910, 5.5646, 5.4353, 5.6776, 5.5523, 5.9099, 5.5368, 5.6163, 5.4470,\n",
      "        5.8791, 5.3686, 5.5558, 5.5370, 5.4882, 5.4502, 5.5783, 5.3092, 5.5468,\n",
      "        5.3529, 5.3277, 5.5756, 5.3384, 5.4849, 5.5592, 5.2394, 5.6755, 5.3848,\n",
      "        5.6001, 5.4553, 5.4726, 5.4859, 5.3137, 5.4930, 5.3735, 5.4826, 5.6708,\n",
      "        5.3776, 5.2141, 5.6989, 5.5573, 5.3368, 5.4840, 5.4005, 5.4930, 5.3770,\n",
      "        5.4515, 5.5180, 5.5228, 5.5711, 5.3982, 5.2547, 5.5258, 5.2758, 5.4693,\n",
      "        5.3186, 5.1604, 5.3593, 5.2221, 5.4444, 5.3724, 5.2818, 5.2607, 5.3709,\n",
      "        5.1013, 5.2997, 5.1813, 5.1439, 5.2896, 5.1891, 5.0400, 5.2821, 5.4106,\n",
      "        5.2118, 5.4881, 5.2494, 5.2866, 5.1154, 5.3430, 5.3793, 5.2744, 5.0744,\n",
      "        5.1392, 5.0781, 5.1689], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.485203  [582924/5599865]\n",
      "average loss by time tensor([5.2723, 5.2654, 5.3475, 5.3949, 5.1699, 5.3879, 5.1887, 5.2795, 5.3807,\n",
      "        5.2727, 5.4823, 5.4245, 5.3440, 5.3174, 5.2037, 5.2128, 5.2388, 5.2970,\n",
      "        5.3248, 5.2526, 5.4961, 5.4706, 5.2763, 5.3121, 5.3218, 5.2215, 5.4009,\n",
      "        5.3531, 5.3605, 5.2732, 5.2321, 5.4227, 5.3639, 5.3347, 5.3611, 5.4439,\n",
      "        5.4137, 5.4010, 5.3019, 5.4619, 5.3740, 5.6412, 5.4182, 5.4411, 5.3346,\n",
      "        5.6077, 5.3139, 5.4496, 5.4650, 5.4377, 5.4118, 5.5013, 5.3628, 5.4857,\n",
      "        5.3342, 5.3546, 5.5191, 5.3775, 5.4676, 5.5077, 5.3947, 5.6944, 5.5254,\n",
      "        5.6978, 5.5766, 5.6036, 5.6361, 5.5149, 5.6054, 5.5832, 5.6662, 5.7594,\n",
      "        5.5594, 5.4836, 5.8013, 5.7192, 5.5448, 5.7298, 5.6806, 5.7294, 5.6526,\n",
      "        5.7431, 5.7588, 5.7280, 5.7444, 5.5885, 5.5282, 5.7342, 5.5607, 5.6973,\n",
      "        5.6172, 5.5065, 5.6435, 5.5285, 5.6778, 5.6414, 5.5648, 5.5141, 5.6489,\n",
      "        5.4967, 5.6282, 5.5807, 5.5569, 5.6600, 5.5873, 5.4469, 5.5949, 5.6529,\n",
      "        5.5365, 5.6502, 5.5479, 5.5941, 5.4736, 5.6237, 5.5974, 5.5591, 5.4401,\n",
      "        5.4717, 5.4072, 5.4477], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.777888  [595324/5599865]\n",
      "average loss by time tensor([4.7158, 4.7771, 4.8236, 4.8854, 4.6823, 4.8952, 4.6878, 4.7553, 4.8758,\n",
      "        4.7306, 4.9258, 4.9140, 4.8342, 4.8061, 4.6652, 4.6577, 4.7408, 4.7436,\n",
      "        4.7472, 4.6310, 4.9191, 4.8868, 4.6888, 4.6984, 4.7160, 4.6222, 4.8177,\n",
      "        4.7998, 4.7915, 4.7038, 4.6245, 4.7936, 4.6851, 4.6436, 4.6504, 4.7931,\n",
      "        4.7462, 4.6871, 4.6044, 4.8016, 4.6901, 4.9390, 4.6788, 4.7131, 4.6133,\n",
      "        4.8302, 4.6022, 4.7346, 4.7213, 4.6992, 4.7008, 4.8012, 4.6526, 4.8102,\n",
      "        4.6352, 4.6159, 4.8410, 4.6781, 4.7925, 4.8892, 4.6477, 4.9185, 4.7929,\n",
      "        4.9607, 4.8560, 4.8718, 4.8774, 4.7428, 4.8770, 4.7897, 4.8775, 4.9593,\n",
      "        4.7236, 4.6457, 4.9333, 4.8561, 4.6905, 4.8305, 4.7299, 4.8036, 4.7201,\n",
      "        4.8173, 4.8778, 4.8726, 4.9463, 4.8225, 4.7005, 4.8632, 4.6486, 4.8299,\n",
      "        4.7135, 4.6415, 4.7707, 4.6606, 4.8601, 4.8052, 4.7566, 4.7381, 4.9085,\n",
      "        4.6858, 4.8194, 4.7216, 4.6967, 4.7951, 4.7526, 4.6402, 4.7868, 4.8554,\n",
      "        4.7262, 4.9419, 4.8311, 4.9137, 4.7878, 4.9369, 4.9662, 4.9016, 4.8260,\n",
      "        4.8515, 4.8365, 4.9291], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.416317  [607724/5599865]\n",
      "average loss by time tensor([5.2144, 5.2415, 5.4178, 5.4996, 5.2447, 5.5852, 5.2215, 5.2650, 5.4879,\n",
      "        5.2651, 5.5635, 5.5580, 5.4604, 5.4272, 5.1859, 5.1619, 5.1640, 5.1860,\n",
      "        5.2918, 5.1741, 5.5647, 5.5913, 5.2627, 5.3579, 5.4075, 5.2486, 5.5067,\n",
      "        5.4916, 5.4921, 5.3609, 5.2696, 5.6017, 5.3642, 5.2954, 5.3514, 5.6752,\n",
      "        5.6365, 5.4725, 5.3265, 5.6423, 5.4764, 5.7871, 5.3783, 5.4721, 5.2396,\n",
      "        5.6503, 5.2248, 5.4278, 5.4039, 5.3214, 5.2999, 5.4673, 5.2647, 5.5051,\n",
      "        5.3256, 5.3643, 5.6188, 5.3491, 5.5157, 5.6378, 5.3533, 5.7222, 5.4895,\n",
      "        5.6599, 5.5551, 5.6052, 5.6316, 5.4522, 5.5926, 5.4957, 5.6365, 5.7029,\n",
      "        5.4238, 5.2988, 5.6738, 5.6079, 5.3571, 5.5384, 5.4297, 5.4706, 5.3499,\n",
      "        5.4791, 5.5231, 5.4942, 5.5794, 5.3955, 5.2266, 5.5138, 5.1900, 5.4704,\n",
      "        5.2544, 5.1162, 5.2642, 5.1772, 5.4540, 5.3938, 5.2984, 5.2624, 5.5280,\n",
      "        5.2238, 5.4544, 5.3160, 5.2526, 5.4588, 5.3761, 5.2499, 5.4491, 5.6025,\n",
      "        5.3682, 5.6702, 5.4246, 5.4974, 5.3047, 5.5311, 5.5498, 5.3896, 5.2125,\n",
      "        5.2571, 5.1949, 5.2426], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.421920  [620124/5599865]\n",
      "average loss by time tensor([6.3724, 6.3990, 6.5224, 6.6010, 6.3063, 6.6004, 6.2751, 6.3685, 6.5663,\n",
      "        6.3473, 6.6945, 6.6840, 6.5779, 6.5596, 6.3404, 6.3314, 6.4054, 6.4135,\n",
      "        6.4794, 6.3612, 6.7678, 6.7249, 6.3696, 6.4147, 6.4537, 6.2667, 6.5653,\n",
      "        6.5374, 6.5692, 6.3991, 6.3486, 6.6856, 6.3861, 6.3518, 6.3730, 6.6586,\n",
      "        6.5507, 6.4043, 6.2971, 6.6227, 6.4225, 6.8174, 6.4377, 6.5707, 6.4146,\n",
      "        6.8020, 6.3589, 6.5662, 6.5122, 6.4711, 6.4240, 6.6082, 6.3859, 6.6231,\n",
      "        6.4243, 6.4090, 6.7189, 6.4238, 6.6035, 6.7769, 6.4219, 6.7663, 6.4938,\n",
      "        6.6941, 6.4716, 6.6059, 6.6650, 6.4365, 6.6101, 6.4875, 6.5790, 6.6861,\n",
      "        6.3921, 6.2806, 6.6414, 6.5100, 6.2956, 6.4434, 6.2888, 6.3592, 6.2668,\n",
      "        6.3745, 6.4802, 6.4560, 6.4904, 6.3000, 6.1976, 6.4389, 6.1954, 6.4137,\n",
      "        6.2475, 6.1385, 6.2634, 6.1396, 6.3830, 6.2925, 6.1770, 6.1442, 6.4277,\n",
      "        6.1328, 6.3070, 6.1758, 6.1604, 6.3452, 6.2480, 6.1334, 6.3620, 6.4902,\n",
      "        6.1964, 6.5039, 6.2591, 6.3327, 6.0839, 6.3215, 6.3311, 6.2054, 6.0568,\n",
      "        6.0817, 6.0723, 6.1511], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.079008  [632524/5599865]\n",
      "average loss by time tensor([5.0402, 5.0626, 5.2162, 5.2947, 5.0423, 5.2897, 5.0238, 5.0739, 5.2210,\n",
      "        5.0427, 5.2601, 5.2797, 5.1709, 5.1441, 4.9833, 4.9227, 4.9690, 4.9786,\n",
      "        5.0291, 4.9818, 5.3290, 5.2865, 5.0154, 5.0006, 5.0233, 4.9361, 5.1169,\n",
      "        5.0939, 5.1117, 4.9722, 4.9069, 5.1495, 4.9452, 4.9436, 5.0076, 5.2275,\n",
      "        5.1484, 5.0343, 5.0181, 5.2084, 5.0966, 5.3609, 5.1016, 5.1605, 5.0700,\n",
      "        5.3321, 5.0789, 5.1769, 5.1299, 5.1206, 5.0939, 5.1788, 5.0607, 5.1838,\n",
      "        5.0421, 5.0546, 5.2303, 5.0464, 5.1367, 5.1980, 5.0096, 5.2248, 5.0556,\n",
      "        5.2210, 5.1259, 5.1413, 5.1320, 5.0411, 5.1412, 5.0640, 5.1598, 5.2704,\n",
      "        5.0773, 5.0045, 5.2578, 5.1645, 5.0064, 5.1133, 5.0171, 5.0486, 4.9771,\n",
      "        5.0284, 5.0979, 5.0957, 5.1722, 5.0455, 5.0614, 5.1871, 5.0337, 5.1772,\n",
      "        5.0734, 4.9929, 5.0228, 4.9979, 5.1238, 5.0557, 5.0024, 5.0297, 5.1511,\n",
      "        5.0055, 5.0884, 5.0051, 4.9753, 5.0546, 5.0006, 4.9165, 4.9888, 5.0305,\n",
      "        4.9142, 5.0552, 4.9566, 5.0127, 4.9366, 5.0449, 5.0329, 4.9328, 4.8976,\n",
      "        4.9008, 4.8842, 4.8904], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.064380  [644924/5599865]\n",
      "average loss by time tensor([4.8273, 4.8266, 4.8182, 4.8422, 4.7396, 4.8266, 4.6979, 4.7370, 4.7829,\n",
      "        4.7189, 4.8571, 4.9202, 4.8979, 4.8652, 4.7440, 4.7496, 4.7720, 4.7682,\n",
      "        4.7452, 4.7159, 4.9234, 4.8927, 4.7544, 4.7832, 4.7966, 4.7695, 4.8780,\n",
      "        4.8402, 4.8308, 4.7847, 4.7792, 4.8981, 4.8355, 4.8537, 4.8670, 4.9786,\n",
      "        4.9582, 4.9432, 4.9308, 5.0048, 4.9673, 5.0755, 4.9107, 4.9618, 4.9134,\n",
      "        5.0875, 4.9257, 5.0061, 5.0140, 5.0411, 5.0305, 5.0964, 5.0528, 5.1390,\n",
      "        5.0730, 5.0946, 5.1912, 5.1457, 5.2324, 5.2740, 5.1757, 5.3121, 5.2007,\n",
      "        5.2615, 5.1768, 5.2256, 5.2323, 5.1648, 5.2243, 5.2087, 5.2447, 5.2416,\n",
      "        5.1281, 5.0725, 5.2092, 5.1825, 5.1297, 5.1408, 5.1051, 5.1441, 5.1052,\n",
      "        5.1545, 5.2526, 5.2839, 5.3268, 5.2415, 5.2079, 5.3275, 5.2488, 5.3398,\n",
      "        5.2717, 5.2288, 5.2881, 5.2621, 5.3585, 5.3009, 5.2832, 5.2530, 5.3384,\n",
      "        5.2303, 5.3102, 5.2225, 5.1871, 5.2223, 5.1984, 5.1832, 5.2223, 5.2806,\n",
      "        5.2217, 5.3031, 5.1973, 5.2151, 5.1675, 5.2198, 5.2417, 5.2076, 5.1495,\n",
      "        5.1691, 5.1730, 5.1615], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.242186  [657324/5599865]\n",
      "average loss by time tensor([5.2047, 5.2393, 5.3093, 5.3591, 5.2563, 5.3648, 5.2104, 5.2640, 5.2267,\n",
      "        5.1686, 5.2937, 5.2821, 5.2435, 5.2170, 5.1229, 5.1133, 5.1501, 5.1031,\n",
      "        5.0805, 5.0712, 5.1905, 5.1501, 5.0186, 5.0770, 5.0832, 5.0417, 5.1423,\n",
      "        5.1023, 5.0810, 5.0311, 4.9962, 5.1836, 5.0958, 5.1045, 5.0987, 5.2480,\n",
      "        5.2107, 5.1005, 5.0847, 5.2300, 5.1734, 5.3916, 5.1751, 5.2003, 5.1454,\n",
      "        5.3839, 5.1589, 5.2045, 5.1451, 5.1762, 5.1299, 5.1703, 5.1111, 5.2041,\n",
      "        5.1163, 5.0894, 5.2590, 5.1113, 5.2390, 5.2834, 5.0996, 5.3764, 5.2128,\n",
      "        5.3417, 5.2293, 5.2848, 5.2921, 5.1672, 5.2349, 5.2205, 5.3115, 5.3830,\n",
      "        5.1894, 5.1150, 5.3905, 5.3502, 5.2289, 5.3363, 5.2944, 5.3331, 5.3092,\n",
      "        5.3762, 5.4599, 5.4414, 5.4906, 5.4091, 5.3463, 5.4827, 5.3064, 5.4117,\n",
      "        5.3217, 5.1862, 5.3171, 5.2810, 5.3999, 5.3541, 5.3198, 5.2987, 5.4193,\n",
      "        5.2488, 5.3490, 5.3154, 5.3026, 5.3612, 5.3023, 5.2467, 5.3346, 5.4013,\n",
      "        5.2766, 5.4296, 5.3033, 5.3279, 5.2667, 5.3479, 5.3040, 5.2533, 5.2128,\n",
      "        5.2396, 5.2299, 5.2663], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.894459  [669724/5599865]\n",
      "average loss by time tensor([5.7174, 5.7512, 5.8265, 5.9108, 5.7021, 6.0255, 5.6898, 5.8246, 5.8212,\n",
      "        5.6922, 5.9582, 5.9451, 5.7763, 5.7583, 5.6351, 5.6071, 5.6664, 5.6626,\n",
      "        5.6721, 5.5875, 5.9079, 5.8882, 5.5935, 5.6512, 5.7021, 5.6095, 5.7777,\n",
      "        5.7668, 5.7681, 5.6995, 5.6497, 5.8725, 5.7150, 5.6894, 5.7372, 5.9183,\n",
      "        5.8388, 5.7639, 5.7013, 5.9209, 5.8370, 6.1405, 5.8311, 5.8940, 5.7984,\n",
      "        6.1050, 5.7512, 5.9265, 5.9011, 5.9384, 5.9137, 6.0297, 5.8572, 6.0359,\n",
      "        5.9014, 5.9025, 6.1243, 5.9445, 6.0374, 6.1185, 5.8731, 6.1707, 5.9898,\n",
      "        6.1551, 6.0577, 6.0469, 6.0835, 5.9518, 6.0351, 5.9689, 6.0495, 6.1559,\n",
      "        5.9207, 5.8136, 6.1627, 6.0552, 5.9300, 6.0135, 5.9467, 6.0108, 5.9274,\n",
      "        6.0231, 6.0736, 6.0636, 6.1183, 5.9968, 5.9207, 6.0787, 5.9283, 6.0831,\n",
      "        6.0149, 5.8330, 5.9761, 5.8966, 6.0615, 5.9796, 5.8786, 5.8858, 6.0402,\n",
      "        5.8247, 5.9514, 5.8932, 5.8625, 5.9734, 5.8793, 5.7370, 5.8674, 5.9912,\n",
      "        5.8590, 6.0195, 5.8545, 5.9457, 5.7870, 5.9382, 5.9126, 5.9018, 5.8108,\n",
      "        5.8989, 5.9005, 5.9656], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.291809  [682124/5599865]\n",
      "average loss by time tensor([4.1935, 4.1880, 4.2720, 4.3247, 4.2171, 4.3017, 4.1611, 4.2531, 4.3034,\n",
      "        4.2450, 4.3701, 4.3865, 4.3111, 4.3672, 4.2647, 4.2941, 4.3232, 4.3156,\n",
      "        4.3129, 4.2550, 4.4532, 4.4030, 4.2653, 4.2513, 4.2795, 4.2029, 4.2902,\n",
      "        4.3278, 4.3207, 4.2763, 4.2123, 4.3038, 4.2262, 4.2382, 4.2328, 4.3281,\n",
      "        4.2833, 4.2321, 4.2120, 4.2854, 4.2700, 4.4599, 4.2538, 4.2602, 4.2137,\n",
      "        4.3753, 4.2331, 4.2689, 4.3155, 4.3166, 4.3294, 4.3732, 4.1827, 4.2460,\n",
      "        4.2260, 4.2826, 4.3343, 4.2786, 4.2979, 4.3319, 4.2743, 4.3675, 4.2385,\n",
      "        4.3264, 4.3301, 4.3659, 4.3481, 4.2839, 4.2989, 4.2810, 4.3137, 4.4238,\n",
      "        4.3032, 4.2773, 4.4066, 4.3631, 4.3055, 4.3224, 4.2821, 4.2766, 4.2255,\n",
      "        4.2566, 4.2608, 4.2571, 4.2997, 4.2823, 4.2506, 4.2925, 4.2260, 4.2725,\n",
      "        4.2653, 4.2119, 4.2471, 4.2200, 4.3157, 4.2906, 4.2910, 4.3010, 4.3212,\n",
      "        4.2847, 4.3737, 4.3352, 4.3434, 4.3718, 4.3155, 4.2472, 4.2807, 4.3029,\n",
      "        4.2834, 4.3737, 4.2795, 4.3321, 4.2776, 4.3080, 4.2905, 4.2790, 4.2506,\n",
      "        4.2964, 4.2835, 4.2971], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.750547  [694524/5599865]\n",
      "average loss by time tensor([5.5815, 5.5850, 5.6076, 5.6558, 5.5207, 5.6496, 5.4603, 5.5625, 5.6397,\n",
      "        5.5828, 5.7319, 5.7034, 5.6510, 5.6290, 5.5068, 5.5114, 5.7882, 5.8044,\n",
      "        5.8113, 5.7435, 5.9463, 5.9515, 5.8321, 5.8691, 5.8422, 5.7449, 5.8614,\n",
      "        5.8002, 5.7695, 5.7009, 5.6439, 5.7524, 5.6741, 5.6368, 5.6410, 5.7655,\n",
      "        5.7738, 5.6140, 5.5287, 5.6585, 5.5729, 5.7855, 5.5710, 5.6108, 5.5328,\n",
      "        5.7513, 5.5070, 5.5946, 5.5122, 5.5069, 5.4644, 5.5418, 5.4491, 5.5737,\n",
      "        5.4659, 5.4770, 5.6395, 5.4682, 5.5853, 5.6269, 5.4863, 5.6797, 5.5501,\n",
      "        5.6636, 5.5547, 5.5962, 5.6246, 5.5695, 5.6479, 5.6179, 5.6535, 5.7226,\n",
      "        5.6022, 5.5277, 5.7293, 5.7232, 5.6581, 5.7643, 5.7837, 5.7887, 5.7750,\n",
      "        5.8590, 5.8910, 5.8734, 5.8817, 5.7861, 5.7540, 5.8759, 5.8008, 5.9160,\n",
      "        5.8551, 5.7478, 5.8935, 5.8410, 5.9547, 5.9409, 5.9154, 5.9252, 6.0699,\n",
      "        5.9265, 6.0579, 5.9962, 5.9404, 5.9935, 5.9666, 5.8917, 6.0159, 6.0713,\n",
      "        6.0276, 6.1511, 6.0716, 6.1115, 6.0326, 6.0793, 6.0935, 6.0511, 5.9874,\n",
      "        6.0121, 6.0434, 6.0744], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.469346  [706924/5599865]\n",
      "average loss by time tensor([4.2685, 4.2796, 4.2563, 4.2701, 4.3517, 4.2741, 4.3698, 4.3661, 4.3052,\n",
      "        4.3346, 4.2840, 4.2846, 4.3211, 4.2758, 4.3381, 4.3323, 4.3325, 4.2988,\n",
      "        4.3123, 4.3481, 4.3140, 4.3233, 4.3122, 4.3149, 4.3286, 4.3844, 4.3458,\n",
      "        4.3485, 4.3563, 4.3918, 4.3810, 4.3444, 4.3911, 4.3958, 4.4159, 4.3685,\n",
      "        4.3889, 4.4117, 4.4540, 4.4091, 4.4276, 4.4179, 4.4117, 4.3762, 4.3903,\n",
      "        4.3373, 4.3387, 4.3443, 4.3145, 4.3225, 4.3048, 4.3258, 4.3257, 4.3422,\n",
      "        4.3908, 4.3824, 4.3330, 4.3468, 4.4301, 4.4716, 4.5203, 4.5634, 4.5805,\n",
      "        4.5932, 4.6236, 4.6445, 4.6383, 4.6298, 4.6376, 4.6124, 4.6134, 4.6210,\n",
      "        4.6016, 4.6347, 4.6190, 4.6379, 4.6130, 4.6238, 4.5954, 4.5818, 4.5859,\n",
      "        4.6069, 4.6013, 4.5683, 4.5621, 4.5485, 4.5274, 4.5288, 4.4869, 4.4878,\n",
      "        4.4824, 4.5208, 4.5327, 4.5696, 4.6004, 4.5711, 4.5645, 4.5565, 4.5726,\n",
      "        4.5726, 4.6048, 4.6048, 4.6210, 4.6168, 4.6210, 4.6210, 4.6048, 4.6290,\n",
      "        4.6199, 4.6268, 4.6218, 4.6279, 4.6543, 4.6380, 4.6257, 4.6399, 4.5702,\n",
      "        4.5504, 4.5461, 4.5274], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.402136  [719324/5599865]\n",
      "average loss by time tensor([5.4975, 5.5531, 5.6466, 5.7163, 5.5073, 5.7024, 5.4749, 5.5435, 5.6397,\n",
      "        5.4522, 5.6196, 5.6247, 5.5544, 5.5557, 5.3885, 5.3728, 5.4465, 5.4608,\n",
      "        5.4924, 5.3782, 5.6257, 5.6348, 5.4077, 5.4636, 5.4947, 5.3784, 5.5796,\n",
      "        5.5560, 5.5269, 5.4500, 5.3412, 5.5672, 5.4118, 5.4074, 5.4220, 5.5189,\n",
      "        5.5028, 5.3845, 5.2515, 5.4564, 5.4014, 5.5923, 5.4032, 5.4218, 5.3258,\n",
      "        5.5522, 5.2696, 5.4319, 5.4447, 5.3958, 5.4074, 5.4601, 5.3358, 5.4555,\n",
      "        5.3204, 5.3076, 5.4860, 5.3009, 5.4107, 5.4785, 5.2534, 5.4911, 5.3443,\n",
      "        5.4634, 5.3937, 5.4170, 5.4166, 5.3037, 5.4154, 5.3894, 5.4601, 5.4974,\n",
      "        5.2740, 5.1555, 5.4759, 5.4287, 5.2089, 5.3676, 5.2943, 5.3499, 5.3119,\n",
      "        5.4240, 5.4691, 5.4555, 5.5066, 5.4208, 5.2653, 5.4218, 5.2532, 5.4322,\n",
      "        5.3302, 5.1871, 5.3236, 5.2652, 5.3945, 5.3645, 5.3140, 5.2645, 5.3628,\n",
      "        5.2475, 5.3827, 5.2799, 5.2262, 5.3193, 5.2353, 5.1629, 5.3008, 5.3287,\n",
      "        5.2402, 5.3864, 5.2927, 5.3280, 5.2375, 5.3381, 5.3342, 5.2838, 5.1919,\n",
      "        5.2400, 5.2317, 5.2916], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.454277  [731724/5599865]\n",
      "average loss by time tensor([5.3595, 5.4085, 5.4294, 5.4523, 5.3137, 5.4865, 5.3340, 5.3852, 5.4806,\n",
      "        5.3785, 5.5110, 5.5051, 5.4468, 5.4336, 5.2659, 5.2489, 5.3250, 5.3309,\n",
      "        5.3869, 5.2890, 5.4604, 5.4465, 5.2466, 5.3006, 5.3359, 5.2400, 5.4409,\n",
      "        5.3941, 5.3651, 5.2475, 5.1999, 5.3669, 5.2489, 5.2144, 5.2292, 5.3618,\n",
      "        5.3331, 5.2716, 5.1769, 5.3604, 5.2824, 5.4870, 5.3160, 5.3793, 5.2473,\n",
      "        5.4761, 5.2131, 5.3753, 5.3509, 5.3569, 5.3000, 5.4206, 5.2616, 5.4321,\n",
      "        5.2863, 5.2820, 5.4973, 5.3231, 5.4248, 5.4980, 5.3117, 5.5632, 5.4193,\n",
      "        5.5480, 5.4639, 5.5094, 5.5438, 5.3815, 5.5008, 5.4854, 5.5934, 5.6527,\n",
      "        5.5157, 5.4682, 5.6797, 5.6748, 5.5196, 5.6313, 5.5487, 5.6241, 5.5326,\n",
      "        5.6091, 5.6439, 5.6309, 5.6804, 5.5990, 5.4884, 5.6692, 5.4838, 5.6428,\n",
      "        5.5228, 5.4076, 5.5324, 5.5005, 5.6664, 5.6321, 5.5580, 5.5328, 5.6446,\n",
      "        5.4684, 5.6141, 5.5548, 5.5346, 5.5944, 5.5469, 5.4732, 5.5961, 5.6707,\n",
      "        5.5511, 5.6831, 5.5632, 5.6003, 5.4794, 5.6516, 5.6226, 5.5649, 5.4481,\n",
      "        5.4939, 5.4515, 5.5157], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.918427  [744124/5599865]\n",
      "average loss by time tensor([5.0059, 4.9878, 5.0154, 5.0175, 4.9667, 4.9981, 4.9011, 4.9139, 4.9707,\n",
      "        4.9392, 5.0165, 5.0162, 4.9943, 4.9837, 4.9577, 4.9809, 5.0203, 5.0055,\n",
      "        5.0355, 4.9646, 5.0461, 5.0408, 5.0227, 5.0147, 4.9872, 4.9307, 5.0130,\n",
      "        4.9789, 4.9885, 5.0006, 4.9885, 5.0776, 5.0082, 5.0064, 4.9820, 5.0630,\n",
      "        5.0385, 5.0190, 5.0059, 5.0641, 5.0355, 5.0761, 5.0248, 5.0080, 4.9417,\n",
      "        5.0339, 4.9331, 4.9676, 4.9057, 4.8910, 4.8789, 4.9140, 4.8624, 4.9477,\n",
      "        4.8865, 4.9186, 4.9907, 4.9213, 4.9654, 4.9682, 4.8848, 4.9618, 4.8904,\n",
      "        4.9228, 4.8895, 4.9100, 4.9021, 4.8570, 4.9093, 4.8935, 4.9469, 4.9669,\n",
      "        4.9320, 4.8710, 4.9700, 4.9406, 4.8628, 4.9347, 4.8946, 4.9258, 4.8979,\n",
      "        4.9370, 4.9412, 4.9026, 4.9106, 4.8804, 4.7889, 4.8295, 4.7805, 4.8159,\n",
      "        4.8025, 4.7660, 4.8017, 4.8215, 4.9185, 4.8783, 4.8149, 4.8252, 4.8942,\n",
      "        4.8457, 4.8979, 4.8707, 4.8180, 4.8739, 4.8412, 4.7973, 4.8616, 4.8224,\n",
      "        4.7662, 4.8167, 4.7451, 4.7709, 4.7529, 4.7950, 4.7929, 4.7610, 4.7177,\n",
      "        4.7170, 4.7016, 4.7323], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.306070  [756524/5599865]\n",
      "average loss by time tensor([5.3872, 5.4129, 5.4098, 5.3958, 5.4022, 5.4129, 5.4392, 5.4037, 5.3774,\n",
      "        5.3320, 5.2988, 5.2906, 5.2548, 5.2621, 5.2263, 5.2205, 5.2142, 5.2276,\n",
      "        5.2645, 5.2286, 5.2291, 5.1993, 5.2105, 5.2202, 5.2134, 5.2780, 5.2978,\n",
      "        5.2964, 5.2886, 5.2914, 5.2979, 5.2753, 5.2534, 5.2509, 5.2529, 5.2386,\n",
      "        5.2404, 5.1820, 5.1673, 5.2502, 5.2359, 5.3419, 5.3138, 5.3699, 5.3149,\n",
      "        5.3963, 5.3226, 5.3444, 5.3238, 5.3011, 5.2961, 5.3028, 5.2377, 5.2618,\n",
      "        5.1886, 5.1808, 5.2939, 5.2277, 5.2692, 5.2862, 5.2235, 5.3321, 5.2681,\n",
      "        5.3306, 5.2861, 5.3030, 5.2791, 5.2345, 5.2821, 5.2587, 5.2821, 5.3596,\n",
      "        5.2576, 5.2621, 5.3873, 5.3416, 5.2750, 5.3308, 5.2682, 5.2889, 5.3065,\n",
      "        5.3459, 5.3582, 5.3599, 5.4090, 5.3428, 5.3306, 5.3903, 5.3454, 5.3929,\n",
      "        5.3802, 5.3853, 5.3965, 5.3683, 5.4314, 5.4246, 5.3839, 5.3675, 5.4211,\n",
      "        5.3441, 5.4332, 5.3366, 5.3378, 5.3793, 5.3646, 5.2767, 5.3084, 5.3612,\n",
      "        5.2780, 5.3430, 5.2926, 5.3203, 5.2984, 5.3435, 5.3339, 5.3087, 5.2581,\n",
      "        5.1935, 5.2097, 5.2119], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.795325  [768924/5599865]\n",
      "average loss by time tensor([4.7480, 4.7629, 4.8700, 4.8927, 4.8127, 4.9082, 4.7557, 4.7637, 4.8576,\n",
      "        4.7532, 4.8604, 4.8438, 4.8412, 4.8233, 4.7253, 4.7211, 4.7171, 4.7324,\n",
      "        4.7686, 4.7560, 4.9144, 4.9193, 4.7473, 4.7892, 4.8425, 4.7666, 4.8910,\n",
      "        4.8533, 4.8411, 4.7814, 4.7851, 4.9082, 4.8020, 4.7874, 4.7828, 4.8499,\n",
      "        4.8550, 4.7914, 4.7548, 4.8854, 4.7915, 4.9244, 4.7616, 4.8498, 4.7757,\n",
      "        4.9359, 4.7441, 4.8637, 4.8543, 4.8332, 4.8203, 4.9218, 4.7759, 4.8355,\n",
      "        4.7533, 4.7653, 4.8530, 4.7469, 4.8398, 4.9118, 4.7446, 4.8960, 4.7993,\n",
      "        4.8766, 4.8649, 4.9158, 4.9133, 4.7954, 4.8134, 4.7805, 4.8247, 4.8579,\n",
      "        4.7252, 4.7245, 4.8410, 4.8192, 4.7151, 4.7963, 4.7091, 4.7122, 4.6866,\n",
      "        4.7276, 4.8120, 4.7925, 4.8454, 4.7622, 4.7260, 4.8590, 4.7385, 4.8634,\n",
      "        4.7648, 4.6823, 4.7554, 4.7392, 4.8468, 4.7979, 4.7702, 4.7708, 4.8528,\n",
      "        4.7354, 4.8316, 4.7541, 4.7155, 4.8392, 4.8247, 4.7319, 4.7860, 4.7986,\n",
      "        4.7066, 4.8098, 4.7018, 4.7483, 4.6455, 4.7148, 4.7307, 4.7058, 4.6793,\n",
      "        4.6894, 4.6652, 4.6870], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.310268  [781324/5599865]\n",
      "average loss by time tensor([5.2703, 5.2633, 5.3891, 5.4519, 5.3779, 5.5535, 5.3793, 5.4114, 5.5221,\n",
      "        5.3962, 5.5597, 5.5835, 5.4737, 5.5157, 5.4134, 5.4258, 5.4640, 5.4381,\n",
      "        5.4679, 5.4502, 5.6322, 5.6405, 5.4810, 5.4936, 5.4988, 5.3961, 5.5027,\n",
      "        5.4555, 5.4599, 5.3985, 5.2988, 5.4241, 5.3128, 5.2766, 5.2962, 5.4406,\n",
      "        5.4092, 5.3254, 5.2963, 5.4369, 5.3346, 5.5007, 5.3546, 5.4089, 5.3289,\n",
      "        5.4166, 5.2313, 5.2974, 5.2468, 5.2320, 5.1897, 5.2387, 5.1850, 5.2579,\n",
      "        5.1704, 5.1679, 5.2640, 5.1333, 5.1847, 5.1956, 5.0628, 5.2225, 5.1253,\n",
      "        5.2564, 5.1965, 5.2074, 5.2186, 5.1576, 5.3108, 5.2525, 5.3215, 5.3529,\n",
      "        5.2377, 5.1789, 5.3033, 5.2681, 5.1504, 5.2546, 5.2071, 5.2566, 5.2337,\n",
      "        5.2600, 5.2857, 5.2793, 5.3081, 5.2243, 5.2040, 5.2829, 5.2351, 5.3341,\n",
      "        5.2640, 5.2278, 5.2660, 5.2785, 5.2997, 5.2461, 5.2169, 5.2150, 5.3139,\n",
      "        5.2324, 5.2896, 5.2257, 5.2550, 5.2971, 5.2537, 5.2195, 5.2696, 5.3194,\n",
      "        5.1930, 5.2974, 5.2275, 5.2677, 5.2033, 5.2612, 5.2729, 5.2272, 5.1778,\n",
      "        5.2111, 5.1920, 5.2005], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.987636  [793724/5599865]\n",
      "average loss by time tensor([4.9581, 4.9467, 4.9970, 5.0234, 4.9315, 5.0458, 4.9160, 4.9582, 5.0325,\n",
      "        4.9269, 5.0476, 5.0474, 5.0245, 4.9760, 4.9141, 4.9076, 4.9125, 4.9028,\n",
      "        4.9425, 4.9506, 5.1110, 5.0816, 4.9848, 4.9818, 4.9946, 4.9837, 5.0598,\n",
      "        5.0518, 5.0315, 4.9784, 4.9778, 5.0707, 4.9967, 4.9945, 4.9732, 5.0397,\n",
      "        5.0141, 4.9900, 4.9936, 5.0867, 5.0394, 5.1870, 5.1520, 5.1734, 5.1171,\n",
      "        5.2204, 5.0652, 5.0879, 5.0575, 5.0069, 4.9564, 5.0232, 4.9105, 4.9186,\n",
      "        4.8655, 4.8736, 4.9282, 4.8239, 4.8420, 4.9384, 4.8604, 4.9638, 4.8616,\n",
      "        5.0023, 4.9259, 4.9451, 4.9710, 4.9391, 4.9972, 4.9342, 5.0052, 5.0238,\n",
      "        4.8767, 4.8668, 5.0039, 4.9632, 4.8512, 4.8581, 4.8341, 4.8190, 4.8220,\n",
      "        4.8706, 4.9679, 4.9820, 5.0149, 4.9340, 4.8999, 4.9678, 4.8910, 4.9602,\n",
      "        4.9406, 4.9478, 4.9741, 5.0073, 5.0691, 4.9997, 4.9736, 5.0033, 5.0784,\n",
      "        4.9801, 5.0226, 4.9575, 4.9792, 5.0674, 5.0607, 5.0750, 5.0924, 5.0778,\n",
      "        5.0053, 5.1691, 5.0478, 5.0599, 5.0059, 5.0681, 5.0723, 5.0432, 5.0211,\n",
      "        4.9944, 4.9831, 4.9815], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.597932  [806124/5599865]\n",
      "average loss by time tensor([5.4752, 5.4624, 5.5036, 5.5317, 5.4441, 5.4867, 5.4311, 5.4863, 5.5072,\n",
      "        5.4732, 5.5551, 5.6033, 5.5502, 5.5445, 5.4775, 5.4435, 5.4391, 5.4337,\n",
      "        5.4113, 5.4274, 5.5406, 5.5784, 5.5356, 5.5947, 5.5959, 5.5668, 5.6003,\n",
      "        5.5979, 5.5612, 5.5274, 5.5282, 5.6495, 5.5873, 5.5645, 5.5161, 5.6110,\n",
      "        5.5973, 5.5796, 5.5645, 5.5943, 5.5645, 5.6594, 5.5415, 5.5408, 5.5478,\n",
      "        5.5954, 5.5048, 5.5228, 5.5742, 5.5511, 5.5614, 5.5853, 5.5266, 5.5084,\n",
      "        5.5194, 5.5114, 5.5891, 5.5726, 5.5962, 5.6226, 5.5686, 5.6087, 5.5745,\n",
      "        5.6343, 5.5892, 5.6070, 5.6052, 5.5864, 5.5966, 5.6007, 5.6038, 5.6367,\n",
      "        5.5725, 5.5851, 5.6276, 5.6568, 5.6106, 5.6255, 5.6481, 5.6626, 5.6394,\n",
      "        5.5746, 5.6332, 5.6308, 5.6173, 5.5835, 5.5424, 5.5929, 5.5834, 5.5681,\n",
      "        5.5771, 5.6426, 5.6404, 5.6809, 5.7218, 5.7019, 5.7202, 5.7534, 5.7947,\n",
      "        5.7328, 5.6994, 5.6795, 5.7275, 5.7503, 5.7262, 5.7540, 5.7565, 5.7711,\n",
      "        5.7443, 5.7273, 5.7205, 5.7186, 5.7149, 5.7101, 5.7023, 5.6874, 5.6625,\n",
      "        5.6571, 5.6877, 5.6490], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.063160  [818524/5599865]\n",
      "average loss by time tensor([5.0484, 5.0403, 5.0296, 5.0624, 5.0887, 5.0880, 5.0242, 5.0185, 4.9831,\n",
      "        4.9789, 5.0704, 5.0653, 5.0597, 5.0704, 5.0403, 5.0600, 5.0706, 5.0784,\n",
      "        5.0876, 5.0923, 5.1403, 5.1496, 5.1020, 5.1179, 5.1201, 5.0968, 5.0756,\n",
      "        5.0981, 5.1165, 5.1613, 5.1532, 5.1840, 5.1694, 5.1637, 5.1793, 5.2084,\n",
      "        5.1902, 5.1613, 5.1371, 5.1348, 5.1452, 5.1660, 5.1032, 5.1371, 5.1158,\n",
      "        5.1268, 5.0935, 5.0647, 5.1048, 5.0968, 5.1048, 5.0427, 5.0645, 5.0290,\n",
      "        5.0242, 5.0161, 5.0425, 5.0048, 5.0081, 5.0516, 5.0116, 5.0796, 5.0389,\n",
      "        5.0873, 5.0315, 5.0485, 5.0739, 5.0338, 5.0723, 5.0939, 5.1257, 5.1453,\n",
      "        5.0824, 5.0565, 5.0592, 5.0552, 5.0450, 5.0406, 5.0323, 4.9919, 4.9839,\n",
      "        4.9274, 4.9121, 4.9276, 4.9406, 4.9113, 4.8832, 4.9010, 4.9032, 4.9340,\n",
      "        4.9396, 4.8877, 4.8905, 4.8914, 5.0071, 4.9809, 4.9690, 5.0076, 5.0500,\n",
      "        5.0243, 5.0375, 5.0131, 5.0338, 5.0748, 5.0619, 5.0351, 5.0723, 5.1203,\n",
      "        5.0837, 5.1797, 5.1325, 5.1461, 5.1326, 5.1806, 5.1761, 5.1184, 5.0987,\n",
      "        5.1176, 5.1134, 5.1141], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.914458  [830924/5599865]\n",
      "average loss by time tensor([4.8246, 4.8344, 4.8385, 4.8638, 4.8017, 4.8356, 4.8065, 4.8468, 4.8741,\n",
      "        4.8663, 4.8957, 4.8998, 4.8743, 4.8791, 4.8754, 4.8759, 4.8894, 4.8949,\n",
      "        4.9076, 4.9014, 4.9373, 4.9447, 4.9418, 4.9392, 4.9218, 4.9303, 4.9226,\n",
      "        4.9203, 4.9198, 4.9105, 4.9259, 4.9429, 4.9342, 4.9033, 4.9019, 4.9147,\n",
      "        4.8846, 4.8446, 4.8282, 4.8141, 4.8056, 4.8058, 4.7677, 4.7770, 4.7823,\n",
      "        4.7962, 4.7915, 4.7439, 4.7887, 4.7905, 4.8000, 4.7675, 4.8360, 4.8553,\n",
      "        4.8994, 4.8996, 4.9209, 4.9015, 4.8403, 4.8649, 4.8807, 4.8863, 4.8682,\n",
      "        4.8692, 4.8671, 4.8550, 4.8685, 4.8853, 4.8568, 4.8943, 4.9136, 4.9840,\n",
      "        4.9909, 4.9991, 5.0238, 4.9804, 4.9795, 4.9519, 4.9458, 4.9206, 4.9225,\n",
      "        4.8959, 4.8618, 4.8549, 4.8822, 4.8647, 4.8832, 4.9091, 4.8916, 4.9033,\n",
      "        4.9217, 4.9517, 4.9393, 4.9441, 4.9418, 4.9366, 4.9617, 4.9485, 4.9642,\n",
      "        4.9603, 4.9363, 4.9590, 4.9915, 4.9788, 4.9875, 5.0305, 5.0416, 5.0607,\n",
      "        5.0786, 5.0893, 5.0983, 5.0888, 5.0968, 5.1010, 5.0741, 5.0899, 5.0696,\n",
      "        5.1080, 5.0930, 5.0988], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.124532  [843324/5599865]\n",
      "average loss by time tensor([5.9827, 6.0006, 6.0373, 6.0551, 6.0496, 6.0922, 6.0502, 6.0858, 6.1432,\n",
      "        6.1058, 6.1586, 6.1667, 6.1189, 6.1258, 6.1044, 6.0780, 6.1074, 6.1218,\n",
      "        6.1275, 6.0626, 6.1215, 6.1146, 6.0670, 6.1152, 6.0868, 6.0772, 6.1114,\n",
      "        6.0924, 6.1417, 6.1117, 6.0648, 6.0852, 6.0510, 6.0246, 6.0107, 6.0455,\n",
      "        6.0476, 6.0362, 6.0555, 6.0743, 6.0555, 6.1464, 6.0550, 6.0534, 6.0292,\n",
      "        6.0990, 6.0211, 6.0142, 6.0431, 6.0420, 6.0639, 6.0537, 6.0454, 6.0704,\n",
      "        6.0508, 6.0300, 6.0355, 5.9958, 6.0211, 6.0294, 5.9867, 6.0490, 6.0152,\n",
      "        6.0491, 6.0003, 6.0211, 6.0314, 6.0278, 6.0736, 6.0888, 6.1103, 6.1472,\n",
      "        6.1087, 6.1077, 6.1999, 6.1436, 6.1682, 6.2128, 6.1715, 6.2248, 6.2354,\n",
      "        6.2288, 6.2168, 6.2552, 6.2909, 6.2545, 6.2312, 6.2651, 6.2090, 6.2153,\n",
      "        6.2037, 6.1988, 6.2196, 6.2212, 6.2570, 6.2419, 6.2373, 6.2352, 6.2691,\n",
      "        6.2774, 6.2864, 6.3030, 6.3044, 6.3284, 6.3044, 6.2668, 6.2697, 6.2614,\n",
      "        6.1918, 6.2247, 6.1313, 6.1355, 6.1415, 6.1843, 6.1670, 6.1227, 6.0827,\n",
      "        6.1065, 6.0908, 6.0757], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.171085  [855724/5599865]\n",
      "average loss by time tensor([4.1264, 4.1165, 4.1173, 4.0893, 4.1373, 4.1057, 4.1301, 4.1219, 4.0996,\n",
      "        4.1173, 4.0766, 4.0767, 4.0921, 4.0998, 4.1063, 4.1303, 4.1172, 4.1277,\n",
      "        4.1197, 4.1509, 4.1031, 4.1092, 4.1116, 4.1199, 4.1322, 4.1299, 4.0844,\n",
      "        4.1367, 4.1435, 4.1619, 4.1790, 4.1694, 4.1766, 4.1963, 4.2087, 4.2009,\n",
      "        4.1964, 4.2012, 4.2544, 4.1545, 4.1729, 4.1820, 4.1713, 4.1459, 4.1887,\n",
      "        4.1914, 4.2421, 4.2086, 4.2215, 4.2236, 4.2282, 4.2150, 4.2396, 4.2061,\n",
      "        4.1980, 4.2142, 4.1775, 4.2193, 4.2090, 4.1855, 4.1840, 4.1416, 4.1785,\n",
      "        4.1771, 4.1879, 4.1575, 4.1633, 4.1710, 4.1685, 4.2004, 4.2131, 4.2406,\n",
      "        4.2675, 4.2810, 4.2258, 4.2123, 4.2363, 4.2238, 4.2488, 4.2278, 4.2450,\n",
      "        4.2492, 4.2283, 4.2304, 4.2097, 4.2355, 4.2843, 4.2451, 4.3104, 4.2871,\n",
      "        4.3114, 4.3361, 4.3002, 4.3122, 4.2602, 4.2368, 4.2555, 4.2173, 4.1780,\n",
      "        4.2026, 4.2220, 4.2165, 4.1886, 4.0443, 4.0534, 4.0758, 4.0329, 4.0646,\n",
      "        4.0776, 4.0565, 4.0978, 4.0621, 4.1015, 4.0755, 4.0686, 4.0527, 4.0724,\n",
      "        4.0671, 4.0987, 4.0912], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.694516  [868124/5599865]\n",
      "average loss by time tensor([5.5323, 5.5242, 5.5242, 5.5161, 5.5323, 5.5484, 5.5565, 5.5403, 5.5572,\n",
      "        5.5527, 5.5609, 5.6013, 5.6210, 5.6694, 5.6855, 5.7016, 5.6935, 5.6613,\n",
      "        5.6613, 5.6613, 5.6703, 5.6582, 5.6371, 5.6452, 5.6452, 5.6532, 5.6694,\n",
      "        5.6694, 5.6935, 5.7016, 5.7016, 5.6935, 5.6694, 5.6774, 5.7016, 5.7016,\n",
      "        5.6935, 5.7016, 5.7177, 5.7013, 5.7043, 5.7522, 5.7282, 5.7309, 5.7336,\n",
      "        5.7360, 5.7218, 5.7150, 5.7133, 5.6804, 5.6788, 5.6752, 5.6750, 5.6432,\n",
      "        5.6462, 5.6673, 5.6368, 5.6164, 5.6276, 5.6524, 5.6358, 5.6449, 5.6532,\n",
      "        5.6689, 5.6553, 5.6504, 5.6348, 5.6154, 5.6422, 5.6074, 5.6113, 5.6324,\n",
      "        5.6352, 5.5643, 5.6006, 5.5619, 5.5586, 5.5770, 5.5823, 5.6084, 5.6209,\n",
      "        5.6257, 5.6681, 5.6835, 5.7092, 5.7203, 5.6994, 5.7083, 5.6828, 5.6998,\n",
      "        5.7113, 5.6996, 5.7525, 5.7666, 5.7725, 5.7842, 5.8028, 5.7803, 5.7970,\n",
      "        5.7665, 5.7790, 5.7742, 5.7961, 5.8122, 5.8244, 5.8137, 5.8187, 5.8703,\n",
      "        5.8221, 5.8427, 5.8720, 5.9077, 5.9423, 5.9659, 5.9493, 5.9437, 5.9440,\n",
      "        5.8831, 5.8749, 5.8757], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.092449  [880524/5599865]\n",
      "average loss by time tensor([5.1895, 5.1949, 5.2451, 5.2675, 5.2915, 5.2666, 5.2619, 5.2650, 5.2850,\n",
      "        5.2876, 5.2679, 5.2360, 5.1641, 5.1566, 5.1698, 5.1470, 5.1712, 5.1033,\n",
      "        5.0720, 5.0519, 5.0369, 4.9715, 4.9922, 4.9977, 5.0045, 4.9823, 4.9928,\n",
      "        4.9449, 4.9770, 4.9551, 4.9656, 4.9523, 4.9130, 4.9305, 4.8969, 4.9121,\n",
      "        4.9457, 4.9365, 4.9580, 4.9052, 4.8904, 4.9117, 4.9151, 4.9017, 4.9687,\n",
      "        4.9402, 4.9620, 4.9093, 4.9311, 4.9785, 5.0150, 4.9380, 4.9139, 4.9054,\n",
      "        4.8974, 4.9297, 4.9363, 4.9563, 4.9403, 4.9448, 4.9695, 4.9598, 4.9867,\n",
      "        4.9764, 5.0102, 4.9930, 5.0249, 5.0128, 5.0000, 4.9839, 5.0081, 5.0484,\n",
      "        5.0081, 5.0323, 5.0404, 4.9935, 5.0128, 4.9865, 5.0161, 5.0104, 5.0365,\n",
      "        5.0340, 5.0088, 5.0275, 5.0172, 4.9955, 5.0400, 5.0508, 5.0539, 5.0806,\n",
      "        5.1048, 5.1210, 5.1342, 5.1183, 5.1203, 5.1672, 5.1905, 5.1947, 5.1931,\n",
      "        5.2177, 5.2581, 5.2956, 5.3190, 5.3297, 5.3365, 5.3087, 5.3131, 5.3542,\n",
      "        5.3306, 5.3790, 5.4032, 5.4183, 5.3504, 5.3703, 5.3781, 5.4092, 5.4476,\n",
      "        5.4192, 5.3848, 5.3570], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.153595  [892924/5599865]\n",
      "average loss by time tensor([5.1007, 5.0774, 5.0634, 5.0398, 5.0425, 5.0887, 5.0806, 5.1129, 5.1210,\n",
      "        5.1174, 5.0968, 5.1371, 5.1139, 5.1061, 5.1104, 5.0948, 5.0759, 5.0682,\n",
      "        5.0671, 5.0784, 5.0946, 5.0888, 5.1096, 5.1379, 5.1302, 5.1394, 5.0923,\n",
      "        5.1023, 5.1085, 5.1235, 5.1550, 5.1621, 5.1793, 5.1522, 5.1585, 5.1623,\n",
      "        5.1805, 5.1785, 5.1622, 5.1624, 5.1751, 5.2021, 5.1724, 5.1404, 5.1296,\n",
      "        5.1320, 5.1334, 5.0838, 5.0854, 5.1129, 5.1048, 5.1358, 5.1397, 5.1601,\n",
      "        5.1319, 5.1024, 5.1044, 5.0726, 5.0904, 5.1210, 5.1290, 5.1451, 5.1584,\n",
      "        5.1452, 5.1613, 5.1763, 5.1767, 5.1485, 5.1758, 5.1584, 5.2016, 5.1855,\n",
      "        5.1902, 5.2099, 5.1937, 5.1863, 5.1983, 5.1949, 5.2130, 5.1798, 5.1978,\n",
      "        5.2177, 5.2419, 5.2569, 5.2657, 5.2500, 5.2339, 5.2500, 5.1960, 5.2097,\n",
      "        5.1935, 5.1923, 5.2016, 5.1720, 5.1378, 5.1958, 5.1886, 5.1809, 5.1779,\n",
      "        5.1909, 5.1800, 5.1722, 5.1972, 5.1713, 5.1900, 5.1730, 5.1320, 5.1544,\n",
      "        5.1733, 5.1373, 5.1793, 5.2209, 5.2115, 5.2039, 5.2045, 5.2118, 5.1997,\n",
      "        5.1816, 5.1827, 5.1724], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.575111  [905324/5599865]\n",
      "average loss by time tensor([5.4509, 5.4850, 5.4945, 5.4659, 5.4791, 5.4640, 5.4891, 5.4443, 5.3764,\n",
      "        5.3634, 5.3570, 5.3166, 5.3040, 5.3371, 5.3788, 5.3888, 5.4264, 5.4198,\n",
      "        5.4313, 5.4849, 5.4683, 5.4534, 5.4670, 5.4780, 5.5083, 5.5051, 5.4878,\n",
      "        5.5054, 5.5048, 5.4958, 5.5192, 5.4782, 5.4628, 5.4722, 5.4665, 5.4454,\n",
      "        5.4873, 5.5219, 5.5263, 5.5189, 5.5743, 5.5666, 5.5539, 5.5788, 5.5994,\n",
      "        5.5816, 5.6232, 5.6348, 5.6132, 5.6222, 5.6004, 5.5695, 5.5739, 5.6014,\n",
      "        5.6136, 5.5975, 5.6073, 5.6041, 5.5948, 5.6155, 5.6402, 5.6868, 5.6792,\n",
      "        5.6715, 5.6690, 5.6401, 5.5991, 5.6313, 5.6170, 5.6552, 5.6494, 5.6790,\n",
      "        5.7411, 5.7438, 5.7032, 5.7214, 5.7172, 5.6826, 5.7048, 5.6535, 5.6187,\n",
      "        5.6115, 5.6476, 5.6628, 5.6935, 5.7339, 5.7419, 5.6855, 5.6431, 5.7031,\n",
      "        5.7012, 5.6968, 5.6808, 5.6671, 5.6543, 5.6397, 5.6164, 5.6249, 5.6300,\n",
      "        5.6268, 5.6065, 5.6484, 5.6452, 5.6371, 5.6452, 5.6290, 5.6228, 5.6298,\n",
      "        5.6452, 5.6299, 5.6106, 5.5901, 5.5806, 5.5495, 5.5349, 5.5776, 5.6098,\n",
      "        5.6221, 5.5863, 5.5921], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.568913  [917724/5599865]\n",
      "average loss by time tensor([4.5711, 4.5325, 4.4883, 4.4977, 4.5419, 4.5386, 4.5086, 4.4990, 4.5532,\n",
      "        4.5270, 4.5477, 4.5484, 4.5242, 4.4839, 4.4857, 4.4690, 4.4879, 4.5117,\n",
      "        4.5290, 4.5206, 4.5391, 4.5463, 4.5657, 4.5776, 4.5392, 4.5108, 4.5275,\n",
      "        4.5400, 4.5872, 4.6114, 4.6130, 4.6466, 4.6371, 4.6345, 4.6412, 4.6325,\n",
      "        4.6218, 4.5903, 4.5989, 4.5976, 4.5915, 4.6460, 4.6149, 4.6150, 4.5830,\n",
      "        4.6341, 4.5795, 4.6318, 4.6025, 4.5925, 4.5636, 4.5689, 4.4875, 4.5355,\n",
      "        4.5069, 4.5169, 4.5634, 4.5101, 4.5561, 4.5805, 4.5484, 4.5923, 4.5429,\n",
      "        4.5342, 4.5068, 4.5051, 4.4927, 4.4887, 4.5180, 4.4979, 4.4753, 4.4724,\n",
      "        4.4757, 4.4730, 4.5387, 4.5212, 4.4970, 4.4939, 4.4878, 4.4535, 4.4901,\n",
      "        4.4950, 4.4808, 4.4721, 4.4978, 4.5044, 4.5122, 4.5292, 4.5456, 4.5350,\n",
      "        4.5562, 4.5709, 4.6268, 4.6026, 4.6314, 4.6415, 4.6622, 4.6282, 4.6414,\n",
      "        4.6427, 4.6554, 4.6667, 4.6482, 4.6483, 4.6606, 4.6658, 4.6849, 4.6661,\n",
      "        4.6824, 4.6913, 4.6710, 4.6981, 4.6589, 4.6649, 4.6805, 4.6659, 4.6740,\n",
      "        4.6495, 4.6373, 4.6140], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.692832  [930124/5599865]\n",
      "average loss by time tensor([4.9627, 4.9635, 4.9737, 4.9758, 4.9448, 4.9182, 4.9194, 4.9145, 4.9516,\n",
      "        4.9330, 4.9123, 4.9365, 4.9455, 4.9296, 4.9662, 4.9587, 4.9640, 4.9758,\n",
      "        4.9642, 4.9609, 4.9106, 4.9264, 4.9141, 4.9154, 4.8905, 4.8948, 4.8729,\n",
      "        4.8492, 4.8651, 4.8755, 4.8618, 4.8145, 4.8145, 4.8065, 4.8027, 4.7927,\n",
      "        4.7840, 4.7621, 4.7728, 4.7354, 4.7539, 4.7180, 4.7620, 4.7526, 4.7605,\n",
      "        4.7281, 4.7393, 4.7147, 4.7164, 4.7090, 4.6989, 4.6680, 4.6885, 4.6456,\n",
      "        4.6701, 4.6862, 4.6831, 4.7005, 4.6692, 4.6671, 4.6662, 4.6643, 4.6568,\n",
      "        4.6490, 4.6627, 4.6595, 4.6503, 4.6543, 4.6531, 4.6569, 4.6128, 4.6173,\n",
      "        4.6598, 4.6662, 4.6334, 4.5797, 4.6038, 4.5737, 4.5819, 4.5699, 4.5855,\n",
      "        4.5918, 4.5717, 4.5730, 4.5714, 4.5761, 4.6023, 4.5834, 4.6216, 4.5827,\n",
      "        4.5783, 4.6160, 4.5480, 4.5659, 4.5470, 4.5773, 4.5565, 4.5593, 4.5463,\n",
      "        4.5904, 4.5932, 4.5841, 4.5703, 4.5521, 4.4983, 4.4652, 4.4564, 4.4313,\n",
      "        4.4312, 4.4545, 4.4056, 4.3992, 4.4116, 4.4066, 4.3767, 4.3754, 4.3996,\n",
      "        4.3838, 4.3780, 4.3871], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.601843  [942524/5599865]\n",
      "average loss by time tensor([4.7603, 4.7263, 4.7323, 4.7062, 4.7062, 4.6731, 4.6778, 4.6518, 4.6593,\n",
      "        4.6621, 4.6333, 4.6978, 4.6694, 4.6379, 4.6651, 4.6747, 4.6665, 4.6193,\n",
      "        4.6239, 4.6591, 4.6548, 4.6635, 4.6801, 4.7178, 4.7212, 4.7252, 4.7136,\n",
      "        4.7227, 4.6981, 4.7191, 4.7320, 4.6885, 4.6621, 4.6033, 4.5821, 4.5454,\n",
      "        4.5476, 4.5569, 4.5754, 4.5466, 4.5642, 4.5498, 4.5404, 4.5349, 4.5321,\n",
      "        4.5025, 4.4984, 4.4784, 4.4792, 4.4392, 4.4642, 4.4458, 4.3935, 4.3731,\n",
      "        4.3611, 4.3835, 4.3900, 4.3987, 4.4086, 4.4304, 4.4335, 4.4457, 4.4837,\n",
      "        4.4718, 4.5146, 4.5264, 4.5341, 4.5175, 4.5001, 4.4394, 4.4416, 4.3986,\n",
      "        4.3882, 4.4138, 4.4067, 4.4993, 4.5520, 4.5507, 4.5367, 4.5472, 4.5698,\n",
      "        4.5351, 4.5502, 4.5909, 4.5887, 4.5604, 4.5547, 4.5601, 4.5887, 4.6149,\n",
      "        4.6129, 4.6613, 4.6694, 4.7500, 4.7581, 4.7984, 4.8306, 4.8468, 4.8548,\n",
      "        4.8694, 4.8329, 4.8103, 4.7869, 4.7600, 4.7290, 4.7246, 4.7016, 4.6545,\n",
      "        4.6411, 4.6220, 4.5922, 4.5988, 4.6078, 4.6164, 4.6168, 4.6062, 4.6506,\n",
      "        4.6609, 4.6622, 4.6533], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.943046  [954924/5599865]\n",
      "average loss by time tensor([4.8463, 4.8517, 4.8512, 4.8177, 4.8123, 4.7955, 4.8073, 4.8232, 4.7905,\n",
      "        4.7784, 4.7856, 4.8017, 4.7965, 4.8052, 4.7834, 4.7772, 4.7689, 4.8035,\n",
      "        4.7847, 4.7932, 4.7864, 4.8674, 4.8899, 4.8996, 4.8733, 4.8944, 4.8771,\n",
      "        4.9052, 4.9069, 4.9592, 4.9722, 4.9885, 5.0331, 5.0501, 5.0331, 5.0349,\n",
      "        5.0683, 5.0568, 5.0459, 4.9968, 5.0080, 4.9609, 4.9799, 5.0027, 4.9947,\n",
      "        4.9624, 4.9817, 4.9163, 4.9367, 4.9706, 4.9906, 4.9633, 4.9583, 4.9489,\n",
      "        4.9897, 5.0139, 4.9447, 4.9972, 5.0058, 5.0078, 4.9963, 4.9884, 5.0241,\n",
      "        5.0124, 4.9938, 5.0068, 4.9938, 5.0037, 5.0108, 4.9920, 4.9983, 4.9783,\n",
      "        4.9390, 4.8965, 4.9137, 4.9250, 4.9231, 4.9508, 4.9892, 4.9701, 4.9758,\n",
      "        4.9391, 4.9009, 4.9180, 4.8996, 4.9314, 4.9385, 4.9492, 4.9689, 4.9562,\n",
      "        4.9784, 5.0383, 5.0080, 5.0390, 4.9461, 4.9632, 4.9604, 4.9710, 4.9531,\n",
      "        4.9698, 4.9609, 4.9673, 4.9704, 4.9897, 4.9904, 5.0046, 5.0074, 5.0283,\n",
      "        5.0322, 5.0172, 4.9955, 4.9779, 4.9839, 4.9758, 4.9839, 5.0000, 4.9911,\n",
      "        4.9274, 4.9597, 4.9435], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.484923  [967324/5599865]\n",
      "average loss by time tensor([4.7661, 4.7061, 4.6660, 4.6647, 4.6593, 4.6399, 4.6538, 4.6956, 4.7059,\n",
      "        4.7235, 4.7431, 4.7109, 4.6877, 4.6717, 4.6290, 4.5968, 4.5806, 4.6290,\n",
      "        4.6290, 4.6210, 4.6452, 4.6855, 4.6426, 4.6252, 4.6326, 4.6289, 4.6070,\n",
      "        4.6048, 4.6315, 4.6178, 4.6282, 4.5726, 4.6210, 4.6371, 4.6371, 4.6290,\n",
      "        4.6048, 4.5806, 4.5990, 4.6031, 4.5885, 4.5951, 4.5921, 4.5882, 4.5560,\n",
      "        4.5856, 4.5421, 4.5482, 4.5456, 4.5566, 4.5470, 4.5273, 4.5014, 4.5114,\n",
      "        4.5273, 4.5096, 4.4903, 4.4778, 4.4861, 4.4885, 4.4531, 4.4410, 4.4352,\n",
      "        4.4404, 4.4286, 4.3847, 4.3770, 4.3811, 4.3600, 4.3506, 4.3768, 4.3763,\n",
      "        4.3650, 4.3776, 4.4167, 4.3770, 4.3629, 4.3871, 4.3744, 4.3431, 4.3251,\n",
      "        4.3437, 4.3205, 4.3283, 4.3145, 4.3065, 4.3065, 4.2984, 4.2810, 4.2925,\n",
      "        4.3075, 4.3380, 4.3228, 4.3614, 4.3487, 4.3455, 4.3473, 4.3401, 4.3260,\n",
      "        4.3573, 4.3646, 4.3791, 4.3613, 4.3671, 4.3455, 4.3286, 4.3358, 4.3029,\n",
      "        4.3231, 4.3330, 4.3300, 4.3490, 4.3975, 4.4013, 4.4011, 4.4241, 4.3784,\n",
      "        4.3980, 4.3839, 4.3875], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 3.983636  [979724/5599865]\n",
      "average loss by time tensor([4.2536, 4.2411, 4.1982, 4.1709, 4.1817, 4.0968, 4.0831, 4.0505, 4.0199,\n",
      "        4.0279, 4.0313, 4.0233, 4.0201, 3.9914, 3.9938, 3.9491, 3.9273, 3.9697,\n",
      "        3.9315, 3.9482, 3.9091, 3.9025, 3.9158, 3.8897, 3.8939, 3.9443, 3.8876,\n",
      "        3.8802, 3.8630, 3.8804, 3.8688, 3.8463, 3.8466, 3.8569, 3.8950, 3.8857,\n",
      "        3.9041, 3.9099, 3.8998, 3.8688, 3.8834, 3.9040, 3.9480, 3.9386, 3.9493,\n",
      "        3.9355, 3.9516, 3.9465, 3.9677, 3.9558, 3.9566, 3.9462, 3.9746, 3.9516,\n",
      "        3.9919, 4.0309, 4.0119, 4.0368, 4.0386, 4.0200, 4.0393, 4.0515, 4.0814,\n",
      "        4.1085, 4.1042, 4.1101, 4.1519, 4.1703, 4.1416, 4.1307, 4.0743, 4.0709,\n",
      "        4.0731, 4.0714, 4.0385, 4.0332, 4.0252, 4.0192, 4.0647, 4.0597, 4.0450,\n",
      "        4.0488, 3.9895, 3.9835, 3.9869, 4.0022, 4.0002, 3.9409, 3.9788, 3.9639,\n",
      "        3.9516, 3.9787, 3.9654, 4.0008, 3.9783, 3.9741, 3.9938, 3.9279, 3.9067,\n",
      "        3.9366, 3.9300, 3.9123, 3.9478, 3.9286, 3.9375, 3.9718, 3.9334, 3.9424,\n",
      "        3.9369, 3.8995, 3.9109, 3.9479, 3.9571, 3.9140, 3.8989, 3.9211, 3.9575,\n",
      "        3.9502, 3.9462, 3.9212], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.934036  [992124/5599865]\n",
      "average loss by time tensor([5.9835, 5.9842, 5.9733, 5.9794, 5.9255, 5.9260, 5.8080, 5.8167, 5.8620,\n",
      "        5.8415, 5.8552, 5.8713, 5.8560, 5.8703, 5.8304, 5.8101, 5.8404, 5.8358,\n",
      "        5.8288, 5.7546, 5.7783, 5.7900, 5.7391, 5.7842, 5.7987, 5.8042, 5.8507,\n",
      "        5.8386, 5.8885, 5.8530, 5.8418, 5.9272, 5.8959, 5.8950, 5.8830, 5.9434,\n",
      "        5.9223, 5.8932, 5.8767, 5.9326, 5.9110, 5.9442, 5.9388, 5.9128, 5.9041,\n",
      "        5.9389, 5.8806, 5.9477, 5.9560, 5.9413, 5.9029, 5.9372, 5.8875, 5.8894,\n",
      "        5.8643, 5.8160, 5.9112, 5.8923, 5.9093, 5.9027, 5.8985, 5.9610, 5.9226,\n",
      "        5.9647, 5.9215, 5.9565, 5.9362, 5.9281, 6.0126, 5.9821, 6.0281, 6.0216,\n",
      "        5.9790, 5.9332, 5.9994, 5.9925, 5.9682, 6.0035, 5.9583, 5.9643, 5.9552,\n",
      "        6.0016, 6.0269, 6.0077, 5.9906, 5.9595, 5.9301, 6.0216, 5.9902, 6.0520,\n",
      "        6.0249, 5.9932, 6.0041, 6.0005, 6.0524, 6.0388, 6.0147, 6.0188, 6.0540,\n",
      "        5.9940, 6.0687, 6.0605, 6.0497, 6.0425, 6.0307, 5.9824, 6.0270, 6.0559,\n",
      "        5.9837, 6.0185, 5.9675, 5.9928, 5.9698, 5.9796, 5.9615, 5.9668, 5.9219,\n",
      "        5.9293, 5.9050, 5.9310], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.803907  [1004524/5599865]\n",
      "average loss by time tensor([4.7554, 4.7526, 4.7575, 4.7311, 4.7202, 4.7631, 4.6728, 4.6764, 4.7126,\n",
      "        4.6960, 4.7173, 4.7496, 4.7178, 4.7010, 4.7200, 4.7264, 4.7135, 4.7117,\n",
      "        4.7246, 4.7354, 4.7764, 4.7752, 4.7600, 4.8067, 4.8019, 4.7882, 4.8496,\n",
      "        4.8551, 4.8884, 4.9005, 4.9045, 4.9166, 4.8909, 4.8650, 4.8889, 4.8996,\n",
      "        4.8802, 4.8420, 4.8187, 4.8634, 4.8465, 4.8636, 4.8134, 4.8117, 4.7740,\n",
      "        4.8022, 4.7341, 4.7515, 4.7696, 4.7512, 4.7429, 4.7463, 4.7376, 4.7789,\n",
      "        4.7623, 4.7624, 4.7516, 4.7466, 4.7398, 4.7134, 4.7183, 4.7287, 4.7073,\n",
      "        4.7239, 4.7224, 4.7306, 4.7241, 4.7483, 4.7528, 4.7394, 4.7635, 4.8074,\n",
      "        4.7792, 4.7442, 4.7689, 4.7830, 4.7724, 4.8026, 4.7671, 4.7540, 4.7563,\n",
      "        4.7764, 4.7619, 4.7388, 4.7581, 4.7478, 4.7545, 4.7750, 4.7767, 4.8039,\n",
      "        4.8266, 4.7946, 4.8525, 4.8587, 4.8478, 4.8433, 4.8843, 4.8826, 4.8645,\n",
      "        4.8263, 4.8675, 4.8851, 4.8981, 4.9093, 4.8960, 4.8814, 4.8912, 4.9197,\n",
      "        4.9253, 4.9533, 4.9754, 4.9464, 4.9432, 4.9883, 4.9791, 4.9732, 4.9367,\n",
      "        4.9039, 4.9240, 4.8765], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.169824  [1016924/5599865]\n",
      "average loss by time tensor([5.2290, 5.2414, 5.2631, 5.2816, 5.2279, 5.2507, 5.2469, 5.2520, 5.2476,\n",
      "        5.2248, 5.2046, 5.2271, 5.2286, 5.2437, 5.1972, 5.2510, 5.2728, 5.2867,\n",
      "        5.2766, 5.2595, 5.2521, 5.2732, 5.2436, 5.2500, 5.2733, 5.2471, 5.2821,\n",
      "        5.3193, 5.3397, 5.2872, 5.3074, 5.3678, 5.3533, 5.3108, 5.3050, 5.3206,\n",
      "        5.3502, 5.3192, 5.3408, 5.3560, 5.3649, 5.3434, 5.3113, 5.3203, 5.2961,\n",
      "        5.3016, 5.3064, 5.3127, 5.3101, 5.3091, 5.2584, 5.2975, 5.2606, 5.2493,\n",
      "        5.2287, 5.2439, 5.2536, 5.2690, 5.2834, 5.2455, 5.2262, 5.2060, 5.1680,\n",
      "        5.1813, 5.1856, 5.1877, 5.1724, 5.1724, 5.1671, 5.1276, 5.1370, 5.1011,\n",
      "        5.0919, 5.0847, 5.1012, 5.0998, 5.0514, 5.0957, 5.1136, 5.1328, 5.0948,\n",
      "        5.0700, 5.1189, 5.1175, 5.1207, 5.1427, 5.1332, 5.0948, 5.1078, 5.0697,\n",
      "        5.0763, 5.0120, 5.0134, 5.0197, 5.0064, 5.0245, 5.0082, 5.0330, 5.0673,\n",
      "        5.0453, 5.0628, 4.9850, 4.9622, 4.9545, 4.9397, 4.9612, 4.9746, 4.9860,\n",
      "        4.9457, 4.9177, 4.9071, 4.9219, 4.9311, 4.9685, 4.9759, 4.9965, 5.0005,\n",
      "        4.9982, 5.0102, 5.0228], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.712407  [1029324/5599865]\n",
      "average loss by time tensor([4.8486, 4.8863, 4.8879, 4.8699, 4.8861, 4.8609, 4.8577, 4.8454, 4.8095,\n",
      "        4.7920, 4.7735, 4.7636, 4.7908, 4.7915, 4.7806, 4.7822, 4.7525, 4.7454,\n",
      "        4.7510, 4.7416, 4.7533, 4.7540, 4.7868, 4.7873, 4.7984, 4.7816, 4.7552,\n",
      "        4.7661, 4.7402, 4.7531, 4.7338, 4.7410, 4.7498, 4.7672, 4.7795, 4.7763,\n",
      "        4.7581, 4.7409, 4.7496, 4.7388, 4.7275, 4.7283, 4.7408, 4.7250, 4.7223,\n",
      "        4.7455, 4.7504, 4.7201, 4.7065, 4.7122, 4.6831, 4.6739, 4.6753, 4.6867,\n",
      "        4.6992, 4.7394, 4.7571, 4.7622, 4.7447, 4.7572, 4.7982, 4.8143, 4.7775,\n",
      "        4.7590, 4.7344, 4.7270, 4.7097, 4.7379, 4.7366, 4.7356, 4.7074, 4.7198,\n",
      "        4.7552, 4.7808, 4.7398, 4.7742, 4.7931, 4.7629, 4.7229, 4.7591, 4.7525,\n",
      "        4.7333, 4.7313, 4.7561, 4.7303, 4.7032, 4.7322, 4.7151, 4.6924, 4.6752,\n",
      "        4.6961, 4.6920, 4.6630, 4.6511, 4.6207, 4.6173, 4.5957, 4.5970, 4.5637,\n",
      "        4.5546, 4.5386, 4.5338, 4.5491, 4.5406, 4.5323, 4.5397, 4.5388, 4.5366,\n",
      "        4.5339, 4.5701, 4.5821, 4.5711, 4.6063, 4.5992, 4.6082, 4.5807, 4.5647,\n",
      "        4.5389, 4.5037, 4.5166], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.131983  [1041724/5599865]\n",
      "average loss by time tensor([5.0293, 5.0031, 5.0506, 5.0412, 5.0091, 5.0249, 4.9757, 4.9848, 5.0244,\n",
      "        5.0190, 5.0701, 5.1165, 5.1419, 5.1077, 5.0677, 5.0545, 5.0903, 5.0595,\n",
      "        5.0946, 5.0320, 5.0497, 5.0472, 5.0222, 5.0418, 5.0155, 4.9751, 5.0820,\n",
      "        5.0608, 5.0793, 5.0606, 5.0145, 5.0926, 5.0605, 5.0558, 5.0769, 5.1057,\n",
      "        5.1001, 5.1279, 5.0905, 5.1241, 5.1106, 5.1470, 5.1175, 5.1395, 5.1287,\n",
      "        5.1700, 5.1179, 5.0999, 5.0973, 5.1112, 5.0896, 5.1298, 5.1004, 5.1108,\n",
      "        5.0694, 5.0871, 5.1760, 5.1724, 5.2525, 5.2645, 5.1678, 5.2365, 5.2318,\n",
      "        5.2258, 5.2308, 5.2417, 5.2618, 5.2201, 5.2080, 5.1690, 5.1931, 5.2344,\n",
      "        5.1766, 5.1469, 5.2069, 5.2195, 5.2154, 5.2250, 5.1822, 5.1873, 5.1615,\n",
      "        5.1660, 5.1953, 5.2097, 5.2298, 5.1928, 5.2131, 5.2731, 5.2522, 5.2799,\n",
      "        5.2574, 5.2213, 5.2195, 5.2052, 5.2253, 5.1837, 5.1704, 5.1573, 5.1885,\n",
      "        5.1715, 5.1849, 5.1443, 5.1300, 5.1413, 5.1281, 5.1053, 5.1360, 5.1414,\n",
      "        5.1361, 5.1349, 5.1141, 5.1295, 5.1040, 5.1069, 5.1027, 5.0875, 5.0711,\n",
      "        5.1554, 5.1251, 5.1334], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.897404  [1054124/5599865]\n",
      "average loss by time tensor([4.7303, 4.7286, 4.7001, 4.6881, 4.6774, 4.6959, 4.7055, 4.6714, 4.6675,\n",
      "        4.6749, 4.6678, 4.6598, 4.6788, 4.6795, 4.6850, 4.7103, 4.7265, 4.7278,\n",
      "        4.7403, 4.7666, 4.7720, 4.7729, 4.7940, 4.7740, 4.7237, 4.7852, 4.7435,\n",
      "        4.7531, 4.7639, 4.7817, 4.7750, 4.7661, 4.7780, 4.7963, 4.7779, 4.7734,\n",
      "        4.7912, 4.8063, 4.8063, 4.8228, 4.7817, 4.7577, 4.8062, 4.8182, 4.8358,\n",
      "        4.8126, 4.7757, 4.7656, 4.7916, 4.7972, 4.8180, 4.7973, 4.7896, 4.7730,\n",
      "        4.7894, 4.8054, 4.7634, 4.7893, 4.7781, 4.7850, 4.8148, 4.8087, 4.8548,\n",
      "        4.8603, 4.8423, 4.8676, 4.8599, 4.8478, 4.8590, 4.8335, 4.8194, 4.8283,\n",
      "        4.8248, 4.8336, 4.8744, 4.9133, 4.9223, 4.9410, 4.9323, 4.9551, 4.9677,\n",
      "        4.9758, 4.9939, 5.0497, 5.0755, 5.0271, 5.0570, 5.0857, 5.1212, 5.0935,\n",
      "        5.1073, 5.1012, 5.1077, 5.1294, 5.1019, 5.1165, 5.1290, 5.0968, 5.1694,\n",
      "        5.1855, 5.1694, 5.1935, 5.1774, 5.1712, 5.1561, 5.1451, 5.1667, 5.1296,\n",
      "        5.1106, 5.1591, 5.1727, 5.1532, 5.1384, 5.1452, 5.1452, 5.1935, 5.1694,\n",
      "        5.1935, 5.1935, 5.2500], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.100806  [1066524/5599865]\n",
      "average loss by time tensor([5.3132, 5.2924, 5.2964, 5.2418, 5.2417, 5.2669, 5.2444, 5.2657, 5.2936,\n",
      "        5.2707, 5.2974, 5.2974, 5.2935, 5.2765, 5.2667, 5.2726, 5.2756, 5.2639,\n",
      "        5.2756, 5.2655, 5.2923, 5.3075, 5.3107, 5.3145, 5.3003, 5.3032, 5.2886,\n",
      "        5.2628, 5.2474, 5.2401, 5.2247, 5.2445, 5.1926, 5.1591, 5.1846, 5.1940,\n",
      "        5.1974, 5.1450, 5.1128, 5.1205, 5.0890, 5.0693, 5.0457, 5.0526, 5.0606,\n",
      "        5.0743, 5.0616, 5.0938, 5.0826, 5.0897, 5.1116, 5.1104, 5.1300, 5.1393,\n",
      "        5.1069, 5.0991, 5.0992, 5.0934, 5.0730, 5.0722, 5.0387, 5.0690, 5.0544,\n",
      "        5.0514, 5.0301, 5.0507, 5.0604, 5.0360, 5.0486, 5.0728, 5.0434, 5.0548,\n",
      "        5.0120, 4.9949, 5.0502, 5.0540, 5.0376, 5.0589, 5.0358, 5.0247, 5.0313,\n",
      "        5.0518, 5.0700, 5.0391, 5.0566, 5.0464, 4.9941, 5.0136, 4.9602, 4.9948,\n",
      "        4.9828, 4.9486, 4.9840, 4.9362, 4.9506, 4.9575, 4.9595, 4.9581, 4.9539,\n",
      "        4.9439, 4.9525, 4.9383, 4.9617, 4.9581, 4.9417, 4.9109, 4.9450, 4.9655,\n",
      "        4.9547, 4.9909, 5.0016, 5.0221, 4.9943, 4.9647, 4.9564, 4.9955, 4.9834,\n",
      "        5.0014, 5.0246, 5.0134], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.756042  [1078924/5599865]\n",
      "average loss by time tensor([4.6492, 4.6444, 4.6357, 4.6221, 4.6782, 4.6400, 4.6700, 4.6324, 4.6442,\n",
      "        4.6589, 4.6610, 4.6826, 4.7169, 4.7092, 4.7105, 4.7119, 4.7499, 4.7679,\n",
      "        4.7629, 4.7442, 4.7513, 4.7822, 4.8113, 4.7914, 4.7894, 4.7851, 4.7689,\n",
      "        4.7865, 4.8100, 4.8013, 4.8496, 4.8269, 4.8386, 4.8242, 4.8063, 4.7937,\n",
      "        4.7798, 4.8053, 4.7999, 4.8030, 4.8233, 4.7981, 4.7810, 4.7350, 4.7539,\n",
      "        4.7437, 4.7457, 4.7264, 4.7462, 4.7635, 4.7667, 4.7494, 4.7583, 4.7373,\n",
      "        4.7579, 4.7336, 4.6948, 4.7166, 4.7221, 4.7128, 4.7104, 4.7446, 4.7475,\n",
      "        4.7691, 4.7872, 4.7780, 4.7615, 4.7494, 4.7463, 4.7798, 4.7939, 4.7850,\n",
      "        4.8139, 4.7852, 4.7608, 4.7695, 4.7811, 4.7982, 4.7944, 4.7843, 4.8285,\n",
      "        4.8154, 4.7892, 4.7576, 4.7883, 4.8418, 4.8547, 4.8617, 4.8311, 4.8299,\n",
      "        4.8186, 4.8549, 4.8363, 4.7985, 4.7937, 4.7872, 4.7857, 4.7848, 4.7596,\n",
      "        4.7666, 4.7053, 4.7183, 4.7368, 4.7005, 4.6709, 4.6702, 4.6853, 4.7022,\n",
      "        4.6966, 4.7228, 4.7361, 4.7654, 4.7814, 4.7614, 4.7778, 4.7629, 4.7105,\n",
      "        4.7005, 4.7176, 4.7154], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.965654  [1091324/5599865]\n",
      "average loss by time tensor([4.9295, 4.9554, 4.9679, 4.9563, 4.9905, 5.0242, 5.0081, 4.9758, 4.9919,\n",
      "        4.9758, 4.9758, 4.9384, 4.9113, 4.9113, 4.9355, 4.9355, 5.0000, 4.9839,\n",
      "        4.9677, 4.9677, 4.9758, 4.9737, 4.9340, 4.9556, 4.9651, 4.9937, 4.9717,\n",
      "        4.9355, 4.9233, 4.9369, 4.9667, 4.9643, 4.9534, 4.9529, 4.9874, 5.0421,\n",
      "        5.0776, 5.0544, 5.0786, 5.0737, 5.0850, 5.0600, 5.0686, 5.0426, 5.0433,\n",
      "        5.0616, 5.0421, 5.0663, 5.0483, 5.0463, 5.0578, 5.0878, 5.0542, 5.0396,\n",
      "        4.9902, 4.9903, 4.9761, 4.9614, 4.9837, 5.0001, 4.9122, 4.9630, 4.8758,\n",
      "        4.8755, 4.8395, 4.8663, 4.8568, 4.8402, 4.8672, 4.8594, 4.8555, 4.8929,\n",
      "        4.9296, 4.8962, 4.9416, 4.9857, 4.9611, 4.9906, 4.9858, 4.9973, 5.0199,\n",
      "        5.0325, 4.9570, 4.9552, 4.9882, 5.0130, 4.9905, 4.9935, 5.0045, 5.0325,\n",
      "        4.9760, 4.9296, 4.9539, 4.9413, 4.9823, 4.9794, 4.9754, 4.9558, 5.0077,\n",
      "        4.9326, 4.9505, 4.9245, 4.9034, 4.9368, 4.9341, 4.9478, 4.9787, 4.9920,\n",
      "        4.9830, 4.9615, 4.9161, 4.9196, 4.8813, 4.8929, 4.8755, 4.8775, 4.8489,\n",
      "        4.8829, 4.8697, 4.8648], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.100264  [1103724/5599865]\n",
      "average loss by time tensor([5.3277, 5.3409, 5.3538, 5.3873, 5.3384, 5.3983, 5.3184, 5.3563, 5.3509,\n",
      "        5.3007, 5.2946, 5.3107, 5.3139, 5.3204, 5.2685, 5.2896, 5.2851, 5.2839,\n",
      "        5.2871, 5.2567, 5.2893, 5.3186, 5.2634, 5.2507, 5.2186, 5.1682, 5.2456,\n",
      "        5.2330, 5.2595, 5.2127, 5.2071, 5.2537, 5.2303, 5.2038, 5.1740, 5.1960,\n",
      "        5.2271, 5.2241, 5.1774, 5.2288, 5.2255, 5.2710, 5.2325, 5.2278, 5.1980,\n",
      "        5.2105, 5.1254, 5.2128, 5.1891, 5.1462, 5.0821, 5.1064, 5.0281, 5.0463,\n",
      "        5.0448, 5.0533, 5.1036, 5.0808, 5.1064, 5.1071, 5.0639, 5.0906, 5.0761,\n",
      "        5.1082, 5.1017, 5.1154, 5.1497, 5.1694, 5.1852, 5.1849, 5.1770, 5.1723,\n",
      "        5.1294, 5.0827, 5.1242, 5.0972, 5.0485, 5.0269, 5.0044, 5.0316, 4.9779,\n",
      "        4.9769, 4.9477, 4.9504, 4.9536, 4.9333, 4.9341, 4.8994, 4.8758, 4.8945,\n",
      "        4.8511, 4.8363, 4.8685, 4.8858, 4.8836, 4.8627, 4.8601, 4.8910, 4.8840,\n",
      "        4.8607, 4.8899, 4.8930, 4.8087, 4.8185, 4.8416, 4.8347, 4.8691, 4.8653,\n",
      "        4.8646, 4.9039, 4.8666, 4.8741, 4.9049, 4.9321, 4.9550, 4.9463, 4.9702,\n",
      "        5.0183, 5.0143, 5.0313], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.533210  [1116124/5599865]\n",
      "average loss by time tensor([5.5088, 5.4854, 5.4876, 5.5168, 5.4989, 5.4852, 5.4468, 5.3899, 5.4029,\n",
      "        5.3549, 5.3838, 5.4000, 5.3909, 5.3988, 5.3943, 5.4018, 5.3966, 5.3880,\n",
      "        5.4318, 5.4412, 5.4618, 5.4531, 5.4525, 5.4389, 5.4540, 5.4092, 5.4109,\n",
      "        5.4018, 5.3746, 5.3790, 5.3871, 5.3871, 5.3710, 5.3629, 5.3211, 5.2938,\n",
      "        5.3145, 5.3306, 5.3639, 5.3629, 5.3244, 5.3281, 5.3226, 5.3468, 5.3867,\n",
      "        5.3579, 5.3871, 5.3952, 5.3837, 5.4052, 5.4125, 5.5994, 5.5946, 5.6074,\n",
      "        5.5942, 5.5616, 5.5898, 5.5803, 5.5770, 5.5736, 5.5671, 5.5755, 5.5700,\n",
      "        5.6022, 5.6340, 5.6121, 5.6210, 5.6047, 5.6223, 5.6183, 5.6164, 5.5867,\n",
      "        5.6049, 5.6182, 5.6272, 5.6129, 5.6289, 5.6522, 5.6654, 5.6846, 5.6707,\n",
      "        5.6622, 5.6814, 5.6642, 5.6424, 5.5702, 5.5231, 5.5685, 5.5762, 5.5797,\n",
      "        5.5618, 5.5617, 5.6118, 5.6098, 5.6413, 5.6365, 5.6662, 5.6649, 5.6901,\n",
      "        5.6507, 5.6315, 5.6588, 5.6805, 5.6730, 5.6772, 5.7063, 5.7235, 5.7278,\n",
      "        5.6947, 5.6695, 5.6563, 5.6639, 5.6382, 5.6574, 5.6765, 5.6771, 5.6890,\n",
      "        5.6948, 5.6757, 5.6921], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.751302  [1128524/5599865]\n",
      "average loss by time tensor([4.7814, 4.7890, 4.7948, 4.8023, 4.8239, 4.8423, 4.7930, 4.8219, 4.8108,\n",
      "        4.8228, 4.8295, 4.8503, 4.8513, 4.8434, 4.8398, 4.8662, 4.8363, 4.8057,\n",
      "        4.8045, 4.8321, 4.8177, 4.8180, 4.7984, 4.7984, 4.7581, 4.7926, 4.7383,\n",
      "        4.7519, 4.7352, 4.7322, 4.7628, 4.7377, 4.7315, 4.7477, 4.7545, 4.7735,\n",
      "        4.7665, 4.7471, 4.7606, 4.7863, 4.7806, 4.7527, 4.7405, 4.7258, 4.7097,\n",
      "        4.7419, 4.7419, 4.7339, 4.7500, 4.7805, 4.7984, 4.8065, 4.7742, 4.7548,\n",
      "        4.7751, 4.7831, 4.7499, 4.7503, 4.7633, 4.7782, 4.7677, 4.7535, 4.7645,\n",
      "        4.7457, 4.7074, 4.6901, 4.6816, 4.6938, 4.7068, 4.7227, 4.6944, 4.6846,\n",
      "        4.7101, 4.7012, 4.7006, 4.7063, 4.7025, 4.6676, 4.6580, 4.6366, 4.7008,\n",
      "        4.6665, 4.6302, 4.6120, 4.6038, 4.6596, 4.6814, 4.6747, 4.7236, 4.6840,\n",
      "        4.7596, 4.7670, 4.7668, 4.7632, 4.7308, 4.7227, 4.7606, 4.7543, 4.7664,\n",
      "        4.7706, 4.7774, 4.7840, 4.7954, 4.7355, 4.7349, 4.7637, 4.7695, 4.7733,\n",
      "        4.7841, 4.7638, 4.7584, 4.7407, 4.7342, 4.7068, 4.7162, 4.7142, 4.7026,\n",
      "        4.7008, 4.7309, 4.7338], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.317695  [1140924/5599865]\n",
      "average loss by time tensor([5.2758, 5.3047, 5.2951, 5.2785, 5.2485, 5.2809, 5.2473, 5.2705, 5.2915,\n",
      "        5.2406, 5.2333, 5.2252, 5.2264, 5.2539, 5.2339, 5.2479, 5.2499, 5.2538,\n",
      "        5.2856, 5.2895, 5.3354, 5.3382, 5.3156, 5.3165, 5.3410, 5.3107, 5.3485,\n",
      "        5.3298, 5.3842, 5.3910, 5.3466, 5.3509, 5.3121, 5.3018, 5.3041, 5.3201,\n",
      "        5.3135, 5.2232, 5.1868, 5.2216, 5.2658, 5.3056, 5.2812, 5.2965, 5.2586,\n",
      "        5.2656, 5.2163, 5.2458, 5.2500, 5.2828, 5.2949, 5.2984, 5.2388, 5.2274,\n",
      "        5.2309, 5.1979, 5.2257, 5.2572, 5.2447, 5.2620, 5.2500, 5.2948, 5.3079,\n",
      "        5.3187, 5.3387, 5.3790, 5.3747, 5.3541, 5.3738, 5.3952, 5.3826, 5.4039,\n",
      "        5.3223, 5.3025, 5.2984, 5.3387, 5.3302, 5.3258, 5.3317, 5.2924, 5.2985,\n",
      "        5.3091, 5.2903, 5.3387, 5.3387, 5.3564, 5.3620, 5.3703, 5.3765, 5.4055,\n",
      "        5.4142, 5.3718, 5.3576, 5.3607, 5.3623, 5.3349, 5.3145, 5.3548, 5.3952,\n",
      "        5.3548, 5.3468, 5.3387, 5.3387, 5.3468, 5.3548, 5.3952, 5.3710, 5.3749,\n",
      "        5.3856, 5.3987, 5.4095, 5.4079, 5.4357, 5.4318, 5.4320, 5.4413, 5.3971,\n",
      "        5.3871, 5.3316, 5.3455], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.235522  [1153324/5599865]\n",
      "average loss by time tensor([5.2376, 5.1965, 5.2027, 5.1906, 5.1377, 5.1563, 5.1459, 5.2062, 5.2242,\n",
      "        5.1852, 5.2262, 5.2100, 5.1950, 5.2224, 5.1804, 5.1777, 5.1838, 5.1894,\n",
      "        5.1956, 5.1613, 5.1911, 5.1732, 5.1550, 5.1546, 5.1562, 5.1340, 5.1959,\n",
      "        5.2287, 5.1819, 5.1820, 5.1860, 5.2468, 5.2550, 5.2936, 5.2954, 5.3566,\n",
      "        5.3382, 5.3367, 5.3419, 5.3794, 5.3616, 5.3468, 5.3353, 5.3477, 5.3395,\n",
      "        5.3775, 5.3415, 5.3369, 5.3350, 5.2940, 5.2924, 5.2970, 5.2814, 5.2937,\n",
      "        5.2814, 5.2815, 5.2823, 5.3148, 5.3035, 5.2943, 5.2919, 5.2293, 5.2322,\n",
      "        5.2538, 5.2396, 5.2304, 5.2380, 5.2422, 5.2581, 5.2258, 5.2258, 5.1935,\n",
      "        5.1694, 5.1210, 5.1406, 5.1494, 5.1616, 5.1822, 5.1763, 5.1672, 5.1853,\n",
      "        5.2069, 5.1817, 5.1659, 5.1813, 5.1677, 5.1467, 5.1618, 5.2028, 5.1900,\n",
      "        5.2005, 5.2389, 5.2404, 5.2339, 5.2500, 5.2581, 5.2244, 5.2248, 5.2138,\n",
      "        5.2177, 5.2500, 5.2419, 5.2339, 5.2500, 5.2339, 5.2486, 5.2500, 5.2258,\n",
      "        5.2339, 5.2419, 5.2339, 5.2500, 5.2096, 5.2216, 5.2294, 5.2789, 5.3029,\n",
      "        5.3149, 5.3128, 5.3334], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.489007  [1165724/5599865]\n",
      "average loss by time tensor([5.2014, 5.2490, 5.2790, 5.2784, 5.2919, 5.2861, 5.3175, 5.3548, 5.3710,\n",
      "        5.4032, 5.3629, 5.3790, 5.3871, 5.3710, 5.3898, 5.3748, 5.3853, 5.3540,\n",
      "        5.3434, 5.3357, 5.2879, 5.2891, 5.3308, 5.2878, 5.2624, 5.3090, 5.2836,\n",
      "        5.3036, 5.2841, 5.3295, 5.3263, 5.2987, 5.3449, 5.3458, 5.3530, 5.3546,\n",
      "        5.3961, 5.3444, 5.3257, 5.3311, 5.3118, 5.3175, 5.3940, 5.3979, 5.4437,\n",
      "        5.4632, 5.4779, 5.5214, 5.4981, 5.4743, 5.4985, 5.5050, 5.4931, 5.4889,\n",
      "        5.4770, 5.4608, 5.4919, 5.4522, 5.4571, 5.4521, 5.4615, 5.4592, 5.4664,\n",
      "        5.4759, 5.4960, 5.5098, 5.4928, 5.5172, 5.5216, 5.5458, 5.5659, 5.5600,\n",
      "        5.5580, 5.5889, 5.5764, 5.5771, 5.6135, 5.6129, 5.6121, 5.6030, 5.6049,\n",
      "        5.6427, 5.6498, 5.6210, 5.5968, 5.5887, 5.6210, 5.6210, 5.6114, 5.6161,\n",
      "        5.6210, 5.6129, 5.6278, 5.6223, 5.6739, 5.6916, 5.7086, 5.7419, 5.7661,\n",
      "        5.8306, 5.8849, 5.8978, 5.8885, 5.8327, 5.8309, 5.6159, 5.6297, 5.6329,\n",
      "        5.6248, 5.6135, 5.5920, 5.6031, 5.5880, 5.5589, 5.5258, 5.4718, 5.4716,\n",
      "        5.4431, 5.4401, 5.4709], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.942942  [1178124/5599865]\n",
      "average loss by time tensor([4.9113, 4.9380, 4.9559, 4.8533, 4.8316, 4.8629, 4.8182, 4.8230, 4.8339,\n",
      "        4.8294, 4.8550, 4.8670, 4.8660, 4.8739, 4.8452, 4.8527, 4.8878, 4.8712,\n",
      "        4.8483, 4.8449, 4.8593, 4.8026, 4.7982, 4.8156, 4.8260, 4.8090, 4.8209,\n",
      "        4.8198, 4.7860, 4.7677, 4.7500, 4.7648, 4.7449, 4.7512, 4.7449, 4.7416,\n",
      "        4.7475, 4.7540, 4.7431, 4.7727, 4.7467, 4.7777, 4.7693, 4.7817, 4.7654,\n",
      "        4.7874, 4.7513, 4.7741, 4.7631, 4.7716, 4.7764, 4.7725, 4.7591, 4.7885,\n",
      "        4.8091, 4.8740, 4.9179, 4.9485, 4.9614, 4.9581, 4.9596, 4.9922, 4.9712,\n",
      "        5.0196, 5.0135, 5.0008, 5.0022, 4.9978, 4.9750, 4.9871, 4.9934, 4.9798,\n",
      "        4.9576, 4.9268, 4.9558, 5.0101, 4.9976, 5.0245, 5.0255, 5.0598, 5.0559,\n",
      "        5.0610, 5.0630, 5.0623, 5.0559, 5.0587, 5.0459, 5.0550, 5.0451, 5.0545,\n",
      "        5.0825, 5.0722, 5.0999, 5.0923, 5.1023, 5.1443, 5.1409, 5.1386, 5.1558,\n",
      "        5.1243, 5.1374, 5.1567, 5.1461, 5.1338, 5.1145, 5.0912, 5.1079, 5.1282,\n",
      "        5.1255, 5.1458, 5.1350, 5.1573, 5.1269, 5.1102, 5.1121, 5.1383, 5.0957,\n",
      "        5.1033, 5.0916, 5.0924], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.052341  [1190524/5599865]\n",
      "average loss by time tensor([5.1041, 5.1300, 5.1345, 5.1276, 5.1166, 5.1283, 5.1059, 5.0884, 5.1025,\n",
      "        5.1119, 5.0806, 5.0930, 5.1290, 5.1290, 5.1290, 5.1694, 5.1532, 5.1532,\n",
      "        5.1452, 5.1129, 5.1131, 5.0963, 5.0794, 5.0907, 5.1330, 5.1115, 5.1213,\n",
      "        5.1359, 5.1045, 5.1290, 5.1218, 5.1229, 5.1059, 5.1046, 5.1382, 5.1640,\n",
      "        5.1623, 5.1390, 5.0985, 5.1147, 5.1073, 5.1023, 5.0826, 5.0951, 5.1284,\n",
      "        5.1371, 5.0753, 5.0691, 5.0913, 5.0745, 5.1292, 5.1532, 5.1532, 5.1452,\n",
      "        5.1613, 5.1532, 5.1452, 5.1210, 5.0968, 5.0968, 5.1153, 5.0930, 5.1041,\n",
      "        5.1014, 5.0631, 5.0539, 5.0484, 5.0806, 5.0383, 5.0477, 5.0509, 5.0572,\n",
      "        5.0425, 5.0659, 5.0329, 5.0344, 5.0427, 5.0435, 5.0479, 5.0204, 5.0182,\n",
      "        4.9782, 4.9590, 4.9761, 4.9656, 4.9735, 5.0210, 4.9832, 5.0202, 5.0107,\n",
      "        5.0237, 5.0279, 4.9987, 4.9715, 4.9536, 4.9407, 4.9505, 4.9513, 4.9533,\n",
      "        4.9716, 4.9088, 4.9097, 4.8792, 4.8572, 4.8598, 4.8673, 4.8822, 4.8767,\n",
      "        4.8694, 4.8714, 4.8999, 4.8952, 4.9534, 4.9295, 4.9515, 4.9391, 4.9515,\n",
      "        5.0020, 5.0050, 4.9903], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.235870  [1202924/5599865]\n",
      "average loss by time tensor([5.2857, 5.2696, 5.2678, 5.2573, 5.2107, 5.2772, 5.1973, 5.2145, 5.2369,\n",
      "        5.1901, 5.2340, 5.2502, 5.2598, 5.2258, 5.1907, 5.2017, 5.2519, 5.2881,\n",
      "        5.3386, 5.2790, 5.3500, 5.3473, 5.2752, 5.3029, 5.2859, 5.2201, 5.2873,\n",
      "        5.2713, 5.2338, 5.1919, 5.1678, 5.2090, 5.1552, 5.1670, 5.1873, 5.2164,\n",
      "        5.1902, 5.1423, 5.1181, 5.1847, 5.1278, 5.1372, 5.0703, 5.1027, 5.0892,\n",
      "        5.1519, 5.0473, 5.1161, 5.1069, 5.1043, 5.1131, 5.1262, 5.1117, 5.1986,\n",
      "        5.1445, 5.1776, 5.2139, 5.1415, 5.1948, 5.2248, 5.1840, 5.2296, 5.1756,\n",
      "        5.2105, 5.1901, 5.2178, 5.2061, 5.1484, 5.1639, 5.1831, 5.2144, 5.2077,\n",
      "        5.1472, 5.1315, 5.1967, 5.2002, 5.1558, 5.2001, 5.2264, 5.2518, 5.2585,\n",
      "        5.2735, 5.2666, 5.2717, 5.2952, 5.2797, 5.2499, 5.3231, 5.2703, 5.2889,\n",
      "        5.2827, 5.2357, 5.3026, 5.2717, 5.3081, 5.2856, 5.2946, 5.2982, 5.3285,\n",
      "        5.2913, 5.3718, 5.3697, 5.3281, 5.3604, 5.3623, 5.3045, 5.3210, 5.2669,\n",
      "        5.2891, 5.3587, 5.2918, 5.3478, 5.2958, 5.3430, 5.3457, 5.3506, 5.3375,\n",
      "        5.3391, 5.3303, 5.3423], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.238280  [1215324/5599865]\n",
      "average loss by time tensor([5.2500, 5.2339, 5.2603, 5.2339, 5.2151, 5.2209, 5.1895, 5.2258, 5.1935,\n",
      "        5.1935, 5.1532, 5.1920, 5.2061, 5.2159, 5.2184, 5.2086, 5.1770, 5.1762,\n",
      "        5.1861, 5.1746, 5.1730, 5.1969, 5.2016, 5.2177, 5.2339, 5.2573, 5.2695,\n",
      "        5.2867, 5.2700, 5.2591, 5.2478, 5.2524, 5.2663, 5.2425, 5.2823, 5.3226,\n",
      "        5.3710, 5.3710, 5.3790, 5.3117, 5.3061, 5.3266, 5.3063, 5.2887, 5.3329,\n",
      "        5.2948, 5.2693, 5.2705, 5.2726, 5.2732, 5.2667, 5.2136, 5.1981, 5.1835,\n",
      "        5.1957, 5.1816, 5.1959, 5.1984, 5.1645, 5.1690, 5.2292, 5.1998, 5.2329,\n",
      "        5.2359, 5.2319, 5.2517, 5.2529, 5.2613, 5.2146, 5.2012, 5.2373, 5.2356,\n",
      "        5.2774, 5.3060, 5.2934, 5.3206, 5.3211, 5.3308, 5.3235, 5.3275, 5.2788,\n",
      "        5.2444, 5.2129, 5.2356, 5.2009, 5.1604, 5.1413, 5.1322, 5.1396, 5.1452,\n",
      "        5.1941, 5.2467, 5.2416, 5.2548, 5.2368, 5.2552, 5.2578, 5.2590, 5.2338,\n",
      "        5.2387, 5.2521, 5.2491, 5.2597, 5.2670, 5.2708, 5.2904, 5.2441, 5.2329,\n",
      "        5.2084, 5.1639, 5.1830, 5.1847, 5.2071, 5.2237, 5.2087, 5.2137, 5.2255,\n",
      "        5.2318, 5.2205, 5.2174], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.782712  [1227724/5599865]\n",
      "average loss by time tensor([5.9265, 5.9596, 5.9538, 5.9624, 5.9327, 5.8871, 5.8710, 5.8468, 5.8306,\n",
      "        5.8371, 5.8338, 5.8338, 5.7925, 5.8004, 5.8226, 5.8226, 5.8387, 5.8387,\n",
      "        5.8479, 5.8087, 5.8342, 5.8419, 5.8294, 5.8307, 5.8314, 5.7785, 5.7487,\n",
      "        5.7437, 5.7520, 5.7413, 5.7712, 5.7742, 5.7742, 5.7823, 5.7742, 5.7551,\n",
      "        5.7500, 5.7419, 5.7446, 5.7553, 5.7817, 5.7984, 5.8144, 5.8194, 5.8572,\n",
      "        5.8397, 5.8048, 5.7707, 5.7405, 5.7411, 5.7427, 5.7400, 5.7500, 5.7500,\n",
      "        5.7419, 5.7258, 5.7339, 5.7079, 5.7177, 5.7419, 5.7530, 5.7387, 5.7578,\n",
      "        5.8226, 5.8226, 5.8468, 5.8493, 5.8612, 5.8644, 5.8872, 5.8813, 5.8821,\n",
      "        5.8952, 5.8790, 5.8710, 5.8710, 5.8387, 5.8449, 5.8471, 5.8783, 5.8629,\n",
      "        5.8468, 5.8226, 5.7984, 5.7661, 5.7500, 5.7500, 5.7258, 5.7204, 5.6974,\n",
      "        5.7181, 5.7404, 5.7338, 5.7041, 5.7153, 5.7016, 5.7258, 5.7016, 5.6719,\n",
      "        5.6481, 5.6749, 5.6780, 5.6996, 5.7408, 5.7288, 5.7254, 5.7618, 5.7477,\n",
      "        5.7424, 5.7243, 5.7031, 5.6676, 5.6422, 5.6661, 5.6978, 5.6953, 5.6950,\n",
      "        5.7070, 5.6786, 5.7337], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 3.955188  [1240124/5599865]\n",
      "average loss by time tensor([4.0690, 4.0706, 4.0401, 4.0306, 4.1158, 4.0158, 4.1186, 4.0732, 4.0180,\n",
      "        4.0765, 4.0129, 4.0046, 4.0202, 4.0002, 4.0447, 4.0442, 4.0385, 4.0372,\n",
      "        4.0205, 4.0460, 3.9447, 3.9434, 4.0098, 3.9946, 3.9934, 4.0523, 3.9418,\n",
      "        3.9647, 3.9366, 3.9635, 3.9895, 3.8926, 3.9418, 3.9295, 3.9162, 3.8675,\n",
      "        3.8788, 3.9219, 3.9763, 3.9404, 3.9838, 3.9425, 3.9698, 3.9194, 3.9433,\n",
      "        3.8481, 3.9455, 3.8612, 3.8647, 3.8851, 3.8875, 3.8650, 3.9536, 3.9429,\n",
      "        4.0093, 3.9807, 3.9068, 3.9661, 3.9410, 3.9231, 4.0008, 3.8996, 3.9287,\n",
      "        3.8772, 3.9102, 3.8994, 3.8758, 3.9431, 3.8845, 3.8979, 3.8415, 3.7962,\n",
      "        3.8765, 3.9231, 3.8034, 3.8306, 3.9015, 3.8711, 3.9124, 3.8805, 3.9038,\n",
      "        3.9026, 3.8540, 3.8832, 3.8621, 3.9044, 3.9297, 3.8439, 3.9453, 3.8897,\n",
      "        3.9137, 3.9960, 3.9067, 3.9774, 3.8868, 3.9272, 3.9233, 3.9378, 3.9060,\n",
      "        3.9805, 3.9422, 3.9850, 4.0139, 3.9693, 3.9807, 4.0239, 3.9631, 3.9548,\n",
      "        4.0088, 3.9882, 4.0136, 4.0186, 4.0617, 4.0145, 4.0019, 4.0007, 4.0591,\n",
      "        4.0756, 4.0631, 4.0199], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.664188  [1252524/5599865]\n",
      "average loss by time tensor([5.6250, 5.6518, 5.6586, 5.6442, 5.6358, 5.6320, 5.5869, 5.5922, 5.6329,\n",
      "        5.6231, 5.6536, 5.6778, 5.6747, 5.6419, 5.6686, 5.6749, 5.6514, 5.6176,\n",
      "        5.6295, 5.5869, 5.5740, 5.5891, 5.6162, 5.6281, 5.6297, 5.6811, 5.7315,\n",
      "        5.7622, 5.7567, 5.7462, 5.7445, 5.7595, 5.7154, 5.7459, 5.7476, 5.7661,\n",
      "        5.7988, 5.8049, 5.7668, 5.8103, 5.8059, 5.8166, 5.8126, 5.8137, 5.7808,\n",
      "        5.7738, 5.7435, 5.7635, 5.7432, 5.7340, 5.7550, 5.7175, 5.7228, 5.7481,\n",
      "        5.7283, 5.7450, 5.7455, 5.7149, 5.7394, 5.7695, 5.7328, 5.7183, 5.6843,\n",
      "        5.6954, 5.7118, 5.6935, 5.6695, 5.6124, 5.6170, 5.6196, 5.6318, 5.6478,\n",
      "        5.6060, 5.5966, 5.6482, 5.6592, 5.6137, 5.6634, 5.6236, 5.6308, 5.6171,\n",
      "        5.6595, 5.6667, 5.6322, 5.5976, 5.5630, 5.5969, 5.6083, 5.5942, 5.6070,\n",
      "        5.6322, 5.5792, 5.6026, 5.5923, 5.6110, 5.5594, 5.5537, 5.6152, 5.6424,\n",
      "        5.5953, 5.6233, 5.6195, 5.6207, 5.6446, 5.6342, 5.5793, 5.5926, 5.6358,\n",
      "        5.6114, 5.6390, 5.6137, 5.6201, 5.5996, 5.6171, 5.6233, 5.6068, 5.5819,\n",
      "        5.5898, 5.5779, 5.5628], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.392306  [1264924/5599865]\n",
      "average loss by time tensor([5.2372, 5.2640, 5.2627, 5.2581, 5.2581, 5.2470, 5.2438, 5.2311, 5.2373,\n",
      "        5.2294, 5.2309, 5.2552, 5.2611, 5.2688, 5.2758, 5.2543, 5.2563, 5.2553,\n",
      "        5.2579, 5.2268, 5.2587, 5.2336, 5.2525, 5.2563, 5.2906, 5.2529, 5.2468,\n",
      "        5.2211, 5.2073, 5.1876, 5.1557, 5.1861, 5.1661, 5.1872, 5.1724, 5.2082,\n",
      "        5.2148, 5.1978, 5.2106, 5.2549, 5.2483, 5.3155, 5.3204, 5.3255, 5.3431,\n",
      "        5.3577, 5.3993, 5.4205, 5.4274, 5.4677, 5.4516, 5.4677, 5.4758, 5.4610,\n",
      "        5.3972, 5.4298, 5.4498, 5.4548, 5.4855, 5.4718, 5.4721, 5.4703, 5.4829,\n",
      "        5.4825, 5.4681, 5.4866, 5.5036, 5.5033, 5.4846, 5.4833, 5.4789, 5.4807,\n",
      "        5.4606, 5.4818, 5.5235, 5.5213, 5.5410, 5.5519, 5.5129, 5.5080, 5.5424,\n",
      "        5.5186, 5.5049, 5.5363, 5.5468, 5.5308, 5.5183, 5.5033, 5.4656, 5.4640,\n",
      "        5.4565, 5.4138, 5.4577, 5.4258, 5.4425, 5.4361, 5.4087, 5.4150, 5.4595,\n",
      "        5.4469, 5.5130, 5.4741, 5.4774, 5.5246, 5.5020, 5.4701, 5.5373, 5.5173,\n",
      "        5.5144, 5.5079, 5.5074, 5.4977, 5.4913, 5.5314, 5.4978, 5.5171, 5.5082,\n",
      "        5.5081, 5.5255, 5.5200], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.442653  [1277324/5599865]\n",
      "average loss by time tensor([4.5476, 4.5432, 4.5350, 4.5086, 4.5109, 4.4770, 4.4988, 4.5066, 4.5007,\n",
      "        4.5123, 4.3915, 4.3696, 4.3577, 4.3262, 4.3087, 4.3185, 4.3420, 4.3194,\n",
      "        4.2590, 4.2451, 4.2338, 4.2590, 4.3207, 4.3008, 4.2826, 4.3365, 4.3169,\n",
      "        4.3351, 4.3338, 4.3847, 4.4186, 4.4016, 4.4235, 4.4172, 4.4236, 4.4024,\n",
      "        4.4224, 4.4290, 4.4220, 4.3974, 4.4366, 4.4452, 4.4773, 4.4420, 4.4154,\n",
      "        4.4758, 4.4839, 4.4919, 4.4839, 4.4435, 4.4597, 4.4677, 4.5194, 4.4898,\n",
      "        4.4661, 4.4739, 4.4702, 4.4784, 4.4881, 4.4765, 4.5117, 4.4907, 4.4948,\n",
      "        4.4839, 4.4599, 4.4585, 4.4742, 4.4703, 4.4677, 4.4685, 4.4489, 4.4149,\n",
      "        4.4649, 4.4803, 4.4909, 4.5115, 4.5161, 4.5293, 4.5453, 4.5331, 4.5490,\n",
      "        4.5386, 4.5102, 4.5115, 4.5082, 4.4792, 4.4933, 4.5118, 4.4785, 4.4398,\n",
      "        4.4474, 4.4665, 4.5179, 4.5069, 4.5291, 4.5243, 4.5179, 4.5187, 4.5046,\n",
      "        4.5068, 4.4980, 4.4852, 4.4950, 4.4663, 4.4514, 4.4634, 4.4345, 4.4253,\n",
      "        4.4039, 4.3847, 4.3875, 4.3859, 4.3814, 4.3775, 4.3778, 4.3790, 4.3637,\n",
      "        4.3616, 4.3887, 4.4069], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.990207  [1289724/5599865]\n",
      "average loss by time tensor([5.1578, 5.1447, 5.1309, 5.1518, 5.2010, 5.1332, 5.1878, 5.1407, 5.1300,\n",
      "        5.1565, 5.1551, 5.1307, 5.1544, 5.1233, 5.1720, 5.1751, 5.1361, 5.1165,\n",
      "        5.1160, 5.1333, 5.0848, 5.0783, 5.1007, 5.1024, 5.1067, 5.1248, 5.0912,\n",
      "        5.0938, 5.1012, 5.0991, 5.1176, 5.0967, 5.1211, 5.1238, 5.1373, 5.1256,\n",
      "        5.0900, 5.0941, 5.1243, 5.0967, 5.0617, 4.9987, 5.0215, 5.0057, 4.9530,\n",
      "        4.9283, 4.9408, 4.9176, 4.9127, 4.9055, 4.8837, 4.8927, 4.8816, 4.8445,\n",
      "        4.8600, 4.8995, 4.8858, 4.9020, 4.9012, 4.8779, 4.9206, 4.8911, 4.9094,\n",
      "        4.9206, 4.9454, 4.9555, 4.9867, 5.0036, 5.0071, 5.0125, 4.9888, 4.9681,\n",
      "        5.0037, 5.0103, 4.9599, 4.9709, 4.9959, 4.9812, 4.9660, 4.9376, 4.9641,\n",
      "        4.9267, 4.9285, 4.9304, 4.9417, 4.9657, 4.9724, 4.9356, 4.9953, 4.9529,\n",
      "        4.9467, 5.0385, 5.0003, 5.0022, 4.9105, 4.8973, 4.8941, 4.8467, 4.8233,\n",
      "        4.8698, 4.8835, 4.8919, 4.9044, 4.9054, 4.9038, 4.9071, 4.8907, 4.8683,\n",
      "        4.9010, 4.8501, 4.8438, 4.8406, 4.8095, 4.7816, 4.7756, 4.8405, 4.8661,\n",
      "        4.8380, 4.8598, 4.8542], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.789190  [1302124/5599865]\n",
      "average loss by time tensor([5.0024, 4.9854, 4.9919, 5.0081, 5.0119, 5.0098, 4.9702, 4.9897, 5.0090,\n",
      "        5.0100, 5.0196, 5.0260, 5.0329, 5.0565, 5.0484, 5.0356, 5.0306, 5.0381,\n",
      "        5.0242, 4.9839, 4.9516, 4.9355, 4.9355, 4.9113, 4.8871, 4.8871, 4.8952,\n",
      "        4.8952, 4.9200, 4.9335, 4.9194, 4.9126, 4.9194, 4.9618, 4.9614, 4.9242,\n",
      "        4.9253, 4.9274, 4.9194, 4.8952, 4.9032, 4.9435, 4.9449, 4.8869, 4.8897,\n",
      "        4.9011, 4.8919, 4.9026, 4.8887, 4.8650, 4.8505, 4.8296, 4.8464, 4.8620,\n",
      "        4.8706, 4.8300, 4.7797, 4.7889, 4.7499, 4.7382, 4.7591, 4.7546, 4.7607,\n",
      "        4.7553, 4.7191, 4.7076, 4.6653, 4.7065, 4.6696, 4.6898, 4.6679, 4.6903,\n",
      "        4.6791, 4.6821, 4.6806, 4.6666, 4.6713, 4.6848, 4.6571, 4.6711, 4.7021,\n",
      "        4.6619, 4.6507, 4.6431, 4.6478, 4.6746, 4.6523, 4.6427, 4.6741, 4.6513,\n",
      "        4.6671, 4.6693, 4.6643, 4.6735, 4.6352, 4.6062, 4.6255, 4.5987, 4.6048,\n",
      "        4.6048, 4.6210, 4.6048, 4.5726, 4.5662, 4.6120, 4.6079, 4.5895, 4.5996,\n",
      "        4.6035, 4.5806, 4.6129, 4.6124, 4.5837, 4.5878, 4.5720, 4.5732, 4.5968,\n",
      "        4.6048, 4.6249, 4.6226], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.836943  [1314524/5599865]\n",
      "average loss by time tensor([5.8592, 5.8696, 5.8905, 5.8461, 5.8098, 5.8642, 5.7907, 5.8095, 5.8428,\n",
      "        5.7949, 5.8336, 5.8177, 5.7931, 5.8079, 5.8024, 5.7639, 5.8501, 5.8366,\n",
      "        5.8097, 5.7556, 5.8204, 5.7935, 5.7414, 5.7675, 5.7730, 5.7285, 5.7776,\n",
      "        5.7815, 5.8003, 5.7710, 5.7499, 5.8394, 5.8306, 5.8404, 5.8602, 5.8982,\n",
      "        5.9174, 5.8874, 5.8780, 5.9281, 5.9458, 5.9963, 5.9359, 5.9581, 5.9648,\n",
      "        5.9997, 5.9403, 5.9671, 5.9572, 5.9707, 5.9396, 5.9529, 5.8811, 5.9361,\n",
      "        5.8774, 5.8544, 5.8369, 5.8086, 5.8368, 5.8350, 5.7850, 5.8133, 5.7916,\n",
      "        5.7944, 5.7860, 5.7828, 5.7693, 5.7605, 5.7316, 5.6889, 5.6963, 5.6942,\n",
      "        5.6600, 5.6504, 5.7118, 5.7698, 5.7643, 5.8385, 5.8467, 5.8542, 5.8403,\n",
      "        5.8750, 5.8904, 5.8882, 5.9181, 5.9156, 5.8983, 5.9145, 5.8625, 5.9054,\n",
      "        5.8928, 5.8226, 5.8478, 5.8433, 5.8354, 5.8303, 5.7816, 5.7615, 5.7660,\n",
      "        5.7424, 5.7988, 5.7932, 5.7860, 5.8351, 5.8236, 5.8172, 5.8795, 5.9131,\n",
      "        5.8803, 5.9327, 5.8984, 5.8731, 5.8528, 5.8764, 5.8737, 5.8704, 5.7835,\n",
      "        5.8015, 5.8412, 5.8537], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.013055  [1326924/5599865]\n",
      "average loss by time tensor([4.8048, 4.7784, 4.7806, 4.7846, 4.8093, 4.7636, 4.8106, 4.8228, 4.8206,\n",
      "        4.8565, 4.8403, 4.8805, 4.9024, 4.9111, 4.9537, 4.9510, 4.9421, 4.9598,\n",
      "        4.9141, 4.9130, 4.8935, 4.8777, 4.8693, 4.8990, 4.9299, 4.9552, 4.9187,\n",
      "        4.9117, 4.9676, 4.9597, 4.9888, 4.9911, 4.9457, 4.9568, 4.9663, 4.9574,\n",
      "        4.9430, 4.9615, 4.9563, 4.9508, 4.9533, 4.9494, 4.9615, 4.9519, 4.9659,\n",
      "        4.9644, 4.9864, 5.0084, 5.0187, 5.0112, 5.0047, 5.0725, 5.1055, 5.0646,\n",
      "        5.0769, 5.0526, 5.0395, 5.0199, 5.0250, 5.0139, 5.0504, 5.0137, 5.0037,\n",
      "        5.0063, 5.0105, 5.0080, 4.9828, 4.9913, 4.9931, 4.9958, 4.9914, 4.9735,\n",
      "        4.9672, 4.9913, 5.0150, 5.0238, 5.0403, 5.0487, 5.0427, 5.0148, 5.0356,\n",
      "        5.0329, 5.0399, 5.0081, 5.0323, 5.0582, 5.0688, 5.0491, 5.0711, 5.0648,\n",
      "        5.0484, 5.0403, 5.0645, 5.0922, 5.0892, 5.1048, 5.1290, 5.1129, 5.1452,\n",
      "        5.1245, 5.1289, 5.1192, 5.1613, 5.1290, 5.1371, 5.1532, 5.1854, 5.1865,\n",
      "        5.1676, 5.1788, 5.1678, 5.1694, 5.1980, 5.2025, 5.2503, 5.2478, 5.2473,\n",
      "        5.2432, 5.2413, 5.2298], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.595985  [1339324/5599865]\n",
      "average loss by time tensor([5.6344, 5.6356, 5.6532, 5.6597, 5.5687, 5.6487, 5.5665, 5.5936, 5.6141,\n",
      "        5.5663, 5.6311, 5.6092, 5.5638, 5.5617, 5.4935, 5.4771, 5.4999, 5.5009,\n",
      "        5.5201, 5.4754, 5.5311, 5.5446, 5.4953, 5.5461, 5.5058, 5.4387, 5.5412,\n",
      "        5.5252, 5.4979, 5.4656, 5.4100, 5.5160, 5.4741, 5.4432, 5.4566, 5.5267,\n",
      "        5.4953, 5.4474, 5.4344, 5.5394, 5.4986, 5.5403, 5.5040, 5.5321, 5.4992,\n",
      "        5.5661, 5.4787, 5.5664, 5.5600, 5.5644, 5.5543, 5.5693, 5.5407, 5.6004,\n",
      "        5.5614, 5.5552, 5.6103, 5.5477, 5.6365, 5.6267, 5.5542, 5.6345, 5.5670,\n",
      "        5.6202, 5.5964, 5.6423, 5.6235, 5.5484, 5.6045, 5.6047, 5.6292, 5.6629,\n",
      "        5.6128, 5.5686, 5.6962, 5.7114, 5.6723, 5.7382, 5.7176, 5.6950, 5.6691,\n",
      "        5.7093, 5.7027, 5.6832, 5.7164, 5.6950, 5.6676, 5.7508, 5.6831, 5.7489,\n",
      "        5.6828, 5.5967, 5.6626, 5.6207, 5.6553, 5.6145, 5.5742, 5.6423, 5.6814,\n",
      "        5.6613, 5.6664, 5.6347, 5.6427, 5.6300, 5.6345, 5.5829, 5.6178, 5.6996,\n",
      "        5.6688, 5.7018, 5.6944, 5.6913, 5.6462, 5.6703, 5.6765, 5.6836, 5.6351,\n",
      "        5.6273, 5.5802, 5.5962], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.951590  [1351724/5599865]\n",
      "average loss by time tensor([5.1671, 5.1294, 5.1770, 5.1539, 5.1185, 5.1386, 5.1077, 5.0935, 5.1127,\n",
      "        5.1169, 5.1064, 5.1298, 5.1123, 5.1039, 5.0654, 5.0900, 5.1048, 5.1031,\n",
      "        5.1233, 5.1048, 5.0760, 5.0597, 5.0336, 5.0225, 5.0332, 5.0633, 5.0882,\n",
      "        5.0873, 5.0396, 4.9988, 4.9995, 5.0335, 4.9974, 5.0035, 5.0131, 4.9862,\n",
      "        4.9839, 4.9765, 4.9679, 5.0009, 5.0234, 5.0371, 5.0089, 4.9975, 4.9927,\n",
      "        5.0345, 5.0367, 5.0227, 5.0689, 5.0624, 5.0535, 5.0640, 5.0677, 5.0397,\n",
      "        5.0726, 5.1162, 5.1132, 5.1247, 5.1361, 5.1454, 5.1396, 5.1459, 5.1428,\n",
      "        5.1375, 5.1112, 5.0963, 5.0888, 5.0361, 5.0473, 5.0218, 5.0240, 5.0571,\n",
      "        5.0769, 5.0806, 5.0397, 5.0163, 4.9926, 4.9926, 4.9707, 4.9535, 4.9432,\n",
      "        4.9297, 4.9116, 4.9045, 4.9020, 4.9216, 4.8999, 4.8953, 4.8790, 4.9113,\n",
      "        4.8629, 4.8629, 4.8629, 4.8387, 4.8226, 4.8468, 4.8226, 4.6532, 4.6371,\n",
      "        4.6261, 4.6060, 4.6014, 4.6322, 4.6130, 4.6063, 4.6022, 4.5980, 4.6205,\n",
      "        4.6072, 4.6202, 4.6415, 4.6220, 4.6171, 4.5808, 4.5730, 4.5565, 4.5806,\n",
      "        4.5968, 4.5887, 4.5806], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 6.293373  [1364124/5599865]\n",
      "average loss by time tensor([6.5704, 6.5983, 6.6879, 6.7035, 6.5852, 6.6692, 6.5402, 6.5925, 6.6578,\n",
      "        6.5730, 6.6215, 6.6057, 6.6065, 6.5804, 6.5123, 6.4775, 6.4975, 6.4710,\n",
      "        6.4645, 6.4215, 6.5203, 6.5321, 6.4896, 6.5194, 6.5207, 6.4402, 6.5107,\n",
      "        6.4708, 6.4756, 6.4826, 6.4298, 6.4854, 6.4023, 6.3986, 6.3735, 6.4338,\n",
      "        6.4075, 6.3793, 6.3438, 6.4853, 6.4157, 6.4606, 6.4122, 6.4089, 6.3738,\n",
      "        6.4367, 6.2861, 6.3877, 6.3663, 6.3776, 6.3231, 6.3987, 6.3354, 6.3727,\n",
      "        6.3434, 6.3253, 6.4288, 6.2852, 6.3170, 6.3077, 6.1970, 6.3013, 6.2341,\n",
      "        6.2299, 6.1572, 6.1725, 6.1912, 6.1296, 6.1787, 6.1646, 6.2350, 6.2178,\n",
      "        6.1498, 6.0410, 6.1680, 6.1569, 6.0962, 6.1642, 6.1306, 6.1327, 6.1252,\n",
      "        6.1509, 6.1415, 6.1209, 6.1646, 6.1327, 6.0791, 6.1408, 6.0843, 6.1540,\n",
      "        6.0531, 6.0006, 6.0637, 6.0005, 6.0691, 6.0582, 6.0590, 6.0914, 6.1927,\n",
      "        6.0951, 6.1810, 6.1451, 6.1386, 6.1719, 6.1254, 6.0901, 6.1556, 6.2212,\n",
      "        6.1523, 6.1703, 6.0901, 6.1417, 6.0951, 6.0755, 6.0791, 6.0517, 5.9465,\n",
      "        5.9607, 5.9221, 5.9644], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.797855  [1376524/5599865]\n",
      "average loss by time tensor([4.6257, 4.6150, 4.6198, 4.5886, 4.5855, 4.5979, 4.5666, 4.5976, 4.5878,\n",
      "        4.5798, 4.6057, 4.5887, 4.5484, 4.5484, 4.5591, 4.5584, 4.5533, 4.5447,\n",
      "        4.5783, 4.5668, 4.5982, 4.5734, 4.6047, 4.6314, 4.6247, 4.6706, 4.6843,\n",
      "        4.7235, 4.7323, 4.7188, 4.6978, 4.7176, 4.7357, 4.7428, 4.7516, 4.7584,\n",
      "        4.7648, 4.7552, 4.7823, 4.7902, 4.7823, 4.7661, 4.7419, 4.6694, 4.6855,\n",
      "        4.7002, 4.7222, 4.7361, 4.7626, 4.7173, 4.7196, 4.7608, 4.7821, 4.7934,\n",
      "        4.8225, 4.8542, 4.8228, 4.8078, 4.8239, 4.7903, 4.8124, 4.8142, 4.7931,\n",
      "        4.8145, 4.8285, 4.8298, 4.8142, 4.8179, 4.8695, 4.8733, 4.8940, 4.9200,\n",
      "        4.9342, 4.9194, 4.9522, 4.9673, 4.9711, 4.9506, 4.9645, 4.9817, 4.9877,\n",
      "        4.9985, 4.9915, 4.9274, 4.9194, 4.9032, 4.8658, 4.8705, 4.8894, 4.9018,\n",
      "        4.9174, 4.8952, 4.8782, 4.8785, 4.9100, 4.9291, 4.9377, 4.9207, 4.9107,\n",
      "        4.9083, 4.9416, 4.9058, 4.9040, 4.9096, 4.8896, 4.9099, 4.9485, 4.9919,\n",
      "        4.9674, 4.9684, 4.9624, 4.9829, 4.9551, 4.9516, 4.9588, 5.0000, 4.9532,\n",
      "        4.8979, 4.8789, 4.8665], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.872454  [1388924/5599865]\n",
      "average loss by time tensor([4.9762, 4.9567, 4.9632, 4.9527, 4.9936, 4.8223, 4.8842, 4.9028, 4.8577,\n",
      "        4.8658, 4.8381, 4.8058, 4.7860, 4.7764, 4.8042, 4.7726, 4.8018, 4.8185,\n",
      "        4.8333, 4.8287, 4.8383, 4.8467, 4.8432, 4.8548, 4.8548, 4.8387, 4.8145,\n",
      "        4.7969, 4.7973, 4.8145, 4.7679, 4.7738, 4.7950, 4.8106, 4.7984, 4.7742,\n",
      "        4.7823, 4.7693, 4.7883, 4.8069, 4.7903, 4.7581, 4.7339, 4.7339, 4.7500,\n",
      "        4.7254, 4.7339, 4.7500, 4.7742, 4.7472, 4.7428, 4.7730, 4.7847, 4.7823,\n",
      "        4.8226, 4.8226, 4.8061, 4.7984, 4.8065, 4.8226, 4.8065, 4.7742, 4.8145,\n",
      "        4.8548, 4.8387, 4.8548, 4.8468, 4.8226, 4.8065, 4.7935, 4.7993, 4.8145,\n",
      "        4.8034, 4.8227, 4.8306, 4.8586, 4.8851, 4.8627, 4.8648, 4.8352, 4.8451,\n",
      "        4.8558, 4.8659, 4.8591, 4.8961, 4.9765, 5.0002, 4.9863, 5.0139, 5.0130,\n",
      "        5.0262, 5.0888, 5.0533, 5.0799, 5.0352, 5.0177, 5.0289, 5.0029, 4.9856,\n",
      "        5.0498, 4.9958, 4.9798, 5.0077, 5.0284, 5.0187, 5.0468, 5.0146, 4.9689,\n",
      "        4.9881, 4.9674, 4.9685, 4.9715, 4.9986, 4.9621, 4.9562, 4.9637, 4.9775,\n",
      "        4.9303, 4.8962, 4.9181], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.063283  [1401324/5599865]\n",
      "average loss by time tensor([5.0988, 5.1209, 5.0812, 5.0693, 5.0395, 5.0799, 5.0688, 5.1444, 5.1620,\n",
      "        5.1458, 5.1771, 5.1691, 5.1763, 5.1588, 5.0732, 5.0518, 5.0483, 5.0346,\n",
      "        5.0551, 5.0072, 5.0746, 5.0392, 4.9984, 4.9856, 5.0133, 4.9752, 5.0628,\n",
      "        5.0579, 5.0692, 5.0284, 5.0386, 5.0997, 5.0600, 5.0558, 5.0751, 5.0777,\n",
      "        5.0815, 5.0536, 5.0242, 5.0753, 5.0684, 5.1195, 5.1071, 5.1217, 5.1442,\n",
      "        5.2039, 5.0923, 5.1633, 5.1368, 5.0992, 5.0754, 5.1078, 5.0472, 5.0742,\n",
      "        5.0309, 5.0110, 5.0993, 5.0531, 5.0790, 5.0748, 4.9922, 5.0801, 5.0409,\n",
      "        5.0688, 5.0629, 5.0783, 5.1231, 5.0480, 5.0575, 5.0220, 5.0806, 5.0715,\n",
      "        5.0051, 4.9841, 5.1111, 5.1350, 5.1038, 5.1128, 5.1198, 5.0672, 5.0374,\n",
      "        5.0673, 5.0871, 5.0675, 5.1007, 5.0952, 5.0515, 5.0863, 5.0127, 5.0439,\n",
      "        5.0392, 4.9761, 5.0501, 5.0406, 5.0898, 5.0521, 5.0378, 5.0665, 5.0833,\n",
      "        5.0812, 5.1260, 5.0959, 5.0643, 5.0408, 5.0223, 4.9580, 5.0018, 5.0280,\n",
      "        5.0082, 5.0398, 5.0047, 4.9862, 4.9415, 5.0067, 5.0601, 5.0418, 4.9632,\n",
      "        4.9709, 4.9603, 4.9246], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.133621  [1413724/5599865]\n",
      "average loss by time tensor([5.2395, 5.2325, 5.2068, 5.2128, 5.2353, 5.2432, 5.2730, 5.2351, 5.2142,\n",
      "        5.2465, 5.2190, 5.2350, 5.2567, 5.2737, 5.2908, 5.2855, 5.2571, 5.2713,\n",
      "        5.2591, 5.2778, 5.2258, 5.1704, 5.2162, 5.1882, 5.1297, 5.1512, 5.0765,\n",
      "        5.0943, 5.0852, 5.0934, 5.0457, 5.0419, 5.0925, 5.0769, 5.0966, 5.0818,\n",
      "        5.0910, 5.0960, 5.1260, 5.1388, 5.1634, 5.1754, 5.2240, 5.2175, 5.1806,\n",
      "        5.1935, 5.2125, 5.1703, 5.1439, 5.1693, 5.1852, 5.1803, 5.2276, 5.1894,\n",
      "        5.2116, 5.2265, 5.1910, 5.1832, 5.1572, 5.1097, 5.1449, 5.1065, 5.1413,\n",
      "        5.1168, 5.1057, 5.0753, 5.0150, 5.0454, 5.0171, 5.0007, 5.0089, 5.0189,\n",
      "        5.0459, 5.0160, 5.0036, 5.0017, 5.0281, 4.9980, 5.0446, 5.0450, 5.0830,\n",
      "        5.0698, 5.0391, 5.0330, 5.0523, 5.0603, 5.0834, 5.0729, 5.1015, 5.1051,\n",
      "        5.0807, 5.1047, 5.0865, 5.1263, 5.0712, 5.1055, 5.1021, 5.1049, 5.0781,\n",
      "        5.1261, 5.1146, 5.1446, 5.1834, 5.1708, 5.1999, 5.2764, 5.1926, 5.1593,\n",
      "        5.1682, 5.1398, 5.1509, 5.1034, 5.1446, 5.0510, 5.0170, 5.0273, 5.0580,\n",
      "        5.0436, 5.0342, 4.9939], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.223343  [1426124/5599865]\n",
      "average loss by time tensor([5.2366, 5.2374, 5.2302, 5.2286, 5.2374, 5.1950, 5.2255, 5.2232, 5.1974,\n",
      "        5.2176, 5.2270, 5.2269, 5.2222, 5.2389, 5.2528, 5.2388, 5.2054, 5.1957,\n",
      "        5.1869, 5.2285, 5.2181, 5.2272, 5.2489, 5.2451, 5.2622, 5.2727, 5.2316,\n",
      "        5.2338, 5.2404, 5.2434, 5.2532, 5.2135, 5.1853, 5.1713, 5.1697, 5.1724,\n",
      "        5.1666, 5.1683, 5.1961, 5.1884, 5.2042, 5.1602, 5.1886, 5.2240, 5.2327,\n",
      "        5.2020, 5.1990, 5.1752, 5.2482, 5.2567, 5.2422, 5.2373, 5.2462, 5.1954,\n",
      "        5.1917, 5.1975, 5.1964, 5.2264, 5.1920, 5.1883, 5.1898, 5.1625, 5.1719,\n",
      "        5.1629, 5.1541, 5.1583, 5.1647, 5.1426, 5.1474, 5.1731, 5.1547, 5.1296,\n",
      "        5.1265, 5.1452, 5.1619, 5.1627, 5.1831, 5.1793, 5.1697, 5.2159, 5.2199,\n",
      "        5.2410, 5.2136, 5.2145, 5.2443, 5.2086, 5.2136, 5.2285, 5.2471, 5.2611,\n",
      "        5.2748, 5.2904, 5.3013, 5.3144, 5.2805, 5.2860, 5.2890, 5.2828, 5.2623,\n",
      "        5.2594, 5.2570, 5.3044, 5.2678, 5.2715, 5.2714, 5.2834, 5.2431, 5.2364,\n",
      "        5.2608, 5.2518, 5.2881, 5.2894, 5.2960, 5.2876, 5.2888, 5.2938, 5.3033,\n",
      "        5.2519, 5.2486, 5.2532], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.099592  [1438524/5599865]\n",
      "average loss by time tensor([5.0655, 5.0455, 5.0912, 5.0691, 5.0304, 5.0382, 5.0090, 5.0423, 5.0596,\n",
      "        5.0676, 5.0954, 5.0872, 5.0655, 5.0566, 5.0333, 5.0460, 5.0694, 5.0824,\n",
      "        5.0975, 5.0797, 5.1127, 5.1272, 5.1207, 5.1189, 5.1303, 5.0910, 5.1289,\n",
      "        5.1499, 5.1842, 5.1838, 5.1932, 5.2528, 5.2669, 5.2880, 5.2701, 5.3098,\n",
      "        5.2897, 5.2403, 5.2200, 5.2356, 5.2177, 5.2208, 5.1924, 5.1714, 5.1594,\n",
      "        5.1691, 5.1269, 5.1341, 5.1110, 5.1015, 5.0807, 5.0968, 5.0665, 5.0961,\n",
      "        5.0663, 5.0511, 5.1078, 5.0745, 5.1226, 5.1472, 5.1143, 5.1912, 5.1614,\n",
      "        5.1483, 5.1169, 5.1163, 5.1369, 5.0939, 5.0952, 5.1080, 5.1356, 5.1417,\n",
      "        5.0986, 5.0731, 5.1172, 5.1119, 5.0896, 5.1304, 5.1040, 5.0804, 5.0484,\n",
      "        5.0540, 5.0626, 5.0439, 5.0593, 5.0682, 5.0392, 5.0865, 5.0349, 5.0460,\n",
      "        5.0195, 4.9924, 5.0302, 5.0120, 5.0561, 5.0414, 5.0272, 5.0437, 5.0897,\n",
      "        5.0266, 5.0784, 5.0761, 5.0462, 5.1133, 5.0893, 5.0343, 5.0601, 5.0840,\n",
      "        5.0512, 5.0782, 5.0358, 5.0379, 4.9998, 5.0139, 5.0518, 5.0309, 5.0309,\n",
      "        5.0312, 5.0317, 5.0701], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.875896  [1450924/5599865]\n",
      "average loss by time tensor([5.8693, 5.9033, 5.9146, 5.8758, 5.8428, 5.7885, 5.7985, 5.7810, 5.8359,\n",
      "        5.8493, 5.8382, 5.8301, 5.8290, 5.7984, 5.7984, 5.7500, 5.7299, 5.7789,\n",
      "        5.7936, 5.8053, 5.7825, 5.7587, 5.7713, 5.6935, 5.7097, 5.7823, 5.8322,\n",
      "        5.8328, 5.8163, 5.7788, 5.7732, 5.7511, 5.7297, 5.7292, 5.7296, 5.7007,\n",
      "        5.7097, 5.7097, 5.7569, 5.7592, 5.7699, 5.7740, 5.7620, 5.7868, 5.8006,\n",
      "        5.7826, 5.8299, 5.8408, 5.8662, 5.8699, 5.8914, 5.8629, 5.9015, 5.9032,\n",
      "        5.8952, 5.8790, 5.8790, 5.9170, 5.9355, 5.9666, 5.9688, 5.9672, 5.9194,\n",
      "        5.9113, 5.9355, 5.9113, 5.8790, 5.8766, 5.8710, 5.8952, 5.9032, 5.8952,\n",
      "        5.8571, 5.8226, 5.8694, 5.8421, 5.7807, 5.8264, 5.8218, 5.8368, 5.8336,\n",
      "        5.8578, 5.8357, 5.8512, 5.8196, 5.8600, 5.8605, 5.8824, 5.8654, 5.8963,\n",
      "        5.9018, 5.8871, 5.8952, 5.9089, 5.9860, 5.9793, 5.9924, 6.0066, 5.9974,\n",
      "        6.0271, 6.0344, 6.0614, 6.0425, 6.0518, 6.0801, 6.0438, 6.0745, 6.0855,\n",
      "        6.0449, 5.9819, 5.9845, 6.0121, 6.0233, 6.0357, 6.0428, 6.0390, 6.0258,\n",
      "        6.0536, 6.0114, 6.0119], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 5.183334  [1463324/5599865]\n",
      "average loss by time tensor([5.3276, 5.3348, 5.3145, 5.3548, 5.3226, 5.3234, 5.3306, 5.3226, 5.3387,\n",
      "        5.3330, 5.2976, 5.2823, 5.2742, 5.2701, 5.2875, 5.3131, 5.3069, 5.2921,\n",
      "        5.2973, 5.3126, 5.2912, 5.2661, 5.2903, 5.3226, 5.3226, 5.3306, 5.3226,\n",
      "        5.3041, 5.3226, 5.2903, 5.2984, 5.3065, 5.3306, 5.3016, 5.3019, 5.3237,\n",
      "        5.3324, 5.3551, 5.3448, 5.3415, 5.3425, 5.2985, 5.3350, 5.2897, 5.2446,\n",
      "        5.2757, 5.2969, 5.2247, 5.2166, 5.2048, 5.1748, 5.1756, 5.1890, 5.1760,\n",
      "        5.2052, 5.2127, 5.1814, 5.1711, 5.1608, 5.1317, 5.1757, 5.1226, 5.0723,\n",
      "        5.0680, 5.0410, 5.0022, 4.9657, 4.9773, 4.9946, 4.9802, 4.9527, 4.9725,\n",
      "        4.9977, 5.0001, 4.9567, 4.9767, 4.9823, 4.9634, 4.9750, 5.0017, 5.0221,\n",
      "        5.0112, 5.0634, 5.0969, 5.0774, 5.0559, 5.0273, 5.0277, 5.0443, 5.0079,\n",
      "        5.0105, 5.0161, 5.0095, 5.0010, 4.9682, 4.9525, 4.9927, 4.9921, 5.0091,\n",
      "        5.0560, 5.0557, 5.1446, 5.1826, 5.1862, 5.2008, 5.2686, 5.2394, 5.2415,\n",
      "        5.2138, 5.2051, 5.2173, 5.2288, 5.2290, 5.1898, 5.2080, 5.2088, 5.2639,\n",
      "        5.2656, 5.3271, 5.2602], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.892225  [1475724/5599865]\n",
      "average loss by time tensor([4.8280, 4.8165, 4.8229, 4.8693, 4.8757, 4.8835, 4.8954, 4.8806, 4.8886,\n",
      "        4.8708, 4.8671, 4.8507, 4.8612, 4.8779, 4.8919, 4.9179, 4.9332, 4.9028,\n",
      "        4.9267, 4.9007, 4.9064, 4.8927, 4.8960, 4.8761, 4.8660, 4.8631, 4.8429,\n",
      "        4.8575, 4.8675, 4.8812, 4.8699, 4.8258, 4.8238, 4.8491, 4.8435, 4.8577,\n",
      "        4.8507, 4.8396, 4.8616, 4.8420, 4.8552, 4.8635, 4.8798, 4.8604, 4.8040,\n",
      "        4.8088, 4.8061, 4.7768, 4.7737, 4.7540, 4.7548, 4.7685, 4.8282, 4.8115,\n",
      "        4.8121, 4.7795, 4.7532, 4.8046, 4.7689, 4.7774, 4.7563, 4.7220, 4.7426,\n",
      "        4.7396, 4.7485, 4.7512, 4.7634, 4.8107, 4.8205, 4.7463, 4.7602, 4.7562,\n",
      "        4.7797, 4.8470, 4.8304, 4.8674, 4.9660, 4.9432, 4.9410, 4.9425, 4.9543,\n",
      "        4.9539, 4.9959, 4.9978, 4.9928, 5.0330, 5.0563, 5.0284, 5.0674, 5.0238,\n",
      "        5.0183, 5.0728, 4.9995, 4.9946, 4.9634, 5.0139, 5.0266, 4.9972, 4.9641,\n",
      "        5.0095, 5.0252, 5.0296, 5.0281, 4.9738, 4.9380, 4.9705, 4.9823, 4.9784,\n",
      "        4.9898, 4.9675, 4.9687, 4.9907, 4.9967, 4.9798, 4.9517, 4.9606, 5.0238,\n",
      "        5.0174, 5.0481, 5.0319], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "loss: 4.712611  [1488124/5599865]\n",
      "average loss by time tensor([4.6367, 4.6499, 4.6989, 4.7095, 4.7063, 4.7511, 4.7346, 4.7429, 4.7462,\n",
      "        4.7498, 4.7977, 4.8464, 4.9100, 4.9166, 4.8793, 4.8412, 4.9143, 4.9254,\n",
      "        4.9163, 4.9396, 5.0022, 5.0080, 4.9762, 4.9608, 4.9655, 4.9365, 4.9505,\n",
      "        4.9408, 4.9488, 4.9559, 4.9545, 4.9739, 4.9557, 4.9094, 4.9225, 4.9200,\n",
      "        4.9495, 4.9290, 4.8821, 4.8963, 4.8861, 4.8857, 4.8047, 4.7871, 4.7594,\n",
      "        4.7620, 4.7426, 4.7609, 4.7589, 4.7579, 4.7388, 4.7404, 4.7238, 4.7211,\n",
      "        4.6429, 4.6761, 4.6773, 4.6057, 4.6229, 4.6046, 4.5511, 4.6488, 4.6766,\n",
      "        4.6700, 4.6472, 4.6391, 4.6101, 4.5998, 4.6351, 4.6428, 4.6565, 4.6538,\n",
      "        4.5993, 4.6377, 4.6628, 4.6669, 4.6290, 4.6776, 4.6475, 4.6541, 4.5939,\n",
      "        4.6266, 4.6490, 4.6712, 4.6536, 4.6034, 4.5912, 4.6204, 4.5861, 4.6187,\n",
      "        4.5812, 4.5655, 4.6085, 4.6035, 4.6290, 4.5789, 4.5610, 4.5814, 4.5983,\n",
      "        4.5314, 4.5689, 4.5832, 4.5656, 4.5845, 4.5766, 4.5435, 4.5866, 4.5987,\n",
      "        4.6037, 4.6314, 4.5999, 4.5898, 4.5515, 4.5361, 4.5101, 4.4702, 4.4601,\n",
      "        4.4639, 4.5023, 4.5185], dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     10\u001b[0m     test_loop(validation_dataloader, model, loss_fn)\n",
      "\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the model to training mode - important for batch normalization and dropout layers\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Unnecessary in this situation but added for best practices\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Compute prediction and loss\u001b[39;49;00m\n",
      "\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\prediction-model\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\prediction-model\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\prediction-model\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n",
      "\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\n",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m, in \u001b[0;36mBikeShareDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n",
      "\u001b[0;32m     45\u001b[0m             x[steps_behind][k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx \u001b[38;5;241m-\u001b[39m (steps_behind \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookback_size \u001b[38;5;241m*\u001b[39m NUM_STATIONS)]\n",
      "\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[1;32m---> 47\u001b[0m             x[steps_behind][k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx \u001b[38;5;241m-\u001b[39m (steps_behind \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookback_size \u001b[38;5;241m*\u001b[39m NUM_STATIONS) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_nearest_differences[current_station][k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFLAT_NUM_FEATURES)\n",
      "\u001b[0;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon))\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(validation_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
